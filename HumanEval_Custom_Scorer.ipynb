{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f43c5a36",
   "metadata": {},
   "source": [
    "# Custom Scorers with HumanEval\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JudgmentLabs/judgment-cookbook/blob/refactor/HumanEval_Custom_Scorer.ipynb)\n",
    "[![Docs](https://img.shields.io/badge/Documentation-blue)](https://docs.judgmentlabs.ai/documentation)\n",
    "\n",
    "In this notebook, you will learn how to evaluate code generation on OpenAI's [HumanEval](https://github.com/openai/human-eval) benchmark and create two **custom scorers**, one which is code-based and one with LLM-as-a-Judge using the [`judgeval`](https://github.com/JudgmentLabs/judgeval) library. \n",
    "\n",
    "1. **Code Execution Scorer**: Uses sandboxed code execution to evaluate code correctness\n",
    "2. **LLM-as-a-Judge Scorer**: Uses language models to evaluate code quality\n",
    "\n",
    "You will generate code using LLMs, create a custom scorers that leverages OpenAI's sandboxed environment and LLM-as-a-Judge, and evaluate it on the HumanEval benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17740081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installations\n",
    "!pip install human-eval datasets openai judgeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0220c8",
   "metadata": {},
   "source": [
    "To run this notebook, select **Runtime -> Run All**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90179020",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You can get your Judgment API key and Org ID for free on the [Judgment Labs Platform](https://app.judgmentlabs.ai/register).\n",
    "\n",
    "![Get Started](./assets/get_started.png)\n",
    "\n",
    "On the Judgment Platform and within your organization, create a project called `humaneval-project`.\n",
    "- Within Judgment's web app, navigate to `Projects` -> `New Project` in the top right corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7767e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set api keys\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ...  # Fill your API keys here\n",
    "os.environ[\"JUDGMENT_API_KEY\"] = ...\n",
    "os.environ[\"JUDGMENT_ORG_ID\"] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "from judgeval import JudgmentClient\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "judgment = JudgmentClient()\n",
    "client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f5296",
   "metadata": {},
   "source": [
    "## 1. Understanding HumanEval\n",
    "\n",
    "HumanEval is a benchmark dataset created by OpenAI for evaluating code generation models. Introduced in the paper [\"Evaluating Large Language Models Trained on Code\"](https://arxiv.org/abs/2107.03374), it contains 164 Python programming problems designed to test functional correctness.\n",
    "\n",
    "### What HumanEval Contains\n",
    "Each problem includes:\n",
    "- **Function signature and docstring**: The problem description\n",
    "- **Test cases**: Automated tests to verify correctness  \n",
    "- **Canonical solution**: Reference implementation\n",
    "- **Entry point**: Function name to test\n",
    "\n",
    "### How HumanEval Evaluates Code\n",
    "HumanEval evaluates model outputs by dynamically building a Python program that stitches together the **Function signature and docstring**, the **model‚Äôs generated solution**, and the **test cases**. This combined program is then executed in a sandbox to verify whether the generated code passes all test cases.\n",
    "\n",
    "The ```check_correctness``` function orchestrates this process: assembles the prompt, generated solution, and tests into a single program, and executes the script in a sandboxed environment.\n",
    "\n",
    "```python\n",
    "# Construct the check program and run it.\n",
    "check_program = (\n",
    "    problem[\"prompt\"] +      # Function signature + docstring\n",
    "    completion +            # Generated code\n",
    "    \"\\n\" +\n",
    "    problem[\"test\"] +       # Test cases\n",
    "    \"\\n\" +\n",
    "    f\"check({problem['entry_point']})\"  # Call the test function\n",
    ")\n",
    "\n",
    "...\n",
    "\n",
    "# WARNING: This executes untrusted model-generated code\n",
    "exec(check_program, exec_globals)\n",
    "```\n",
    "\n",
    "The evaluation is **pass/fail**: if all test cases pass without exceptions, the code is correct. If any test fails or the code crashes, it's incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecd8bd2",
   "metadata": {},
   "source": [
    "### Pass@k\n",
    "\n",
    "Instead of stopping at a simple pass/fail, HumanEval extend this to compute the **Pass@k** statistic. Pass@k measures the probability that at least one of the top-k generated solutions is correct, given:\n",
    "\n",
    "- **n** = total number of generated solutions  \n",
    "- **c** = number of correct solutions among them  \n",
    "- **k** = how many solutions we sample  \n",
    "\n",
    "For example, if we generate **n = 5** completions and some are correct, we can compute **Pass@1** (the chance a single sampled solution is correct) and **Pass@3** (the chance at least one out of three sampled solutions is correct). This provides a more practical view of model performance, since in real use cases we usually sample multiple completions and care whether any of them solves the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c9d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def estimator(n: int, c: int, k: int) -> float:\n",
    "    if n - c < k:\n",
    "        return 1.0\n",
    "    return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d2914",
   "metadata": {},
   "source": [
    "## Define Your Custom Example Class\n",
    "\n",
    "In `judgeval`, all data passed into scorers is represented as an `Example`. The base `Example` object is an abstraction that standardizes how data is stored and accessed. By inheriting from it, you can define your own fields that describe the task you want to monitor.\n",
    "\n",
    "For HumanEval, we‚Äôll create a `HumanEvalExample` that captures the fields needed to represent a problem and its generated code candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe11ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from judgeval.data import Example\n",
    "\n",
    "class HumanEvalExample(Example):\n",
    "    \"\"\"\n",
    "    Custom Example for HumanEval tasks.\n",
    "    \"\"\"\n",
    "    task_id: str\n",
    "    prompt: str\n",
    "    canonical_solution: str\n",
    "    test: str\n",
    "    entry_point: str\n",
    "    generated_codes: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8fa9b1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is:  def add(a: int, b: int) -> int:\n",
      "    # Write your code here\n",
      "The canonical solution is:  def add(a: int, b: int) -> int:\n",
      "    return a + b\n"
     ]
    }
   ],
   "source": [
    "one_example = HumanEvalExample(\n",
    "    task_id=\"HumanEval/0\",\n",
    "    prompt=\"def add(a: int, b: int) -> int:\\n    # Write your code here\",\n",
    "    canonical_solution=\"def add(a: int, b: int) -> int:\\n    return a + b\",\n",
    "    test=\"assert add(1, 2) == 3\",\n",
    "    entry_point=\"add\",\n",
    "    generated_codes=[\"def add(a, b): return a+b\"]\n",
    ")\n",
    "\n",
    "print(\"The prompt is: \", one_example.prompt)\n",
    "print(\"The canonical solution is: \", one_example.canonical_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8775f3",
   "metadata": {},
   "source": [
    "## Custom Code Execution Scorer\n",
    "\n",
    "We'll create a custom scorer using `judgeval` that integrates HumanEval's sandboxed code execution. We'll integrate the `check_correctness` and `estimator` functions to build a custom scorer called `CodeExecutionScorer`.\n",
    "\n",
    "In `judgeval`, the user must implement:\n",
    "\n",
    "`async def a_score_example(self, example: Example)`\n",
    "\n",
    "This method will asynchronously score each example, and the scorer should set three key fields:\n",
    "\n",
    "- **`self.name`**: scorer name shown in the Judgment Labs platform dashboard \n",
    "- **`self.score`**: numeric metric value (e.g., Pass@k in `[0, 1]`)  \n",
    "- **`self.reason`**: human-readable explanation or context behind the score  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af9e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from judgeval.scorers.example_scorer import ExampleScorer\n",
    "from pydantic import Field\n",
    "from human_eval.execution import check_correctness\n",
    "\n",
    "\n",
    "class CodeExecutionScorer(ExampleScorer):\n",
    "    \"\"\"\n",
    "    A scorer for evaluating code generation using check_correctness\n",
    "    and Pass@k statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    k: int = Field(default=1)\n",
    "    n: int = Field(default=1)\n",
    "    \n",
    "    def __init__(self, k: int = 1, n: int = 1):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.n = n\n",
    "        self.name = f\"Pass@{self.k} for {self.n} generations\"\n",
    "        \n",
    "\n",
    "    async def a_score_example(self, example: HumanEvalExample) -> None:\n",
    "        \"\"\"\n",
    "        Score an example by running the generated code against test cases.\n",
    "        \n",
    "        This method uses check_correctness to execute the generated code\n",
    "        in a sandboxed environment and check if it passes all test cases.\n",
    "        \n",
    "        Args:\n",
    "            example (Example): The example containing the problem and generated code\n",
    "            \n",
    "        Returns:\n",
    "            float: The score (1.0 if all tests pass, 0.0 otherwise)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Create problem dict in the format expected by check_correctness\n",
    "        problem = {\n",
    "            \"task_id\": example.task_id,\n",
    "            \"prompt\": example.prompt,\n",
    "            \"test\": example.test,\n",
    "            \"entry_point\": example.entry_point\n",
    "        }\n",
    "\n",
    "        \n",
    "        # Use check_correctness to evaluate the generated code\n",
    "        self.n = len(example.generated_codes)\n",
    "        failed_results = []\n",
    "\n",
    "        for i in range(self.n):\n",
    "            result = check_correctness(\n",
    "                problem=problem,\n",
    "                completion=example.generated_codes[i],\n",
    "                timeout=3.0\n",
    "            )\n",
    "\n",
    "            if not result[\"passed\"]:\n",
    "                failed_results.append(result['result'])\n",
    "\n",
    "        c = self.n - len(failed_results)\n",
    "\n",
    "        pass_k = estimator(self.n, c, self.k)\n",
    "        \n",
    "        self.score = pass_k\n",
    "        self.reason = (\n",
    "            f\"Passed {self.n - len(failed_results)} out of {self.n} tests.\\n\\n\"\n",
    "            \"Failing snippets:\\n\"\n",
    "            + \"\\n---\\n\".join(failed_results)\n",
    "        )        \n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631d91b",
   "metadata": {},
   "source": [
    "## Custom LLM-as-a-Judge Scorer\n",
    "\n",
    "\n",
    "It may not be enough to judge code solutions on their execution results alone, since production-ready code is often evaluated against broader qualities like readability, efficiency, and adherence to best practices. \n",
    "\n",
    "To capture these, we‚Äôll create another custom scorer with `judgeval` that integrates **LLM-as-a-Judge** using a language model to assess generated code against a clear rubric emphasizing **readability**, **efficiency**, and **adherence to best practices**.\n",
    "\n",
    "\n",
    "\n",
    "Unlike the pass/fail execution check, this scorer evaluates multiple generations and returns a **0‚Äìn** score (where n = number of generations), with each generation rated as:\n",
    "- **High Quality (1.0)**: Excellent readability, efficiency, and best practices\n",
    "- **Medium Quality (0.5)**: Good overall with minor issues\n",
    "- **Low Quality (0.0)**: Significant problems\n",
    "\n",
    "The total score is the sum across all generations, giving you both individual generation quality and overall performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08900129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class CodeQualityScorer(ExampleScorer):\n",
    "    \"\"\"\n",
    "    A scorer for evaluating code generation using LLM-as-a-Judge.\n",
    "    \"\"\"\n",
    "\n",
    "    n: int = Field(default=1)\n",
    "    name: str = Field(default=f\"Code Quality for {n} generations\") \n",
    "\n",
    "    def __init__(self, k: int = 1, n: int = 1):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.name = f\"Code Quality for {self.n} generations\"\n",
    "\n",
    "    async def a_score_example(self, example: HumanEvalExample) -> None:\n",
    "        \"\"\"\n",
    "        Score an example by running the generated code against LLM-as-a-Judge.\n",
    "        \n",
    "        Args:\n",
    "            example (HumanEvalExample): The example containing the problem and generated code\n",
    "            \n",
    "        Returns:\n",
    "            float: The score is the sum of individual generation ratings (1.0 for high quality, 0.5 for medium quality, 0.0 for low quality) across n generations\n",
    "        \"\"\"\n",
    "\n",
    "        generations_text = \"\\n\".join([\n",
    "            f\"Generation {i+1}:\\n{example.generated_codes[i]}\\n\" \n",
    "            for i in range(self.n)\n",
    "        ])\n",
    "\n",
    "        prompt = f\"\"\"You are an expert code reviewer. Evaluate each code generation below.\n",
    "\n",
    "        Rate each as:\n",
    "        - HIGH QUALITY (1.0): Excellent readability (clear naming, structure, comments), efficient algorithms, follows best practices (input validation, robustness, maintainability)\n",
    "        - MEDIUM QUALITY (0.5): Good overall with minor issues in readability, efficiency, or best practices  \n",
    "        - LOW QUALITY (0.0): Significant problems with readability, efficiency, or best practices\n",
    "\n",
    "        Consider: Code clarity, naming, algorithm efficiency, error handling, organization, Python best practices.\n",
    "\n",
    "        Problem: {example.prompt}\n",
    "\n",
    "        {generations_text}\n",
    "\n",
    "        For each generation, provide:\n",
    "        SCORE: [1.0/0.5/0.0]\n",
    "        REASON: [brief explanation]\n",
    "\n",
    "        Then provide the total:\n",
    "        TOTAL SCORE: [sum of all scores]\"\"\"\n",
    "\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert code reviewer. Evaluate each code generation below and return the individual scores and the reasons for the scores and the sum of all generation ratings, e.g., 4.5).\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Extract total score using regex\n",
    "        total_score_match = re.search(r'(?:\\*\\*)?TOTAL SCORE:?\\*?\\*?\\s*(\\d+\\.?\\d*)', response.choices[0].message.content, re.IGNORECASE)\n",
    "        if total_score_match:\n",
    "            total_score = float(total_score_match.group(1)) / self.n\n",
    "        else:\n",
    "            total_score = 0.0\n",
    "        \n",
    "        \n",
    "        self.score = total_score\n",
    "        self.reason = response.choices[0].message.content\n",
    "        \n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35205b8",
   "metadata": {},
   "source": [
    "## Code Generation Function\n",
    "\n",
    "Next, we‚Äôll implement a function that, given a HumanEval problem, queries an LLM to produce a candidate implementation. The function is written with `async/await` so multiple problems can be evaluated in parallel, significantly reducing total runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "async def generate_code(problem: Dict[str, Any]) -> str:\n",
    "    \"\"\"Generate code using LLM for a given HumanEval problem.\"\"\"\n",
    "    prompt = problem[\"prompt\"]\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Python programmer. Write ONLY the Python function code that solves the given problem. Do not include any markdown formatting, explanations, or code blocks. Return only the raw Python code.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    generated_code = response.choices[0].message.content\n",
    "    return generated_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971f166",
   "metadata": {},
   "source": [
    "## Load HumanEval Dataset\n",
    "\n",
    "Now let's load the HumanEval [dataset](https://huggingface.co/datasets/openai/openai_humaneval) from Hugging Face and examine its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dee8c284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading HumanEval dataset...\n",
      "   Found 164 problems\n",
      "\n",
      "üìã Example problem structure:\n",
      "   Task ID: HumanEval/0\n",
      "   Entry Point: has_close_elements\n",
      "   Prompt length: 348 characters\n",
      "   Test length: 531 characters\n",
      "\n",
      "üìù Sample prompt:\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given thr...\n"
     ]
    }
   ],
   "source": [
    "from judgeval.dataset import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"üìä Loading HumanEval dataset...\")\n",
    "dataset = load_dataset(\"openai/openai_humaneval\")\n",
    "print(f\"   Found {len(dataset['test'])} problems\")\n",
    "\n",
    "# Examine the structure of a single problem\n",
    "example_problem = dataset[\"test\"][0]\n",
    "print(\"\\nüìã Example problem structure:\")\n",
    "print(f\"   Task ID: {example_problem['task_id']}\")\n",
    "print(f\"   Entry Point: {example_problem['entry_point']}\")\n",
    "print(f\"   Prompt length: {len(example_problem['prompt'])} characters\")\n",
    "print(f\"   Test length: {len(example_problem['test'])} characters\")\n",
    "\n",
    "print(\"\\nüìù Sample prompt:\")\n",
    "print(example_problem['prompt'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8253bf72",
   "metadata": {},
   "source": [
    "Now, let‚Äôs generate code responses for each problem in the HumanEval benchmark, and we‚Äôll create a `HumanEvalExample` for upload into `judgeval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2856d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Generating code...\n",
      "   Problem 1/5: HumanEval/0\n",
      "   Problem 2/5: HumanEval/1\n",
      "   Problem 3/5: HumanEval/2\n",
      "   Problem 4/5: HumanEval/3\n",
      "   Problem 5/5: HumanEval/4\n",
      "   Problem 6/5: HumanEval/5\n",
      "   Problem 7/5: HumanEval/6\n",
      "   Problem 8/5: HumanEval/7\n",
      "   Problem 9/5: HumanEval/8\n",
      "   Problem 10/5: HumanEval/9\n",
      "   Problem 11/5: HumanEval/10\n",
      "   Problem 12/5: HumanEval/11\n",
      "   Problem 13/5: HumanEval/12\n",
      "   Problem 14/5: HumanEval/13\n",
      "   Problem 15/5: HumanEval/14\n",
      "   Problem 16/5: HumanEval/15\n",
      "   Problem 17/5: HumanEval/16\n",
      "   Problem 18/5: HumanEval/17\n",
      "   Problem 19/5: HumanEval/18\n",
      "   Problem 20/5: HumanEval/19\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "print(\"\\nü§ñ Generating code...\")\n",
    "problems = list(dataset[\"test\"].select(range(20)))\n",
    "\n",
    "num_generations = 5\n",
    "\n",
    "# Generate all code in parallel\n",
    "generations = [\n",
    "    await asyncio.gather(*[generate_code(problem) for _ in range(num_generations)])\n",
    "    for problem in problems\n",
    "]\n",
    "\n",
    "# Create examples\n",
    "examples = []\n",
    "for i, (problem, generated_codes) in enumerate(zip(problems, generations)):\n",
    "    print(f\"   Problem {i+1}/5: {problem['task_id']}\")\n",
    "    \n",
    "    example = HumanEvalExample(\n",
    "        task_id=problem[\"task_id\"],\n",
    "        prompt=problem[\"prompt\"],\n",
    "        canonical_solution=problem[\"canonical_solution\"],\n",
    "        test=problem[\"test\"],\n",
    "        entry_point=problem[\"entry_point\"],\n",
    "        generated_codes=generated_codes\n",
    "    )\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a0bec",
   "metadata": {},
   "source": [
    "We use `Dataset.create()` to create a new dataset and upload it to the [Judgment](https://app.judgmentlabs.ai/app) platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "487e2b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-02 16:47:37 - judgeval - INFO - Successfully created dataset humaneval-dataset!\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.create(\n",
    "    name=\"humaneval-dataset\", \n",
    "    project_name=\"humaneval-project\", \n",
    "    examples=examples,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef3345",
   "metadata": {},
   "source": [
    "We use `Dataset.get()` to retrieve an existing dataset from the [Judgment](https://app.judgmentlabs.ai/app) platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "94ea79b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-02 16:47:38 - judgeval - INFO - Successfully retrieved dataset humaneval-dataset!\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.get(\n",
    "    name=\"humaneval-dataset\",\n",
    "    project_name=\"humaneval-project\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aac9879",
   "metadata": {},
   "source": [
    "## Running Evaluation\n",
    "\n",
    "We then use our custom scorer and the judgment client to run evaluations asynchronously on our servers and display the results on the judgment platform for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49865caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö° Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 20 example(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (20/20) [Time Taken: 00:16,  1.19Example/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "üîç You can view your evaluation results here: <a href=\"https://app.judgmentlabs.ai/org/12b03546-81a4-4f6c-b3cf-4670c9b8548e/project/cc6a4ad8-b66d-4980-bd7c-fdd056c88c71/tests/80d078df-d60f-4792-a99f-616a67ca06bc\" target=\"_blank\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">View Results</span></a>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "üîç You can view your evaluation results here: \u001b]8;id=448871;https://app.judgmentlabs.ai/org/12b03546-81a4-4f6c-b3cf-4670c9b8548e/project/cc6a4ad8-b66d-4980-bd7c-fdd056c88c71/tests/80d078df-d60f-4792-a99f-616a67ca06bc\u001b\\\u001b[38;2;106;0;255mView Results\u001b[0m\u001b]8;;\u001b\\\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1:  \\nSCORE: 1.0  \\nREASON: Clear, readable code with good use of sorting to improve efficiency (O(n log n) vs O(n¬≤)). Variable names are descriptive. No major issues with algorithm or style.\\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: Functionally identical to Gen 1; the subtraction order (sorted_numbers[i+1] - sorted_numbers[i]) is equivalent due to abs(). Also efficient and clear.\\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. Efficient, readable, follows best practices.\\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. No issues.\\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. No issues.\\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='e14752a9-2c23-4cc7-85fb-3ee5e77607db', created_at='2025-10-02T16:47:36.817536+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\n\", prompt='from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n', task_id='HumanEval/0', entry_point='has_close_elements', generated_codes=['def has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    sorted_numbers = sorted(numbers)\\n    for i in range(len(sorted_numbers) - 1):\\n        if abs(sorted_numbers[i] - sorted_numbers[i + 1]) < threshold:\\n            return True\\n    return False', 'def has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    sorted_numbers = sorted(numbers)\\n    for i in range(len(sorted_numbers) - 1):\\n        if abs(sorted_numbers[i+1] - sorted_numbers[i]) < threshold:\\n            return True\\n    return False', 'def has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    sorted_numbers = sorted(numbers)\\n    for i in range(len(sorted_numbers) - 1):\\n        if abs(sorted_numbers[i] - sorted_numbers[i + 1]) < threshold:\\n            return True\\n    return False', 'def has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    sorted_numbers = sorted(numbers)\\n    for i in range(len(sorted_numbers) - 1):\\n        if abs(sorted_numbers[i] - sorted_numbers[i + 1]) < threshold:\\n            return True\\n    return False', 'def has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    sorted_numbers = sorted(numbers)\\n    for i in range(len(sorted_numbers) - 1):\\n        if abs(sorted_numbers[i] - sorted_numbers[i + 1]) < threshold:\\n            return True\\n    return False'], canonical_solution='    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n'), trace_id=None, run_duration=8.413471500040032, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1  \\nSCORE: 1.0  \\nREASON: Clear, well-named variables; concise and idiomatic. Handles the required logic efficiently, ignores spaces up front, and uses the correct algorithm for balanced parentheses without unnecessary complexity.\\n\\n---\\n\\nGeneration 2  \\nSCORE: 1.0  \\nREASON: Nearly identical in structure and quality to Gen 1. Good naming and balance handling. Ignores spaces, clean, and handles the problem requirements directly and efficiently.\\n\\n---\\n\\nGeneration 3  \\nSCORE: 1.0  \\nREASON: Also very similar to Gen 1 and 2. Good structure, appropriately ignores spaces, uses clear variable names, and matches the requirements. No redundant parts, efficient and readable.\\n\\n---\\n\\nGeneration 4  \\nSCORE: 1.0  \\nREASON: Uses a list for current group (slight style difference, but fine), ignores spaces appropriately, and overall structure is clear, readable, and follows best practices. Matches requirements tightly.\\n\\n---\\n\\nGeneration 5  \\nSCORE: 1.0  \\nREASON: Slight difference in skipping spaces within the loop instead of up front (equivalent), but otherwise clear, well-named variables, and follows the required logic precisely and efficiently.\\n\\n---\\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='fdb6ce89-832f-41b1-aebb-0592133975df', created_at='2025-10-02T16:47:36.817604+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('(()()) ((())) () ((())()())') == [\\n        '(()())', '((()))', '()', '((())()())'\\n    ]\\n    assert candidate('() (()) ((())) (((())))') == [\\n        '()', '(())', '((()))', '(((())))'\\n    ]\\n    assert candidate('(()(())((())))') == [\\n        '(()(())((())))'\\n    ]\\n    assert candidate('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\\n\", prompt='from typing import List\\n\\n\\ndef separate_paren_groups(paren_string: str) -> List[str]:\\n    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\\n    separate those group into separate strings and return the list of those.\\n    Separate groups are balanced (each open brace is properly closed) and not nested within each other\\n    Ignore any spaces in the input string.\\n    >>> separate_paren_groups(\\'( ) (( )) (( )( ))\\')\\n    [\\'()\\', \\'(())\\', \\'(()())\\']\\n    \"\"\"\\n', task_id='HumanEval/1', entry_point='separate_paren_groups', generated_codes=[\"def separate_paren_groups(paren_string: str) -> List[str]:\\n    result = []\\n    current = ''\\n    balance = 0\\n    cleaned_str = paren_string.replace(' ', '')\\n    for char in cleaned_str:\\n        if char == '(':\\n            if balance == 0:\\n                current = ''\\n            current += char\\n            balance += 1\\n        elif char == ')':\\n            current += char\\n            balance -= 1\\n            if balance == 0:\\n                result.append(current)\\n    return result\", \"def separate_paren_groups(paren_string: str) -> List[str]:\\n    result = []\\n    current = ''\\n    depth = 0\\n    for char in paren_string.replace(' ', ''):\\n        if char == '(':\\n            if depth == 0:\\n                current = ''\\n            current += char\\n            depth += 1\\n        elif char == ')':\\n            current += char\\n            depth -= 1\\n            if depth == 0:\\n                result.append(current)\\n    return result\", \"def separate_paren_groups(paren_string: str) -> List[str]:\\n    paren_string = paren_string.replace(' ', '')\\n    result = []\\n    current_group = ''\\n    balance = 0\\n    for char in paren_string:\\n        if char == '(':\\n            if balance == 0:\\n                current_group = ''\\n            current_group += char\\n            balance += 1\\n        elif char == ')':\\n            current_group += char\\n            balance -= 1\\n            if balance == 0:\\n                result.append(current_group)\\n    return result\", 'def separate_paren_groups(paren_string: str) -> List[str]:\\n    result = []\\n    current = []\\n    count = 0\\n    for char in paren_string.replace(\" \", \"\"):\\n        if char == \\'(\\':\\n            if count == 0:\\n                current = []\\n            current.append(char)\\n            count += 1\\n        elif char == \\')\\':\\n            current.append(char)\\n            count -= 1\\n            if count == 0:\\n                result.append(\\'\\'.join(current))\\n    return result', \"def separate_paren_groups(paren_string: str) -> List[str]:\\n    result = []\\n    current = []\\n    count = 0\\n    for char in paren_string:\\n        if char == ' ':\\n            continue\\n        if char == '(':\\n            if count == 0:\\n                current = []\\n            count += 1\\n            current.append(char)\\n        elif char == ')':\\n            current.append(char)\\n            count -= 1\\n            if count == 0:\\n                result.append(''.join(current))\\n    return result\"], canonical_solution=\"    result = []\\n    current_string = []\\n    current_depth = 0\\n\\n    for c in paren_string:\\n        if c == '(':\\n            current_depth += 1\\n            current_string.append(c)\\n        elif c == ')':\\n            current_depth -= 1\\n            current_string.append(c)\\n\\n            if current_depth == 0:\\n                result.append(''.join(current_string))\\n                current_string.clear()\\n\\n    return result\\n\"), trace_id=None, run_duration=9.389061958994716, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1:  \\nSCORE: 1.0  \\nREASON: Code is concise, readable, uses clear naming, and solves the specified problem efficiently. Meets best practices for this simple task.\\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. Maintains clarity and efficiency.\\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Identical to previous generations. Good readability and correctness.\\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: Again, identical in implementation. Meets all criteria for high quality.\\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Same efficient and clear solution as the others.\\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='e53e6c3e-57cd-4268-b380-3af09caf6046', created_at='2025-10-02T16:47:36.817626+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate(3.5) == 0.5\\n    assert abs(candidate(1.33) - 0.33) < 1e-6\\n    assert abs(candidate(123.456) - 0.456) < 1e-6\\n\", prompt='\\n\\ndef truncate_number(number: float) -> float:\\n    \"\"\" Given a positive floating point number, it can be decomposed into\\n    and integer part (largest integer smaller than given number) and decimals\\n    (leftover part always smaller than 1).\\n\\n    Return the decimal part of the number.\\n    >>> truncate_number(3.5)\\n    0.5\\n    \"\"\"\\n', task_id='HumanEval/2', entry_point='truncate_number', generated_codes=['def truncate_number(number: float) -> float:\\n    return number - int(number)', 'def truncate_number(number: float) -> float:\\n    return number - int(number)', 'def truncate_number(number: float) -> float:\\n    return number - int(number)', 'def truncate_number(number: float) -> float:\\n    return number - int(number)', 'def truncate_number(number: float) -> float:\\n    return number - int(number)'], canonical_solution='    return number % 1.0\\n'), trace_id=None, run_duration=6.660239457967691, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1:  \\nSCORE: 1.0  \\nREASON: Clear, concise, and correct implementation; uses descriptive variable names, correct logic, and is readable. Conforms with Python best practices and requires no further improvement for this context.\\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. Maintains excellent readability, correctness, and follows best practices.\\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. Excellent readability and correctness.\\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. Maintains all guidelines for high-quality code.\\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. High clarity, correctness, and best practice adherence.\\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='65ef3f58-fd4f-4d8c-9b1e-e357218ae399', created_at='2025-10-02T16:47:36.817641+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([]) == False\\n    assert candidate([1, 2, -3, 1, 2, -3]) == False\\n    assert candidate([1, 2, -4, 5, 6]) == True\\n    assert candidate([1, -1, 2, -2, 5, -5, 4, -4]) == False\\n    assert candidate([1, -1, 2, -2, 5, -5, 4, -5]) == True\\n    assert candidate([1, -2, 2, -2, 5, -5, 4, -4]) == True\\n\", prompt='from typing import List\\n\\n\\ndef below_zero(operations: List[int]) -> bool:\\n    \"\"\" You\\'re given a list of deposit and withdrawal operations on a bank account that starts with\\n    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\\n    at that point function should return True. Otherwise it should return False.\\n    >>> below_zero([1, 2, 3])\\n    False\\n    >>> below_zero([1, 2, -4, 5])\\n    True\\n    \"\"\"\\n', task_id='HumanEval/3', entry_point='below_zero', generated_codes=['def below_zero(operations: List[int]) -> bool:\\n    balance = 0\\n    for op in operations:\\n        balance += op\\n        if balance < 0:\\n            return True\\n    return False', 'def below_zero(operations: List[int]) -> bool:\\n    balance = 0\\n    for op in operations:\\n        balance += op\\n        if balance < 0:\\n            return True\\n    return False', 'def below_zero(operations: List[int]) -> bool:\\n    balance = 0\\n    for op in operations:\\n        balance += op\\n        if balance < 0:\\n            return True\\n    return False', 'def below_zero(operations: List[int]) -> bool:\\n    balance = 0\\n    for op in operations:\\n        balance += op\\n        if balance < 0:\\n            return True\\n    return False', 'def below_zero(operations: List[int]) -> bool:\\n    balance = 0\\n    for op in operations:\\n        balance += op\\n        if balance < 0:\\n            return True\\n    return False'], canonical_solution='    balance = 0\\n\\n    for op in operations:\\n        balance += op\\n        if balance < 0:\\n            return True\\n\\n    return False\\n'), trace_id=None, run_duration=8.369998542009853, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=0.8, reason='Generation 1:\\nSCORE: 1.0  \\nREASON: Good naming, handles empty list input (robustness), clear structure, efficient, follows best practices.\\n\\nGeneration 2:\\nSCORE: 0.5  \\nREASON: Does not handle empty list (raises ZeroDivisionError), otherwise clear and efficient.\\n\\nGeneration 3:\\nSCORE: 1.0  \\nREASON: Good naming, handles empty input, clear list comprehension for readability, follows best practices.\\n\\nGeneration 4:\\nSCORE: 0.5  \\nREASON: Does not handle empty list (ZeroDivisionError), but is clear and otherwise correct and idiomatic.\\n\\nGeneration 5:\\nSCORE: 1.0  \\nREASON: Handles empty list input, concise and efficient, clear naming and structure, follows best practices.\\n\\nTOTAL SCORE: 4.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='ff125a42-2681-4867-b514-1b0f1f6dea1f', created_at='2025-10-02T16:47:36.817654+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert abs(candidate([1.0, 2.0, 3.0]) - 2.0/3.0) < 1e-6\\n    assert abs(candidate([1.0, 2.0, 3.0, 4.0]) - 1.0) < 1e-6\\n    assert abs(candidate([1.0, 2.0, 3.0, 4.0, 5.0]) - 6.0/5.0) < 1e-6\\n\\n\", prompt='from typing import List\\n\\n\\ndef mean_absolute_deviation(numbers: List[float]) -> float:\\n    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\\n    around the mean of this dataset.\\n    Mean Absolute Deviation is the average absolute difference between each\\n    element and a centerpoint (mean in this case):\\n    MAD = average | x - x_mean |\\n    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\\n    1.0\\n    \"\"\"\\n', task_id='HumanEval/4', entry_point='mean_absolute_deviation', generated_codes=['def mean_absolute_deviation(numbers: List[float]) -> float:\\n    if not numbers:\\n        return 0.0\\n    x_mean = sum(numbers) / len(numbers)\\n    total_deviation = sum(abs(x - x_mean) for x in numbers)\\n    return total_deviation / len(numbers)', 'def mean_absolute_deviation(numbers: List[float]) -> float:\\n    x_mean = sum(numbers) / len(numbers)\\n    return sum(abs(x - x_mean) for x in numbers) / len(numbers)', 'def mean_absolute_deviation(numbers: List[float]) -> float:\\n    if not numbers:\\n        return 0.0\\n    mean_value = sum(numbers) / len(numbers)\\n    absolute_diffs = [abs(x - mean_value) for x in numbers]\\n    return sum(absolute_diffs) / len(numbers)', 'def mean_absolute_deviation(numbers: List[float]) -> float:\\n    mean_value = sum(numbers) / len(numbers)\\n    absolute_deviations = [abs(x - mean_value) for x in numbers]\\n    return sum(absolute_deviations) / len(absolute_deviations)', 'def mean_absolute_deviation(numbers: List[float]) -> float:\\n    if not numbers:\\n        return 0.0\\n    mean_value = sum(numbers) / len(numbers)\\n    return sum(abs(x - mean_value) for x in numbers) / len(numbers)'], canonical_solution='    mean = sum(numbers) / len(numbers)\\n    return sum(abs(x - mean) for x in numbers) / len(numbers)\\n'), trace_id=None, run_duration=7.016100166016258, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='**Generation 1**  \\nSCORE: 1.0  \\nREASON: Clear, concise implementation with good variable naming, efficient algorithm without unnecessary complexity, handles edge cases correctly, and follows Python best practices.\\n\\n---\\n\\n**Generation 2**  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. Clear, concise, efficient, idiomatic Python, correct handling of all cases.\\n\\n---\\n\\n**Generation 3**  \\nSCORE: 1.0  \\nREASON: Good clarity and variable naming. Uses a different approach but is still efficient and correct. Handles the empty list case explicitly, which helps clarity and robustness. Follows best practices.\\n\\n---\\n\\n**Generation 4**  \\nSCORE: 1.0  \\nREASON: Direct duplicate of Generation 1 and 2. Same reasoning: clear, concise, efficient, best practices.\\n\\n---\\n\\n**Generation 5**  \\nSCORE: 1.0  \\nREASON: Slightly different approach using indexing, but still clear, efficient, and handles edge cases properly. Correct result for empty input, uses readable variable names, and maintains maintainability.\\n\\n---\\n\\n**TOTAL SCORE:** 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='fb7da3a9-4687-402c-b798-1c2c93781e30', created_at='2025-10-02T16:47:36.817668+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([], 7) == []\\n    assert candidate([5, 6, 3, 2], 8) == [5, 8, 6, 8, 3, 8, 2]\\n    assert candidate([2, 2, 2], 2) == [2, 2, 2, 2, 2]\\n\", prompt='from typing import List\\n\\n\\ndef intersperse(numbers: List[int], delimeter: int) -> List[int]:\\n    \"\"\" Insert a number \\'delimeter\\' between every two consecutive elements of input list `numbers\\'\\n    >>> intersperse([], 4)\\n    []\\n    >>> intersperse([1, 2, 3], 4)\\n    [1, 4, 2, 4, 3]\\n    \"\"\"\\n', task_id='HumanEval/5', entry_point='intersperse', generated_codes=['def intersperse(numbers: List[int], delimeter: int) -> List[int]:\\n    result = []\\n    for i, num in enumerate(numbers):\\n        if i > 0:\\n            result.append(delimeter)\\n        result.append(num)\\n    return result', 'def intersperse(numbers: List[int], delimeter: int) -> List[int]:\\n    result = []\\n    for i, num in enumerate(numbers):\\n        if i > 0:\\n            result.append(delimeter)\\n        result.append(num)\\n    return result', 'def intersperse(numbers: List[int], delimeter: int) -> List[int]:\\n    if not numbers:\\n        return []\\n    result = [numbers[0]]\\n    for num in numbers[1:]:\\n        result.append(delimeter)\\n        result.append(num)\\n    return result', 'def intersperse(numbers: List[int], delimeter: int) -> List[int]:\\n    result = []\\n    for i, num in enumerate(numbers):\\n        if i > 0:\\n            result.append(delimeter)\\n        result.append(num)\\n    return result', 'def intersperse(numbers: List[int], delimeter: int) -> List[int]:\\n    if not numbers:\\n        return []\\n    result = []\\n    for i in range(len(numbers) - 1):\\n        result.append(numbers[i])\\n        result.append(delimeter)\\n    result.append(numbers[-1])\\n    return result'], canonical_solution='    if not numbers:\\n        return []\\n\\n    result = []\\n\\n    for n in numbers[:-1]:\\n        result.append(n)\\n        result.append(delimeter)\\n\\n    result.append(numbers[-1])\\n\\n    return result\\n'), trace_id=None, run_duration=7.979887209017761, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=0.8, reason='Passed 4 out of 5 tests.\\n\\nFailing snippets:\\nfailed: ', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=0.8, reason='Generation 1  \\nSCORE: 1.0  \\nREASON: Clear variable naming, correct splitting and iteration, efficient and idiomatic algorithm, no major redundancy, well-organized, and follows Python best practices. Edge cases (e.g. invalid input) are not handled, but that is consistent with the example prompt.\\n\\n---\\n\\nGeneration 2  \\nSCORE: 1.0  \\nREASON: Essentially equivalent to Generation 1 but with slightly different variable names. Well-structured, comprehensible, efficient, and follows best practices. Robust to the specified kind of input. No significant issues.\\n\\n---\\n\\nGeneration 3  \\nSCORE: 0.0  \\nREASON: Obscure one-liner sacrifices readability and correctness. The logic appears unnecessarily convoluted and incorrect for this problem (the use of max(map(...)) + sum(...) - 1 is not a valid nesting calculation). Fails on code clarity and algorithm correctness.\\n\\n---\\n\\nGeneration 4  \\nSCORE: 1.0  \\nREASON: Same logic as Generations 1 and 2, clear and readable structure, correct approach, and maintains consistency with best practices.\\n\\n---\\n\\nGeneration 5  \\nSCORE: 1.0  \\nREASON: Similar to Generations 1, 2, and 4; possibly the clearest as it combines the loop logic concisely. Readable, efficient, and correct.\\n\\n---\\n\\nTOTAL SCORE: 4.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='c4ca8e83-89df-48b9-a055-d5dd2e30293c', created_at='2025-10-02T16:47:36.81768+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('(()()) ((())) () ((())()())') == [2, 3, 1, 3]\\n    assert candidate('() (()) ((())) (((())))') == [1, 2, 3, 4]\\n    assert candidate('(()(())((())))') == [4]\\n\", prompt='from typing import List\\n\\n\\ndef parse_nested_parens(paren_string: str) -> List[int]:\\n    \"\"\" Input to this function is a string represented multiple groups for nested parentheses separated by spaces.\\n    For each of the group, output the deepest level of nesting of parentheses.\\n    E.g. (()()) has maximum two levels of nesting while ((())) has three.\\n\\n    >>> parse_nested_parens(\\'(()()) ((())) () ((())()())\\')\\n    [2, 3, 1, 3]\\n    \"\"\"\\n', task_id='HumanEval/6', entry_point='parse_nested_parens', generated_codes=[\"def parse_nested_parens(paren_string: str) -> List[int]:\\n    results = []\\n    groups = paren_string.split()\\n    for group in groups:\\n        max_depth = 0\\n        current_depth = 0\\n        for char in group:\\n            if char == '(':\\n                current_depth += 1\\n                max_depth = max(max_depth, current_depth)\\n            elif char == ')':\\n                current_depth -= 1\\n        results.append(max_depth)\\n    return results\", \"def parse_nested_parens(paren_string: str) -> List[int]:\\n    result = []\\n    groups = paren_string.split()\\n    for group in groups:\\n        max_depth = 0\\n        current_depth = 0\\n        for char in group:\\n            if char == '(':\\n                current_depth += 1\\n                if current_depth > max_depth:\\n                    max_depth = current_depth\\n            elif char == ')':\\n                current_depth -= 1\\n        result.append(max_depth)\\n    return result\", \"def parse_nested_parens(paren_string: str) -> List[int]:\\n    return [max(map(lambda c: 1 if c == '(' else -1, group := part)) + sum(1 for c in group if c == '(') - 1 for part in paren_string.split()]\", \"def parse_nested_parens(paren_string: str) -> List[int]:\\n    result = []\\n    groups = paren_string.split()\\n    for group in groups:\\n        max_depth = 0\\n        current_depth = 0\\n        for char in group:\\n            if char == '(':\\n                current_depth += 1\\n                if current_depth > max_depth:\\n                    max_depth = current_depth\\n            elif char == ')':\\n                current_depth -= 1\\n        result.append(max_depth)\\n    return result\", \"def parse_nested_parens(paren_string: str) -> List[int]:\\n    result = []\\n    for group in paren_string.split():\\n        max_depth = 0\\n        current_depth = 0\\n        for char in group:\\n            if char == '(':\\n                current_depth += 1\\n                if current_depth > max_depth:\\n                    max_depth = current_depth\\n            elif char == ')':\\n                current_depth -= 1\\n        result.append(max_depth)\\n    return result\"], canonical_solution=\"    def parse_paren_group(s):\\n        depth = 0\\n        max_depth = 0\\n        for c in s:\\n            if c == '(':\\n                depth += 1\\n                max_depth = max(depth, max_depth)\\n            else:\\n                depth -= 1\\n\\n        return max_depth\\n\\n    return [parse_paren_group(x) for x in paren_string.split(' ') if x]\\n\"), trace_id=None, run_duration=8.291621749987826, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='**Generation 1:**  \\nSCORE: 1.0  \\nREASON: The implementation is correct, concise, and directly matches the problem description and docstring. It uses a clear list comprehension, names are appropriate, and the code is optimal for this use-case. No redundancy or issues.  \\n\\n**Generation 2:**  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. Maintains clarity, efficiency, and adherence to best practices.  \\n\\n**Generation 3:**  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1 and 2. Correct, simple, and efficient.  \\n\\n**Generation 4:**  \\nSCORE: 1.0  \\nREASON: Same as above‚Äîno changes or issues. Clear and robust for the stated problem.  \\n\\n**Generation 5:**  \\nSCORE: 1.0  \\nREASON: Also identical to all above. Admirable simplicity and correctness.  \\n\\n**TOTAL SCORE: 5.0**', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='ee9c3b19-14c5-45b0-ad39-fe4db4e202db', created_at='2025-10-02T16:47:36.817693+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([], 'john') == []\\n    assert candidate(['xxx', 'asd', 'xxy', 'john doe', 'xxxAAA', 'xxx'], 'xxx') == ['xxx', 'xxxAAA', 'xxx']\\n    assert candidate(['xxx', 'asd', 'aaaxxy', 'john doe', 'xxxAAA', 'xxx'], 'xx') == ['xxx', 'aaaxxy', 'xxxAAA', 'xxx']\\n    assert candidate(['grunt', 'trumpet', 'prune', 'gruesome'], 'run') == ['grunt', 'prune']\\n\", prompt='from typing import List\\n\\n\\ndef filter_by_substring(strings: List[str], substring: str) -> List[str]:\\n    \"\"\" Filter an input list of strings only for ones that contain given substring\\n    >>> filter_by_substring([], \\'a\\')\\n    []\\n    >>> filter_by_substring([\\'abc\\', \\'bacd\\', \\'cde\\', \\'array\\'], \\'a\\')\\n    [\\'abc\\', \\'bacd\\', \\'array\\']\\n    \"\"\"\\n', task_id='HumanEval/7', entry_point='filter_by_substring', generated_codes=['def filter_by_substring(strings: List[str], substring: str) -> List[str]:\\n    return [s for s in strings if substring in s]', 'def filter_by_substring(strings: List[str], substring: str) -> List[str]:\\n    return [s for s in strings if substring in s]', 'def filter_by_substring(strings: List[str], substring: str) -> List[str]:\\n    return [s for s in strings if substring in s]', 'def filter_by_substring(strings: List[str], substring: str) -> List[str]:\\n    return [s for s in strings if substring in s]', 'def filter_by_substring(strings: List[str], substring: str) -> List[str]:\\n    return [s for s in strings if substring in s]'], canonical_solution='    return [x for x in strings if substring in x]\\n'), trace_id=None, run_duration=5.247420790954493, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='**Generation 1**  \\nSCORE: 1.0  \\nREASON: Clear, concise, and idiomatic code with good variable naming. Handles empty input as desired. Algorithm is efficient and readable. No unnecessary complexity.\\n\\n---\\n\\n**Generation 2**  \\nSCORE: 1.0  \\nREASON: Essentially the same as Generation 1, with a minor change in variable name to `product`. Code is readable, idiomatic, efficient, and correct.\\n\\n---\\n\\n**Generation 3**  \\nSCORE: 1.0  \\nREASON: Identical in structure and logic to Generation 1. Meets all requirements for clarity, best practices, and functionality.\\n\\n---\\n\\n**Generation 4**  \\nSCORE: 1.0  \\nREASON: Same approach as the previous answers. Readable, efficient, and matches the requirements exactly.\\n\\n---\\n\\n**Generation 5**  \\nSCORE: 1.0  \\nREASON: Copies the structure of Generation 2 and 4. Clear, correct, maintains Pythonic style, and fulfills all functional requirements.\\n\\n---\\n\\n**TOTAL SCORE:** 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='b2082979-948c-4fd0-b1fc-a3a2258657a7', created_at='2025-10-02T16:47:36.817705+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([]) == (0, 1)\\n    assert candidate([1, 1, 1]) == (3, 1)\\n    assert candidate([100, 0]) == (100, 0)\\n    assert candidate([3, 5, 7]) == (3 + 5 + 7, 3 * 5 * 7)\\n    assert candidate([10]) == (10, 10)\\n\", prompt='from typing import List, Tuple\\n\\n\\ndef sum_product(numbers: List[int]) -> Tuple[int, int]:\\n    \"\"\" For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.\\n    Empty sum should be equal to 0 and empty product should be equal to 1.\\n    >>> sum_product([])\\n    (0, 1)\\n    >>> sum_product([1, 2, 3, 4])\\n    (10, 24)\\n    \"\"\"\\n', task_id='HumanEval/8', entry_point='sum_product', generated_codes=['def sum_product(numbers: List[int]) -> Tuple[int, int]:\\n    total_sum = sum(numbers)\\n    total_product = 1\\n    for num in numbers:\\n        total_product *= num\\n    return total_sum, total_product', 'def sum_product(numbers: List[int]) -> Tuple[int, int]:\\n    total_sum = sum(numbers)\\n    product = 1\\n    for num in numbers:\\n        product *= num\\n    return (total_sum, product)', 'def sum_product(numbers: List[int]) -> Tuple[int, int]:\\n    total_sum = sum(numbers)\\n    total_product = 1\\n    for num in numbers:\\n        total_product *= num\\n    return total_sum, total_product', 'def sum_product(numbers: List[int]) -> Tuple[int, int]:\\n    total_sum = sum(numbers)\\n    product = 1\\n    for num in numbers:\\n        product *= num\\n    return (total_sum, product)', 'def sum_product(numbers: List[int]) -> Tuple[int, int]:\\n    total_sum = sum(numbers)\\n    product = 1\\n    for num in numbers:\\n        product *= num\\n    return (total_sum, product)'], canonical_solution='    sum_value = 0\\n    prod_value = 1\\n\\n    for n in numbers:\\n        sum_value += n\\n        prod_value *= n\\n    return sum_value, prod_value\\n'), trace_id=None, run_duration=5.582799792056903, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason=\"Generation 1:  \\nSCORE: 1.0  \\nREASON: The code is clear, uses descriptive variable names, follows Python best practices, and has correct and efficient logic. The use of float('-inf') ensures robustness with negative numbers.  \\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1 in structure, clarity, and best practices.  \\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Identical logic and structure as previous generations, clear and robust, follows best practices.  \\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: Equivalent logic, simply uses 'current_max' for naming. Code maintains readability, robustness, and efficiency.  \\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Identical to Generations 1-3. Well-structured, clear, and robust.  \\n\\nTOTAL SCORE: 5.0\", strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='1367f1af-3b55-4aab-9772-7132c6a1393a', created_at='2025-10-02T16:47:36.817719+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([]) == []\\n    assert candidate([1, 2, 3, 4]) == [1, 2, 3, 4]\\n    assert candidate([4, 3, 2, 1]) == [4, 4, 4, 4]\\n    assert candidate([3, 2, 3, 100, 3]) == [3, 3, 3, 100, 100]\\n\", prompt='from typing import List, Tuple\\n\\n\\ndef rolling_max(numbers: List[int]) -> List[int]:\\n    \"\"\" From a given list of integers, generate a list of rolling maximum element found until given moment\\n    in the sequence.\\n    >>> rolling_max([1, 2, 3, 2, 3, 4, 2])\\n    [1, 2, 3, 3, 3, 4, 4]\\n    \"\"\"\\n', task_id='HumanEval/9', entry_point='rolling_max', generated_codes=[\"def rolling_max(numbers: List[int]) -> List[int]:\\r\\n    max_so_far = float('-inf')\\r\\n    result = []\\r\\n    for num in numbers:\\r\\n        if num > max_so_far:\\r\\n            max_so_far = num\\r\\n        result.append(max_so_far)\\r\\n    return result\", \"def rolling_max(numbers: List[int]) -> List[int]:\\n    max_so_far = float('-inf')\\n    result = []\\n    for num in numbers:\\n        if num > max_so_far:\\n            max_so_far = num\\n        result.append(max_so_far)\\n    return result\", \"def rolling_max(numbers: List[int]) -> List[int]:\\n    max_so_far = float('-inf')\\n    result = []\\n    for num in numbers:\\n        if num > max_so_far:\\n            max_so_far = num\\n        result.append(max_so_far)\\n    return result\", \"def rolling_max(numbers: List[int]) -> List[int]:\\n    result = []\\n    current_max = float('-inf')\\n    for num in numbers:\\n        if num > current_max:\\n            current_max = num\\n        result.append(current_max)\\n    return result\", \"def rolling_max(numbers: List[int]) -> List[int]:\\n    max_so_far = float('-inf')\\n    result = []\\n    for num in numbers:\\n        if num > max_so_far:\\n            max_so_far = num\\n        result.append(max_so_far)\\n    return result\"], canonical_solution='    running_max = None\\n    result = []\\n\\n    for n in numbers:\\n        if running_max is None:\\n            running_max = n\\n        else:\\n            running_max = max(running_max, n)\\n\\n        result.append(running_max)\\n\\n    return result\\n'), trace_id=None, run_duration=4.5716515419771895, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=0.9, reason=\"Let's evaluate each generation:\\n\\n---\\n\\n**Generation 1:**\\n```python\\ndef make_palindrome(string: str) -> str:\\n    if not string:\\n        return ''\\n    for i in range(len(string)):\\n        suffix = string[i:]\\n        if is_palindrome(suffix):\\n            prefix = string[:i]\\n            return string + prefix[::-1]\\n    return string + string[:-1][::-1]\\n```\\n**SCORE: 1.0**\\n\\n**REASON:**  \\n- Readable variable names, clear structure, and uses helper function `is_palindrome`.\\n- Algorithm matches the intended description and examples.\\n- Handles empty case.\\n- Correct and efficient.\\n- Follows Python best practices.\\n\\n---\\n\\n**Generation 2:**\\n```python\\ndef make_palindrome(string: str) -> str:\\n    if not string:\\n        return ''\\n    for i in range(len(string)):\\n        if is_palindrome(string[i:]):\\n            return string + string[:i][::-1]\\n    return string + string[:-1][::-1]\\n```\\n**SCORE: 1.0**\\n\\n**REASON:**  \\n- Naming and structure are clear.\\n- Logic is equivalent to Generation 1 but more concise (doesn't name intermediate variables, but all logic is valid and correct, matches all examples).\\n- Maintains readability and efficiency.\\n- Follows best practices.\\n\\n---\\n\\n**Generation 3:**\\n```python\\ndef make_palindrome(string: str) -> str:\\n    for i in range(len(string) + 1):\\n        suffix = string[i:]\\n        if is_palindrome(suffix):\\n            prefix = string[:i]\\n            return string + prefix[::-1]\\n    return string + string[:-1][::-1]  # fallback, though should not reach here\\n```\\n**SCORE: 0.5**\\n\\n**REASON:**\\n- The loop goes from `0` to `len(string)`, inclusive, so for empty strings it will enter once and return immediately. However, if `string` is empty, `is_palindrome('')` is `True`, returning `''`, so it works.\\n- However, lack of explicit handling for empty string (`if not string: return ''`) makes it less clear and maintainable.\\n- Also, the fallback return is unnecessary ‚Äî the loop will always return for any valid input, so the fallback is dead code.\\n- Slightly unidiomatic, but otherwise logic is correct. Minor clarity/maintainability issue.\\n\\n---\\n\\n**Generation 4:**\\n```python\\ndef make_palindrome(string: str) -> str:\\n    if not string:\\n        return ''\\n    for i in range(len(string)):\\n        suffix = string[i:]\\n        if is_palindrome(suffix):\\n            prefix = string[:i]\\n            return string + prefix[::-1]\\n    return string + string[:-1][::-1]\\n```\\n**SCORE: 1.0**\\n\\n**REASON:**  \\n- Identical to Generation 1 ‚Äî all the same positives apply (clarity, efficiency, best practices).\\n- Explicitly handles empty string.\\n- Readable, maintainable, and correct.\\n\\n---\\n\\n**Generation 5:**\\n```python\\ndef make_palindrome(string: str) -> str:\\n    if not string:\\n        return ''\\n    for i in range(len(string)):\\n        suffix = string[i:]\\n        if is_palindrome(suffix):\\n            prefix = string[:i]\\n            return string + prefix[::-1]\\n    return string + string[:-1][::-1]\\n```\\n**SCORE: 1.0**\\n\\n**REASON:**  \\n- Exactly the same as Generation 4 and 1.\\n- Fully correct, clear, and robust.\\n\\n---\\n\\n**TOTAL SCORE: 4.5**\", strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='f4a4caf7-b18e-4c5b-8a7d-8b3f8f6af37f', created_at='2025-10-02T16:47:36.817732+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('') == ''\\n    assert candidate('x') == 'x'\\n    assert candidate('xyz') == 'xyzyx'\\n    assert candidate('xyx') == 'xyx'\\n    assert candidate('jerry') == 'jerryrrej'\\n\", prompt='\\n\\ndef is_palindrome(string: str) -> bool:\\n    \"\"\" Test if given string is a palindrome \"\"\"\\n    return string == string[::-1]\\n\\n\\ndef make_palindrome(string: str) -> str:\\n    \"\"\" Find the shortest palindrome that begins with a supplied string.\\n    Algorithm idea is simple:\\n    - Find the longest postfix of supplied string that is a palindrome.\\n    - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.\\n    >>> make_palindrome(\\'\\')\\n    \\'\\'\\n    >>> make_palindrome(\\'cat\\')\\n    \\'catac\\'\\n    >>> make_palindrome(\\'cata\\')\\n    \\'catac\\'\\n    \"\"\"\\n', task_id='HumanEval/10', entry_point='make_palindrome', generated_codes=[\"def make_palindrome(string: str) -> str:\\n    if not string:\\n        return ''\\n    for i in range(len(string)):\\n        suffix = string[i:]\\n        if is_palindrome(suffix):\\n            prefix = string[:i]\\n            return string + prefix[::-1]\\n    return string + string[:-1][::-1]\", \"def make_palindrome(string: str) -> str:\\n    if not string:\\n        return ''\\n    for i in range(len(string)):\\n        if is_palindrome(string[i:]):\\n            return string + string[:i][::-1]\\n    return string + string[:-1][::-1]\", 'def make_palindrome(string: str) -> str:\\n    for i in range(len(string) + 1):\\n        suffix = string[i:]\\n        if is_palindrome(suffix):\\n            prefix = string[:i]\\n            return string + prefix[::-1]\\n    return string + string[:-1][::-1]  # fallback, though should not reach here', \"def make_palindrome(string: str) -> str:\\n    if not string:\\n        return ''\\n    for i in range(len(string)):\\n        suffix = string[i:]\\n        if is_palindrome(suffix):\\n            prefix = string[:i]\\n            return string + prefix[::-1]\\n    return string + string[:-1][::-1]\", \"def make_palindrome(string: str) -> str:\\n    if not string:\\n        return ''\\n    for i in range(len(string)):\\n        suffix = string[i:]\\n        if is_palindrome(suffix):\\n            prefix = string[:i]\\n            return string + prefix[::-1]\\n    return string + string[:-1][::-1]\"], canonical_solution=\"    if not string:\\n        return ''\\n\\n    beginning_of_suffix = 0\\n\\n    while not is_palindrome(string[beginning_of_suffix:]):\\n        beginning_of_suffix += 1\\n\\n    return string + string[:beginning_of_suffix][::-1]\\n\"), trace_id=None, run_duration=8.359893457964063, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1:  \\nSCORE: 1.0  \\nREASON: Uses clear logic, readable naming, efficient one-line list comprehension, and zip for pairing. Follows Python best practices for this kind of operation. The code matches the docstring and sample input/output.  \\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: Correctly uses bitwise XOR operator on integers after converting bits, which is clear and efficient. Code is compact, readable, and functionally correct, matching the provided requirements.  \\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Logic and structure identical to Generation 1. Efficient and clear code, using best practices.  \\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 3. Code is clear, accurate, and efficient. No issues.  \\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 4. Code remains robust, readable, and efficient.  \\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='59437feb-1acf-42a3-b926-670c86a1b418', created_at='2025-10-02T16:47:36.817745+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('111000', '101010') == '010010'\\n    assert candidate('1', '1') == '0'\\n    assert candidate('0101', '0000') == '0101'\\n\", prompt='from typing import List\\n\\n\\ndef string_xor(a: str, b: str) -> str:\\n    \"\"\" Input are two strings a and b consisting only of 1s and 0s.\\n    Perform binary XOR on these inputs and return result also as a string.\\n    >>> string_xor(\\'010\\', \\'110\\')\\n    \\'100\\'\\n    \"\"\"\\n', task_id='HumanEval/11', entry_point='string_xor', generated_codes=[\"def string_xor(a: str, b: str) -> str:\\n    return ''.join('1' if bit_a != bit_b else '0' for bit_a, bit_b in zip(a, b))\", \"def string_xor(a: str, b: str) -> str:\\n    return ''.join(str(int(x) ^ int(y)) for x, y in zip(a, b))\", \"def string_xor(a: str, b: str) -> str:\\n    return ''.join('1' if x != y else '0' for x, y in zip(a, b))\", \"def string_xor(a: str, b: str) -> str:\\n    return ''.join('1' if x != y else '0' for x, y in zip(a, b))\", \"def string_xor(a: str, b: str) -> str:\\n    return ''.join('1' if x != y else '0' for x, y in zip(a, b))\"], canonical_solution=\"    def xor(i, j):\\n        if i == j:\\n            return '0'\\n        else:\\n            return '1'\\n\\n    return ''.join(xor(x, y) for x, y in zip(a, b))\\n\"), trace_id=None, run_duration=3.959649124997668, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1:\\nSCORE: 1.0  \\nREASON: Clear, concise, and idiomatic Python using max with key=len. Handles empty input robustly by returning None. Follows best practices and has good readability.\\n\\nGeneration 2:\\nSCORE: 1.0  \\nREASON: Identical to Generation 1; same clear, efficient approach and robustness.\\n\\nGeneration 3:\\nSCORE: 1.0  \\nREASON: Identical to above. Correctly solves the problem in a readable, efficient way.\\n\\nGeneration 4:\\nSCORE: 1.0  \\nREASON: Identical solution, equally high quality.\\n\\nGeneration 5:\\nSCORE: 1.0  \\nREASON: Same as previous generations: clean, idiomatic, robust.\\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='651d1409-8dcc-4b69-bf4a-5fd15bfb3a05', created_at='2025-10-02T16:47:36.817768+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([]) == None\\n    assert candidate(['x', 'y', 'z']) == 'x'\\n    assert candidate(['x', 'yyy', 'zzzz', 'www', 'kkkk', 'abc']) == 'zzzz'\\n\", prompt='from typing import List, Optional\\n\\n\\ndef longest(strings: List[str]) -> Optional[str]:\\n    \"\"\" Out of list of strings, return the longest one. Return the first one in case of multiple\\n    strings of the same length. Return None in case the input list is empty.\\n    >>> longest([])\\n\\n    >>> longest([\\'a\\', \\'b\\', \\'c\\'])\\n    \\'a\\'\\n    >>> longest([\\'a\\', \\'bb\\', \\'ccc\\'])\\n    \\'ccc\\'\\n    \"\"\"\\n', task_id='HumanEval/12', entry_point='longest', generated_codes=['def longest(strings: List[str]) -> Optional[str]:\\n    if not strings:\\n        return None\\n    return max(strings, key=len)', 'def longest(strings: List[str]) -> Optional[str]:\\n    if not strings:\\n        return None\\n    return max(strings, key=len)', 'def longest(strings: List[str]) -> Optional[str]:\\n    if not strings:\\n        return None\\n    return max(strings, key=len)', 'def longest(strings: List[str]) -> Optional[str]:\\n    if not strings:\\n        return None\\n    return max(strings, key=len)', 'def longest(strings: List[str]) -> Optional[str]:\\n    if not strings:\\n        return None\\n    return max(strings, key=len)'], canonical_solution='    if not strings:\\n        return None\\n\\n    maxlen = max(len(x) for x in strings)\\n    for s in strings:\\n        if len(s) == maxlen:\\n            return s\\n'), trace_id=None, run_duration=3.9584779168944806, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=0.6, reason='Generation 1:  \\nSCORE: 0.5  \\nREASON: Correct Euclidean algorithm is used; code is concise and readable. However, it does not handle negative inputs. According to mathematical convention, gcd should be always non-negative, e.g., gcd(-3, 5) == 1. Also, does not validate that inputs are integers.\\n\\n---\\n\\nGeneration 2:  \\nSCORE: 0.5  \\nREASON: Identical to Generation 1. Lacks handling of negative numbers and input validation; otherwise, code is efficient and easily understandable.\\n\\n---\\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Uses Euclidean algorithm and returns abs(a), which properly handles negative input. Excellent readability and correctness. Would benefit from input validation or type checking, but meets the requirements and Python conventions well.\\n\\n---\\n\\nGeneration 4:  \\nSCORE: 0.5  \\nREASON: Same as Generations 1 and 2. Lacks negative input handling and input validation.\\n\\n---\\n\\nGeneration 5:  \\nSCORE: 0.5  \\nREASON: Again identical to Generations 1, 2, and 4. No handling of negative arguments; otherwise, efficient and readable.\\n\\n---\\n\\nTOTAL SCORE: 3.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='5b5e25f7-53dc-464c-a729-dcb242fbb012', created_at='2025-10-02T16:47:36.817779+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate(3, 7) == 1\\n    assert candidate(10, 15) == 5\\n    assert candidate(49, 14) == 7\\n    assert candidate(144, 60) == 12\\n\", prompt='\\n\\ndef greatest_common_divisor(a: int, b: int) -> int:\\n    \"\"\" Return a greatest common divisor of two integers a and b\\n    >>> greatest_common_divisor(3, 5)\\n    1\\n    >>> greatest_common_divisor(25, 15)\\n    5\\n    \"\"\"\\n', task_id='HumanEval/13', entry_point='greatest_common_divisor', generated_codes=['def greatest_common_divisor(a: int, b: int) -> int:\\n    while b:\\n        a, b = b, a % b\\n    return a', 'def greatest_common_divisor(a: int, b: int) -> int:\\n    while b:\\n        a, b = b, a % b\\n    return a', 'def greatest_common_divisor(a: int, b: int) -> int:\\n    while b:\\n        a, b = b, a % b\\n    return abs(a)', 'def greatest_common_divisor(a: int, b: int) -> int:\\n    while b:\\n        a, b = b, a % b\\n    return a', 'def greatest_common_divisor(a: int, b: int) -> int:\\n    while b:\\n        a, b = b, a % b\\n    return a'], canonical_solution='    while b:\\n        a, b = b, a % b\\n    return a\\n'), trace_id=None, run_duration=5.764610041980632, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1:  \\nSCORE: 1.0  \\nREASON: The code is concise, uses clear variable names, and leverages a list comprehension for efficiency and readability. It adheres to Python best practices and matches the specification and docstring example. No improvements needed.\\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1, with all the same strengths: clear, efficient, readable, and robust for the task.\\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Identical implementation and merits as above.\\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: Same as previous others; excellent clarity, efficiency, and adherence to requirements.\\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Also identical; continues the pattern of clarity, efficiency, and best practices.\\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='5fabed41-5eb3-424e-99db-ccedde189142', created_at='2025-10-02T16:47:36.817791+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('') == []\\n    assert candidate('asdfgh') == ['a', 'as', 'asd', 'asdf', 'asdfg', 'asdfgh']\\n    assert candidate('WWW') == ['W', 'WW', 'WWW']\\n\", prompt='from typing import List\\n\\n\\ndef all_prefixes(string: str) -> List[str]:\\n    \"\"\" Return list of all prefixes from shortest to longest of the input string\\n    >>> all_prefixes(\\'abc\\')\\n    [\\'a\\', \\'ab\\', \\'abc\\']\\n    \"\"\"\\n', task_id='HumanEval/14', entry_point='all_prefixes', generated_codes=['def all_prefixes(string: str) -> List[str]:\\n    return [string[:i] for i in range(1, len(string) + 1)]', 'def all_prefixes(string: str) -> List[str]:\\n    return [string[:i] for i in range(1, len(string) + 1)]', 'def all_prefixes(string: str) -> List[str]:\\n    return [string[:i] for i in range(1, len(string) + 1)]', 'def all_prefixes(string: str) -> List[str]:\\n    return [string[:i] for i in range(1, len(string) + 1)]', 'def all_prefixes(string: str) -> List[str]:\\n    return [string[:i] for i in range(1, len(string) + 1)]'], canonical_solution='    result = []\\n\\n    for i in range(len(string)):\\n        result.append(string[:i+1])\\n    return result\\n'), trace_id=None, run_duration=4.4534835000522435, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason=\"Generation 1:  \\nSCORE: 1.0  \\nREASON: The code is concise, uses clear naming, leverages efficient idiomatic Python (generator expression and join), exactly meets the docstring and examples, and is highly readable. No improvements necessary.\\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1; the code is robust, clear, follows Python best practices, and fulfills all requirements as specified.\\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Same as above; clear, efficient, maintains the docstring's requirements, and follows best practices.\\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: Identical to previous generations. Excellent clarity, naming, and efficiency. No issues.\\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Again, same as above. The code is clean, efficient, and precisely meets problem requirements.\\n\\nTOTAL SCORE: 5.0\", strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='f841a751-96d0-46a0-b844-e0d42886e79c', created_at='2025-10-02T16:47:36.817803+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate(0) == '0'\\n    assert candidate(3) == '0 1 2 3'\\n    assert candidate(10) == '0 1 2 3 4 5 6 7 8 9 10'\\n\", prompt='\\n\\ndef string_sequence(n: int) -> str:\\n    \"\"\" Return a string containing space-delimited numbers starting from 0 upto n inclusive.\\n    >>> string_sequence(0)\\n    \\'0\\'\\n    >>> string_sequence(5)\\n    \\'0 1 2 3 4 5\\'\\n    \"\"\"\\n', task_id='HumanEval/15', entry_point='string_sequence', generated_codes=[\"def string_sequence(n: int) -> str:\\n    return ' '.join(str(i) for i in range(n + 1))\", \"def string_sequence(n: int) -> str:\\n    return ' '.join(str(i) for i in range(n + 1))\", \"def string_sequence(n: int) -> str:\\n    return ' '.join(str(i) for i in range(n + 1))\", \"def string_sequence(n: int) -> str:\\n    return ' '.join(str(i) for i in range(n + 1))\", \"def string_sequence(n: int) -> str:\\n    return ' '.join(str(i) for i in range(n + 1))\"], canonical_solution=\"    return ' '.join([str(x) for x in range(n + 1)])\\n\"), trace_id=None, run_duration=4.168056666967459, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1:  \\nSCORE: 1.0  \\nREASON: Clear, concise, and efficient code. Uses set comprehension with lowercasing to ensure case-insensitivity, as required. Readable, no extraneous code, meets the problem specification perfectly.\\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. All of the same strengths apply; clear, efficient, and follows best practices.\\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1 and 2. Again, maintains clarity, efficiency, and adheres to best practices.\\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: Uses set on string.lower(), which is a more concise (and equally correct) way to do case-insensitive counting of distinct characters. Same output, same efficiency, fully meets the requirements.\\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 4. Same comments as above: concise, clear, efficient, and matches problem requirements.\\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='f35f592d-d5ce-4308-bfa1-4410a31e1d94', created_at='2025-10-02T16:47:36.817814+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('') == 0\\n    assert candidate('abcde') == 5\\n    assert candidate('abcde' + 'cade' + 'CADE') == 5\\n    assert candidate('aaaaAAAAaaaa') == 1\\n    assert candidate('Jerry jERRY JeRRRY') == 5\\n\", prompt='\\n\\ndef count_distinct_characters(string: str) -> int:\\n    \"\"\" Given a string, find out how many distinct characters (regardless of case) does it consist of\\n    >>> count_distinct_characters(\\'xyzXYZ\\')\\n    3\\n    >>> count_distinct_characters(\\'Jerry\\')\\n    4\\n    \"\"\"\\n', task_id='HumanEval/16', entry_point='count_distinct_characters', generated_codes=['def count_distinct_characters(string: str) -> int:\\n    return len(set(c.lower() for c in string))', 'def count_distinct_characters(string: str) -> int:\\n    return len(set(c.lower() for c in string))', 'def count_distinct_characters(string: str) -> int:\\n    return len(set(c.lower() for c in string))', 'def count_distinct_characters(string: str) -> int:\\n    return len(set(string.lower()))', 'def count_distinct_characters(string: str) -> int:\\n    return len(set(string.lower()))'], canonical_solution='    return len(set(string.lower()))\\n'), trace_id=None, run_duration=3.829540874925442, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=0.7, reason='Generation 1  \\nSCORE: 0.5  \\nREASON: The implementation is correct and readable with clear variable names and structure. However, it does not handle invalid input (unknown tokens are ignored), lacks comments, and repeats the mapping logic as chained if-elif-else rather than a dictionary/map, which could improve extensibility and readability. Code is also duplicated from other generations.\\n\\nGeneration 2  \\nSCORE: 0.5  \\nREASON: Nearly identical to Generation 1, except that the variable is named notes instead of tokens. Readable, works as expected, but suffers the same minor flaws: no input validation, no comments, and mapping is not centralized.\\n\\nGeneration 3  \\nSCORE: 1.0  \\nREASON: Uses a mapping dictionary, which improves readability and makes the mapping more extensible and maintainable. Handles only defined tokens but will silently ignore invalid ones (arguably fine for this task). Structure is clear, variables are well-named, and the implementation follows best practices.\\n\\nGeneration 4  \\nSCORE: 1.0  \\nREASON: Same logic as 1 & 2 with an added explicit else/continue branch that makes ignoring invalid tokens intentional and clear. This is a minor, positive robustness improvement. Naming and structure are fine, though it could be improved with comments or using a mapping dictionary.\\n\\nGeneration 5  \\nSCORE: 0.5  \\nREASON: Identical to 1 & 2. See reasons above: works, clear, but repeated logic, no handling or mention of invalid input, no comments, no use of a mapping structure.\\n\\nTOTAL SCORE: 3.5', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='45f48d4e-ce4f-41e1-a3ed-a34d1a15d39e', created_at='2025-10-02T16:47:36.817825+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('') == []\\n    assert candidate('o o o o') == [4, 4, 4, 4]\\n    assert candidate('.| .| .| .|') == [1, 1, 1, 1]\\n    assert candidate('o| o| .| .| o o o o') == [2, 2, 1, 1, 4, 4, 4, 4]\\n    assert candidate('o| .| o| .| o o| o o|') == [2, 1, 2, 1, 4, 2, 4, 2]\\n\", prompt='from typing import List\\n\\n\\ndef parse_music(music_string: str) -> List[int]:\\n    \"\"\" Input to this function is a string representing musical notes in a special ASCII format.\\n    Your task is to parse this string and return list of integers corresponding to how many beats does each\\n    not last.\\n\\n    Here is a legend:\\n    \\'o\\' - whole note, lasts four beats\\n    \\'o|\\' - half note, lasts two beats\\n    \\'.|\\' - quater note, lasts one beat\\n\\n    >>> parse_music(\\'o o| .| o| o| .| .| .| .| o o\\')\\n    [4, 2, 1, 2, 2, 1, 1, 1, 1, 4, 4]\\n    \"\"\"\\n', task_id='HumanEval/17', entry_point='parse_music', generated_codes=[\"def parse_music(music_string: str) -> List[int]:\\n    tokens = music_string.split()\\n    result = []\\n    for token in tokens:\\n        if token == 'o':\\n            result.append(4)\\n        elif token == 'o|':\\n            result.append(2)\\n        elif token == '.|':\\n            result.append(1)\\n    return result\", \"def parse_music(music_string: str) -> List[int]:\\n    notes = music_string.split()\\n    result = []\\n    for note in notes:\\n        if note == 'o':\\n            result.append(4)\\n        elif note == 'o|':\\n            result.append(2)\\n        elif note == '.|':\\n            result.append(1)\\n    return result\", \"def parse_music(music_string: str) -> List[int]:\\n    tokens = music_string.split()\\n    beats_map = {\\n        'o': 4,\\n        'o|': 2,\\n        '.|': 1\\n    }\\n    result = []\\n    for token in tokens:\\n        if token in beats_map:\\n            result.append(beats_map[token])\\n    return result\", \"def parse_music(music_string: str) -> List[int]:\\n    tokens = music_string.split()\\n    result = []\\n    for token in tokens:\\n        if token == 'o':\\n            result.append(4)\\n        elif token == 'o|':\\n            result.append(2)\\n        elif token == '.|':\\n            result.append(1)\\n        else:\\n            # Handle potential invalid tokens gracefully\\n            continue\\n    return result\", \"def parse_music(music_string: str) -> List[int]:\\n    tokens = music_string.split()\\n    result = []\\n    for token in tokens:\\n        if token == 'o':\\n            result.append(4)\\n        elif token == 'o|':\\n            result.append(2)\\n        elif token == '.|':\\n            result.append(1)\\n    return result\"], canonical_solution=\"    note_map = {'o': 4, 'o|': 2, '.|': 1}\\n    return [note_map[x] for x in music_string.split(' ') if x]\\n\"), trace_id=None, run_duration=7.016766541986726, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=0.5, reason=\"Generation 1  \\nSCORE: 0.5  \\nREASON: The code is readable and uses standard variable names, and the algorithm is correct and efficient for counting overlapping substrings. However, it lacks input validation (e.g., it does not handle the case when substring is '', which would cause an infinite loop), which is important for robustness and maintaining best practices.\\n\\nGeneration 2  \\nSCORE: 0.5  \\nREASON: This code is identical to Generation 1, with the same strengths (clarity and correctness) and the same flaw (no handling for empty substring input).\\n\\nGeneration 3  \\nSCORE: 0.5  \\nREASON: Also identical to previous generations. Same issues: clear, correct, but lacks empty substring check, risking infinite loop.\\n\\nGeneration 4  \\nSCORE: 0.5  \\nREASON: Same as the previous generations; clear and correct, but missing input validation for the empty substring case.\\n\\nGeneration 5  \\nSCORE: 0.5  \\nREASON: Also identical. Lacks input validation for an empty substring argument, which can result in a runtime error (hang).\\n\\nTOTAL SCORE: 2.5\", strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='5795e533-128a-4537-914f-89584cd18bdf', created_at='2025-10-02T16:47:36.817839+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('', 'x') == 0\\n    assert candidate('xyxyxyx', 'x') == 4\\n    assert candidate('cacacacac', 'cac') == 4\\n    assert candidate('john doe', 'john') == 1\\n\", prompt='\\n\\ndef how_many_times(string: str, substring: str) -> int:\\n    \"\"\" Find how many times a given substring can be found in the original string. Count overlaping cases.\\n    >>> how_many_times(\\'\\', \\'a\\')\\n    0\\n    >>> how_many_times(\\'aaa\\', \\'a\\')\\n    3\\n    >>> how_many_times(\\'aaaa\\', \\'aa\\')\\n    3\\n    \"\"\"\\n', task_id='HumanEval/18', entry_point='how_many_times', generated_codes=['def how_many_times(string: str, substring: str) -> int:\\n    count = 0\\n    start = 0\\n    while True:\\n        start = string.find(substring, start)\\n        if start == -1:\\n            break\\n        count += 1\\n        start += 1\\n    return count', 'def how_many_times(string: str, substring: str) -> int:\\n    count = 0\\n    start = 0\\n    while True:\\n        start = string.find(substring, start)\\n        if start == -1:\\n            break\\n        count += 1\\n        start += 1\\n    return count', 'def how_many_times(string: str, substring: str) -> int:\\n    count = 0\\n    start = 0\\n    while True:\\n        start = string.find(substring, start)\\n        if start == -1:\\n            break\\n        count += 1\\n        start += 1\\n    return count', 'def how_many_times(string: str, substring: str) -> int:\\n    count = 0\\n    start = 0\\n    while True:\\n        start = string.find(substring, start)\\n        if start == -1:\\n            break\\n        count += 1\\n        start += 1\\n    return count', 'def how_many_times(string: str, substring: str) -> int:\\n    count = 0\\n    start = 0\\n    while True:\\n        start = string.find(substring, start)\\n        if start == -1:\\n            break\\n        count += 1\\n        start += 1\\n    return count'], canonical_solution='    times = 0\\n\\n    for i in range(len(string) - len(substring) + 1):\\n        if string[i:i+len(substring)] == substring:\\n            times += 1\\n\\n    return times\\n'), trace_id=None, run_duration=5.170383583987132, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1:  \\nSCORE: 1.0  \\nREASON: Clear mapping, proper naming, concise, and straightforward. The algorithm is efficient for the input size and the requirements. While reverse_map is unnecessary, its presence does not harm readability or efficiency. Lacks error handling for invalid input but matches common expectations for the described requirements.\\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: Good variable naming, structure, and efficient sorting using a map. inverse_map is unused but does not detract significantly. Readability and maintainability are high. Lacks input validation but acceptable for the stated function.\\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Clearly written, clean, and efficient. reverse_map unused but does not affect function. Variable names are descriptive, logic is clear, and aligns with Python best practices. Could handle unexpected input better but meets the problem‚Äôs demand.\\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: Excellent naming, minimal unused variables, simple and efficient. The function is clear, readable, and robust for the expected correct inputs. Fully meets the expectations for high-quality code as described.\\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Effective variable naming, consistent logic, and cleanly structured. inverse_map is unused but negligible drawback. Good readability and maintains Pythonic style. Error handling not strictly required for problem scope.\\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='e83a9801-fe6e-4e48-9ce0-4916368b5ace', created_at='2025-10-02T16:47:36.817851+00:00', name=None, dataset_id='85dd37ec-8599-4473-a7a7-42c9e2e36b0a', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('') == ''\\n    assert candidate('three') == 'three'\\n    assert candidate('three five nine') == 'three five nine'\\n    assert candidate('five zero four seven nine eight') == 'zero four five seven eight nine'\\n    assert candidate('six five four three two one zero') == 'zero one two three four five six'\\n\", prompt='from typing import List\\n\\n\\ndef sort_numbers(numbers: str) -> str:\\n    \"\"\" Input is a space-delimited string of numberals from \\'zero\\' to \\'nine\\'.\\n    Valid choices are \\'zero\\', \\'one\\', \\'two\\', \\'three\\', \\'four\\', \\'five\\', \\'six\\', \\'seven\\', \\'eight\\' and \\'nine\\'.\\n    Return the string with numbers sorted from smallest to largest\\n    >>> sort_numbers(\\'three one five\\')\\n    \\'one three five\\'\\n    \"\"\"\\n', task_id='HumanEval/19', entry_point='sort_numbers', generated_codes=[\"def sort_numbers(numbers: str) -> str:\\n    num_map = {\\n        'zero': 0,\\n        'one': 1,\\n        'two': 2,\\n        'three': 3,\\n        'four': 4,\\n        'five': 5,\\n        'six': 6,\\n        'seven': 7,\\n        'eight': 8,\\n        'nine': 9\\n    }\\n    reverse_map = {v: k for k, v in num_map.items()}\\n    tokens = numbers.split()\\n    sorted_numbers = sorted(tokens, key=lambda x: num_map[x])\\n    return ' '.join(sorted_numbers)\", \"def sort_numbers(numbers: str) -> str:\\n    num_map = {\\n        'zero': 0,\\n        'one': 1,\\n        'two': 2,\\n        'three': 3,\\n        'four': 4,\\n        'five': 5,\\n        'six': 6,\\n        'seven': 7,\\n        'eight': 8,\\n        'nine': 9\\n    }\\n    inverse_map = {v: k for k, v in num_map.items()}\\n    number_list = numbers.split()\\n    sorted_numbers = sorted(number_list, key=lambda x: num_map[x])\\n    return ' '.join(sorted_numbers)\", \"def sort_numbers(numbers: str) -> str:\\n    numeral_map = {\\n        'zero': 0,\\n        'one': 1,\\n        'two': 2,\\n        'three': 3,\\n        'four': 4,\\n        'five': 5,\\n        'six': 6,\\n        'seven': 7,\\n        'eight': 8,\\n        'nine': 9\\n    }\\n    reverse_map = {v: k for k, v in numeral_map.items()}\\n    number_list = numbers.split()\\n    sorted_numbers = sorted(number_list, key=lambda n: numeral_map[n])\\n    return ' '.join(sorted_numbers)\", \"def sort_numbers(numbers: str) -> str:\\n    word_to_num = {\\n        'zero': 0,\\n        'one': 1,\\n        'two': 2,\\n        'three': 3,\\n        'four': 4,\\n        'five': 5,\\n        'six': 6,\\n        'seven': 7,\\n        'eight': 8,\\n        'nine': 9\\n    }\\n    num_to_word = {v: k for k, v in word_to_num.items()}\\n    tokens = numbers.split()\\n    sorted_tokens = sorted(tokens, key=lambda x: word_to_num[x])\\n    return ' '.join(sorted_tokens)\", \"def sort_numbers(numbers: str) -> str:\\n    number_map = {\\n        'zero': 0,\\n        'one': 1,\\n        'two': 2,\\n        'three': 3,\\n        'four': 4,\\n        'five': 5,\\n        'six': 6,\\n        'seven': 7,\\n        'eight': 8,\\n        'nine': 9\\n    }\\n    inverse_map = {v: k for k, v in number_map.items()}\\n    nums = numbers.split()\\n    nums_sorted = sorted(nums, key=lambda x: number_map[x])\\n    return ' '.join(nums_sorted)\"], canonical_solution=\"    value_map = {\\n        'zero': 0,\\n        'one': 1,\\n        'two': 2,\\n        'three': 3,\\n        'four': 4,\\n        'five': 5,\\n        'six': 6,\\n        'seven': 7,\\n        'eight': 8,\\n        'nine': 9\\n    }\\n    return ' '.join(sorted([x for x in numbers.split(' ') if x], key=lambda x: value_map[x]))\\n\"), trace_id=None, run_duration=5.306408958043903, evaluation_cost=None)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n‚ö° Running evaluation...\")\n",
    "judgment.run_evaluation(\n",
    "    examples=dataset.examples,\n",
    "    scorers=[CodeExecutionScorer(k=1, n=num_generations), CodeQualityScorer(n=num_generations)],\n",
    "    project_name=\"humaneval-project\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a58b3",
   "metadata": {},
   "source": [
    "Click **View Results** to open the dashboard. You should see something like this ‚Äî be sure to explore the **Tests** page and the analytics panels to view detailed results and insights.  \n",
    "\n",
    "\n",
    "![Dashboard Screenshot](./assets/offline_tests.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

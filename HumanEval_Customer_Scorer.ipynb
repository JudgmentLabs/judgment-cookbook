{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f43c5a36",
   "metadata": {},
   "source": [
    "# Custom Scorers with HumanEval\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JudgmentLabs/judgment-cookbook/blob/refactor/HumanEval_Custom_Scorer.ipynb)\n",
    "[![Docs](https://img.shields.io/badge/Documentation-blue)](https://docs.judgmentlabs.ai/documentation)\n",
    "\n",
    "In this notebook, you will learn how to evaluate code generation on OpenAI's [HumanEval](https://github.com/openai/human-eval) benchmark and create **custom scorers** that are code-based and LLM-as-a-Judge using the [`judgeval`](https://github.com/JudgmentLabs/judgeval) library. \n",
    "\n",
    "1. **Code Execution Scorer**: Uses sandboxed code execution to evaluate code correctness\n",
    "2. **LLM-as-a-Judge Scorer**: Uses language models to evaluate code quality\n",
    "\n",
    "You will generate code using LLMs, create a custom scorers that leverages OpenAI's sandboxed environment and LLM-as-a-Judge, and evaluate it on the HumanEval benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17740081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installations\n",
    "!pip install human-eval datasets openai judgeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0220c8",
   "metadata": {},
   "source": [
    "To run this notebook, select **Runtime* -> Run All*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90179020",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You can get your Judgment API key and Org ID for free on [Judgment](https://app.judgmentlabs.ai/register).\n",
    "\n",
    "![Get Started](./assets/get_started.png)\n",
    "\n",
    "Within your organization, create a project called `humaneval-project`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7767e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set api keys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ...  # Fill your API keys here\n",
    "os.environ[\"JUDGMENT_API_KEY\"] = ...\n",
    "os.environ[\"JUDGMENT_ORG_ID\"] = ...\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624525fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:12:22 - judgeval - WARNING - UPDATE AVAILABLE: You are using 'judgeval==0.11.0', but the latest version is '0.14.1'. While this version is still supported, we recommend upgrading to avoid potential issues or missing features: `pip install --upgrade judgeval`\n"
     ]
    }
   ],
   "source": [
    "from judgeval import JudgmentClient\n",
    "\n",
    "from human_eval.execution import check_correctness\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from typing import Dict, Any\n",
    "import asyncio\n",
    "\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd9cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "judgment = JudgmentClient()\n",
    "client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f5296",
   "metadata": {},
   "source": [
    "## 1. Understanding HumanEval\n",
    "\n",
    "HumanEval is a benchmark dataset created by OpenAI for evaluating code generation models. Introduced in the paper [\"Evaluating Large Language Models Trained on Code\"](https://arxiv.org/abs/2107.03374), it contains 164 Python programming problems designed to test functional correctness.\n",
    "\n",
    "### What HumanEval Contains\n",
    "Each problem includes:\n",
    "- **Function signature and docstring**: The problem description\n",
    "- **Test cases**: Automated tests to verify correctness  \n",
    "- **Canonical solution**: Reference implementation\n",
    "- **Entry point**: Function name to test\n",
    "\n",
    "### How HumanEval Evaluates Code\n",
    "HumanEval evaluates model outputs by dynamically building a Python program that stitches together the **Function signature and docstring**, the **model’s generated solution**, and the **test cases**. This combined program is then executed in a sandbox to verify whether the generated code passes all test cases.\n",
    "\n",
    "The ```check_correctness``` function orchestrates this process: assembles the prompt, generated solution, and tests into a single program, and executes the script in a sandboxed environment.\n",
    "\n",
    "```python\n",
    "# Construct the check program and run it.\n",
    "check_program = (\n",
    "    problem[\"prompt\"] +      # Function signature + docstring\n",
    "    completion +            # Generated code\n",
    "    \"\\n\" +\n",
    "    problem[\"test\"] +       # Test cases\n",
    "    \"\\n\" +\n",
    "    f\"check({problem['entry_point']})\"  # Call the test function\n",
    ")\n",
    "\n",
    "...\n",
    "\n",
    "# WARNING: This executes untrusted model-generated code\n",
    "exec(check_program, exec_globals)\n",
    "```\n",
    "\n",
    "The evaluation is **pass/fail**: if all test cases pass without exceptions, the code is correct. If any test fails or the code crashes, it's incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecd8bd2",
   "metadata": {},
   "source": [
    "### Pass@k\n",
    "\n",
    "Instead of stopping at a simple pass/fail, HumanEval extend this to compute the **Pass@k** statistic. Pass@k measures the probability that at least one of the top-k generated solutions is correct, given:\n",
    "\n",
    "- **n** = total number of generated solutions  \n",
    "- **c** = number of correct solutions among them  \n",
    "- **k** = how many solutions we sample  \n",
    "\n",
    "For example, if we generate **n = 5** completions and some are correct, we can compute **Pass@1** (the chance a single sampled solution is correct) and **Pass@3** (the chance at least one out of three sampled solutions is correct). This provides a more practical view of model performance, since in real use cases we usually sample multiple completions and care whether any of them solves the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c9d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator(n: int, c: int, k: int) -> float:\n",
    "    if n - c < k:\n",
    "        return 1.0\n",
    "    return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d2914",
   "metadata": {},
   "source": [
    "## Define Your Custom Example Class\n",
    "\n",
    "In `judgeval`, all data passed into scorers is represented as an `Example`. The base `Example` object is a container that standardizes how data is stored and accessed. By inheriting from it, you can define your own fields that describe the task you want to monitor.\n",
    "\n",
    "For HumanEval, we’ll create a `HumanEvalExample` that captures the fields needed to represent a problem and its generated code candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe11ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from judgeval.data import Example\n",
    "\n",
    "class HumanEvalExample(Example):\n",
    "    \"\"\"\n",
    "    Custom Example for HumanEval tasks.\n",
    "    \"\"\"\n",
    "    task_id: str\n",
    "    prompt: str\n",
    "    canonical_solution: str\n",
    "    test: str\n",
    "    entry_point: str\n",
    "    generated_codes: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa9b1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is:  def add(a: int, b: int) -> int:\n",
      "    # Write your code here\n",
      "The canonical solution is:  def add(a: int, b: int) -> int:\n",
      "    return a + b\n"
     ]
    }
   ],
   "source": [
    "one_example = HumanEvalExample(\n",
    "    task_id=\"HumanEval/0\",\n",
    "    prompt=\"def add(a: int, b: int) -> int:\\n    # Write your code here\",\n",
    "    canonical_solution=\"def add(a: int, b: int) -> int:\\n    return a + b\",\n",
    "    test=\"assert add(1, 2) == 3\",\n",
    "    entry_point=\"add\",\n",
    "    generated_codes=[\"def add(a, b): return a+b\"]\n",
    ")\n",
    "\n",
    "print(\"The prompt is: \", one_example.prompt)\n",
    "print(\"The canonical solution is: \", one_example.canonical_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8775f3",
   "metadata": {},
   "source": [
    "## Custom Code Execution Scorer\n",
    "\n",
    "We'll create a custom scorer using `judgeval` that integrates HumanEval's sandboxed code execution. We'll integrate the `check_correctness` and `estimator` functions to build a custom scorer called `CodeExecutionScorer`.\n",
    "\n",
    "In `judgeval`, the user must implement:\n",
    "\n",
    "`async def a_score_example(self, example: Example)`\n",
    "\n",
    "This method will asynchronously score each example, and the scorer should set three key fields:\n",
    "\n",
    "- **`self.name`**: label shown in the dashboard  \n",
    "- **`self.score`**: numeric metric value (e.g., Pass@k in `[0, 1]`)  \n",
    "- **`self.reason`**: human-readable explanation or context behind the score  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8af9e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from judgeval.scorers.example_scorer import ExampleScorer\n",
    "\n",
    "class CodeExecutionScorer(ExampleScorer):\n",
    "    \"\"\"\n",
    "    A scorer for evaluating code generation using check_correctness\n",
    "    and Pass@k statistics.\n",
    "    \"\"\"\n",
    "    #default values\n",
    "    k: int = 1\n",
    "    n: int = 1\n",
    "\n",
    "    async def a_score_example(self, example: HumanEvalExample) -> None:\n",
    "        \"\"\"\n",
    "        Score an example by running the generated code against test cases.\n",
    "        \n",
    "        This method uses check_correctness to execute the generated code\n",
    "        in a sandboxed environment and check if it passes all test cases.\n",
    "        \n",
    "        Args:\n",
    "            example (Example): The example containing the problem and generated code\n",
    "            \n",
    "        Returns:\n",
    "            float: The score (1.0 if all tests pass, 0.0 otherwise)\n",
    "        \"\"\"\n",
    "\n",
    "        # Name the scorer\n",
    "        self.name = f\"Pass@{self.k} for {self.n} generations\"\n",
    "\n",
    "        # Create problem dict in the format expected by check_correctness\n",
    "        problem = {\n",
    "            \"task_id\": example.task_id,\n",
    "            \"prompt\": example.prompt,\n",
    "            \"test\": example.test,\n",
    "            \"entry_point\": example.entry_point\n",
    "        }\n",
    "\n",
    "        \n",
    "        # Use check_correctness to evaluate the generated code\n",
    "        self.n = len(example.generated_codes)\n",
    "        failed_results = []\n",
    "\n",
    "        for i in range(self.n):\n",
    "            result = check_correctness(\n",
    "                problem=problem,\n",
    "                completion=example.generated_codes[i],\n",
    "                timeout=3.0\n",
    "            )\n",
    "\n",
    "            if not result[\"passed\"]:\n",
    "                failed_results.append(result['result'])\n",
    "\n",
    "        c = self.n - len(failed_results)\n",
    "\n",
    "        pass_k = estimator(self.n, c, self.k)\n",
    "        \n",
    "        self.score = pass_k\n",
    "        self.reason = (\n",
    "            f\"Passed {self.n - len(failed_results)} out of {self.n} tests.\\n\\n\"\n",
    "            \"Failing snippets:\\n\"\n",
    "            + \"\\n---\\n\".join(failed_results)\n",
    "        )        \n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631d91b",
   "metadata": {},
   "source": [
    "## Custom LLM-as-a-Judge Scorer\n",
    "\n",
    "\n",
    "It may not be enough to judge code solutions on their execution results alone, since production-ready code is often evaluated against broader qualities like readability, efficiency, and adherence to best practices. \n",
    "\n",
    "To capture these, we’ll create another custom scorer with `judgeval` that integrates **LLM-as-a-Judge**, using a language model to assess generated code against a clear rubric emphasizing **readability**, **efficiency**, and **adherence to best practices**.\n",
    "\n",
    "\n",
    "\n",
    "Unlike the pass/fail execution check, this scorer evaluates multiple generations and returns a **0–n** score (where n = number of generations), with each generation rated as:\n",
    "- **High Quality (1.0)**: Excellent readability, efficiency, and best practices\n",
    "- **Medium Quality (0.5)**: Good overall with minor issues\n",
    "- **Low Quality (0.0)**: Significant problems\n",
    "\n",
    "The total score is the sum across all generations, giving you both individual generation quality and overall performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08900129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeQualityScorer(ExampleScorer):\n",
    "    \"\"\"\n",
    "    A scorer for evaluating code generation using LLM-as-a-Judge.\n",
    "    \"\"\"\n",
    "\n",
    "    #default values\n",
    "    n: int = 1\n",
    "\n",
    "    async def a_score_example(self, example: HumanEvalExample) -> None:\n",
    "        \"\"\"\n",
    "        Score an example by running the generated code against LLM-as-a-Judge.\n",
    "        \n",
    "        Args:\n",
    "            example (HumanEvalExample): The example containing the problem and generated code\n",
    "            \n",
    "        Returns:\n",
    "            float: The score is the sum of individual generation ratings (1.0 for high quality, 0.5 for medium quality, 0.0 for low quality) across n generations\n",
    "        \"\"\"\n",
    "\n",
    "        # Name the scorer\n",
    "        self.name = f\"Code Quality for {self.n} generations\"\n",
    "\n",
    "\n",
    "        generations_text = \"\\n\".join([\n",
    "            f\"Generation {i+1}:\\n{example.generated_codes[i]}\\n\" \n",
    "            for i in range(self.n)\n",
    "        ])\n",
    "\n",
    "        prompt = f\"\"\"You are an expert code reviewer. Evaluate each code generation below.\n",
    "\n",
    "        Rate each as:\n",
    "        - HIGH QUALITY (1.0): Excellent readability (clear naming, structure, comments), efficient algorithms, follows best practices (input validation, robustness, maintainability)\n",
    "        - MEDIUM QUALITY (0.5): Good overall with minor issues in readability, efficiency, or best practices  \n",
    "        - LOW QUALITY (0.0): Significant problems with readability, efficiency, or best practices\n",
    "\n",
    "        Consider: Code clarity, naming, algorithm efficiency, error handling, organization, Python best practices.\n",
    "\n",
    "        Problem: {example.prompt}\n",
    "\n",
    "        {generations_text}\n",
    "\n",
    "        For each generation, provide:\n",
    "        SCORE: [1.0/0.5/0.0]\n",
    "        REASON: [brief explanation]\n",
    "\n",
    "        Then provide the total:\n",
    "        TOTAL SCORE: [sum of all scores]\"\"\"\n",
    "\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert code reviewer. Evaluate each code generation below and return the individual scores and the reasons for the scores and the sum of all generation ratings, e.g., 4.5).\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Extract total score using regex\n",
    "        total_score_match = re.search(r'(?:\\*\\*)?TOTAL SCORE:?\\*?\\*?\\s*(\\d+\\.?\\d*)', response.choices[0].message.content, re.IGNORECASE)\n",
    "        if total_score_match:\n",
    "            total_score = float(total_score_match.group(1)) / self.n\n",
    "        else:\n",
    "            total_score = 0.0\n",
    "        \n",
    "        \n",
    "        self.score = total_score\n",
    "        self.reason = response.choices[0].message.content\n",
    "        \n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35205b8",
   "metadata": {},
   "source": [
    "## Code Generation Function\n",
    "\n",
    "Next, we’ll implement a function that, given a HumanEval problem, queries an LLM to produce a candidate implementation. The function is written with `async/await` so multiple problems can be evaluated in parallel, significantly reducing total runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd2f6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_code(problem: Dict[str, Any]) -> str:\n",
    "    \"\"\"Generate code using LLM for a given HumanEval problem.\"\"\"\n",
    "    prompt = problem[\"prompt\"]\n",
    "    \n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Python programmer. Write ONLY the Python function code that solves the given problem. Do not include any markdown formatting, explanations, or code blocks. Return only the raw Python code.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    \n",
    "    generated_code = response.choices[0].message.content\n",
    "    \n",
    "    return generated_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971f166",
   "metadata": {},
   "source": [
    "## Load HumanEval Dataset\n",
    "\n",
    "Now let's load the HumanEval [dataset](https://huggingface.co/datasets/openai/openai_humaneval) from Hugging Face and examine its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dee8c284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewli/judgment-cookbook/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading HumanEval dataset...\n",
      "   Found 164 problems\n",
      "\n",
      "📋 Example problem structure:\n",
      "   Task ID: HumanEval/0\n",
      "   Entry Point: has_close_elements\n",
      "   Prompt length: 348 characters\n",
      "   Test length: 531 characters\n",
      "\n",
      "📝 Sample prompt:\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given thr...\n"
     ]
    }
   ],
   "source": [
    "from judgeval.dataset import Dataset\n",
    "from datasets import load_dataset\n",
    "# Load the HumanEval dataset\n",
    "print(\"📊 Loading HumanEval dataset...\")\n",
    "dataset = load_dataset(\"openai/openai_humaneval\")\n",
    "print(f\"   Found {len(dataset['test'])} problems\")\n",
    "\n",
    "# Examine the structure of a single problem\n",
    "example_problem = dataset[\"test\"][0]\n",
    "print(\"\\n📋 Example problem structure:\")\n",
    "print(f\"   Task ID: {example_problem['task_id']}\")\n",
    "print(f\"   Entry Point: {example_problem['entry_point']}\")\n",
    "print(f\"   Prompt length: {len(example_problem['prompt'])} characters\")\n",
    "print(f\"   Test length: {len(example_problem['test'])} characters\")\n",
    "\n",
    "print(\"\\n📝 Sample prompt:\")\n",
    "print(example_problem['prompt'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8253bf72",
   "metadata": {},
   "source": [
    "Now, let’s generate code responses for each problem in the HumanEval benchmark, and we’ll create a HumanEvalExample for upload into `judgeval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2856d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Generating code...\n",
      "   Problem 1/5: HumanEval/0\n",
      "   Problem 2/5: HumanEval/1\n",
      "   Problem 3/5: HumanEval/2\n",
      "   Problem 4/5: HumanEval/3\n",
      "   Problem 5/5: HumanEval/4\n",
      "   Problem 6/5: HumanEval/5\n",
      "   Problem 7/5: HumanEval/6\n",
      "   Problem 8/5: HumanEval/7\n",
      "   Problem 9/5: HumanEval/8\n",
      "   Problem 10/5: HumanEval/9\n",
      "   Problem 11/5: HumanEval/10\n",
      "   Problem 12/5: HumanEval/11\n",
      "   Problem 13/5: HumanEval/12\n",
      "   Problem 14/5: HumanEval/13\n",
      "   Problem 15/5: HumanEval/14\n",
      "   Problem 16/5: HumanEval/15\n",
      "   Problem 17/5: HumanEval/16\n",
      "   Problem 18/5: HumanEval/17\n",
      "   Problem 19/5: HumanEval/18\n",
      "   Problem 20/5: HumanEval/19\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🤖 Generating code...\")\n",
    "problems = list(dataset[\"test\"].select(range(20)))\n",
    "\n",
    "num_generations = 5\n",
    "\n",
    "# Generate all code in parallel\n",
    "generations = [\n",
    "    await asyncio.gather(*[generate_code(problem) for _ in range(num_generations)])\n",
    "    for problem in problems\n",
    "]\n",
    "\n",
    "# Create examples\n",
    "examples = []\n",
    "for i, (problem, generated_codes) in enumerate(zip(problems, generations)):\n",
    "    print(f\"   Problem {i+1}/5: {problem['task_id']}\")\n",
    "    \n",
    "    example = HumanEvalExample(\n",
    "        task_id=problem[\"task_id\"],\n",
    "        prompt=problem[\"prompt\"],\n",
    "        canonical_solution=problem[\"canonical_solution\"],\n",
    "        test=problem[\"test\"],\n",
    "        entry_point=problem[\"entry_point\"],\n",
    "        generated_codes=generated_codes\n",
    "    )\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a0bec",
   "metadata": {},
   "source": [
    "We use `Dataset.create()` to create a new dataset and upload it to the [Judgment](https://app.judgmentlabs.ai/app) platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "487e2b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:14:44 - judgeval - INFO - Successfully created dataset humaneval-dataset!\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.create(\n",
    "    name=\"humaneval-dataset\", \n",
    "    project_name=\"humaneval-project\", \n",
    "    examples=examples,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef3345",
   "metadata": {},
   "source": [
    "We use `Dataset.get()` to retrieve an existing dataset from the [Judgment](https://app.judgmentlabs.ai/app) platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94ea79b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-30 17:14:45 - judgeval - INFO - Successfully retrieved dataset humaneval-dataset!\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.get(\n",
    "    name=\"humaneval-dataset\",\n",
    "    project_name=\"humaneval-project\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aac9879",
   "metadata": {},
   "source": [
    "## Running Evaluation\n",
    "\n",
    "We then use our custom scorer and the judgment client to run evaluations asynchronously on our servers and display the results on the judgment platform for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49865caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚡ Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 20 example(s) in parallel: |██████████|100% (20/20) [Time Taken: 00:17,  1.12Example/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "🔍 You can view your evaluation results here: <a href=\"https://app.judgmentlabs.ai/org/12b03546-81a4-4f6c-b3cf-4670c9b8548e/project/cc6a4ad8-b66d-4980-bd7c-fdd056c88c71/tests/33dfcd50-6a3f-4f86-aab9-c84f0f6f860c\" target=\"_blank\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">View Results</span></a>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "🔍 You can view your evaluation results here: \u001b]8;id=646599;https://app.judgmentlabs.ai/org/12b03546-81a4-4f6c-b3cf-4670c9b8548e/project/cc6a4ad8-b66d-4980-bd7c-fdd056c88c71/tests/33dfcd50-6a3f-4f86-aab9-c84f0f6f860c\u001b\\\u001b[38;2;106;0;255mView Results\u001b[0m\u001b]8;;\u001b\\\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=0.8, reason=\"Generation 1:  \\nSCORE: 0.5  \\nREASON: This is a brute-force O(n^2) solution that checks all pairs. It's clear and easy to understand, but it is not efficient for large lists. Variable names are generic but acceptable, docstring is missing, and there's no input validation. Overall robustness is okay for small input sizes, but algorithmic efficiency could be improved.\\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: This implementation sorts the list (O(n log n)), then checks only adjacent numbers for threshold difference. This is optimal for this problem. The code is clear, maintains good readability, and follows best practices for this kind of numeric check. Robustness is fine for the expected input. \\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 2. Efficient, readable, concise, and the correct approach.\\n\\nGeneration 4:  \\nSCORE: 0.5  \\nREASON: Identical to Generation 1. Clear and correct, but suboptimal O(n^2) time complexity. Lacks efficiency for large lists.\\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 2/3. Efficient and clean approach using sorting. Meets all expectations.\\n\\nTOTAL SCORE: 4.0\", strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='3d151f4c-d26b-4e70-ae04-5d40abcf3685', created_at='2025-09-30T17:14:43.818526+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\n\", prompt='from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n', task_id='HumanEval/0', entry_point='has_close_elements', generated_codes=['def has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    for i in range(len(numbers)):\\n        for j in range(i + 1, len(numbers)):\\n            if abs(numbers[i] - numbers[j]) < threshold:\\n                return True\\n    return False', 'def has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    sorted_numbers = sorted(numbers)\\n    for i in range(len(sorted_numbers) - 1):\\n        if abs(sorted_numbers[i] - sorted_numbers[i + 1]) < threshold:\\n            return True\\n    return False', 'def has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    sorted_numbers = sorted(numbers)\\n    for i in range(len(sorted_numbers) - 1):\\n        if abs(sorted_numbers[i] - sorted_numbers[i + 1]) < threshold:\\n            return True\\n    return False', 'def has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    for i in range(len(numbers)):\\n        for j in range(i + 1, len(numbers)):\\n            if abs(numbers[i] - numbers[j]) < threshold:\\n                return True\\n    return False', 'def has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    sorted_numbers = sorted(numbers)\\n    for i in range(len(sorted_numbers) - 1):\\n        if abs(sorted_numbers[i] - sorted_numbers[i + 1]) < threshold:\\n            return True\\n    return False'], canonical_solution='    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n'), trace_id=None, run_duration=10.124650207930245, evaluation_cost=None),\n",
       " ScoringResult(success=False, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=False, score=0.0, reason='**Generation 1**  \\n**SCORE:** 1.0  \\n**REASON:**  \\n- Excellent clarity of variable names (`result`, `current`, `depth`).  \\n- Follows Python best practices (avoids mutating input, strips spaces correctly).  \\n- Efficient: O(n) single pass, well-structured loop.\\n- Robust: Handles only parentheses, doesn’t break on extra characters but also doesn’t validate wrong characters (as per instructions, that\\'s acceptable).\\n- Readable and maintainable.\\n\\n---\\n\\n**Generation 2**  \\n**SCORE:** 1.0  \\n**REASON:**  \\n- Very similar logic to Generation 1, but instead skips spaces inline instead of `.replace(\" \", \"\")`, which is equally clear.  \\n- Has a placeholder for handling unexpected characters (with `pass`), showing a consideration for robustness.  \\n- Follows good naming, clear logic, and is easy to maintain.\\n\\n---\\n\\n**Generation 3**\\n**SCORE:** 1.0  \\n**REASON:**  \\n- Clear, efficient, and concise.  \\n- Slight difference in using strings instead of lists for `current`, but since Python string concatenation is efficient for small strings, for this case it’s fine.  \\n- Well-structured, easy to read, no extraneous complexity.\\n\\n---\\n\\n**Generation 4**  \\n**SCORE:** 1.0  \\n**REASON:**  \\n- Identical to Generation 3 (just spacing difference), so all positives apply.\\n\\n---\\n\\n**Generation 5**  \\n**SCORE:** 1.0  \\n**REASON:**  \\n- Good variable names, clear logic.\\n- Handles all grouping robustly, uses explicit `open_count` for depth.\\n- Follows best practices, clear and idiomatic for beginners and experts alike.\\n\\n---\\n\\n### TOTAL SCORE: **5.0**\\n\\n**All generations are HIGH QUALITY (1.0) and follow best practices, clarity, and robustness for the described problem.**', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='e931a4ab-9a33-4369-acd6-4fac47c9683a', created_at='2025-09-30T17:14:43.818572+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('(()()) ((())) () ((())()())') == [\\n        '(()())', '((()))', '()', '((())()())'\\n    ]\\n    assert candidate('() (()) ((())) (((())))') == [\\n        '()', '(())', '((()))', '(((())))'\\n    ]\\n    assert candidate('(()(())((())))') == [\\n        '(()(())((())))'\\n    ]\\n    assert candidate('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\\n\", prompt='from typing import List\\n\\n\\ndef separate_paren_groups(paren_string: str) -> List[str]:\\n    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\\n    separate those group into separate strings and return the list of those.\\n    Separate groups are balanced (each open brace is properly closed) and not nested within each other\\n    Ignore any spaces in the input string.\\n    >>> separate_paren_groups(\\'( ) (( )) (( )( ))\\')\\n    [\\'()\\', \\'(())\\', \\'(()())\\']\\n    \"\"\"\\n', task_id='HumanEval/1', entry_point='separate_paren_groups', generated_codes=['def separate_paren_groups(paren_string: str) -> List[str]:\\n    result = []\\n    current = []\\n    depth = 0\\n    for char in paren_string.replace(\" \", \"\"):\\n        if char == \\'(\\':\\n            if depth == 0:\\n                current = []\\n            current.append(char)\\n            depth += 1\\n        elif char == \\')\\':\\n            current.append(char)\\n            depth -= 1\\n            if depth == 0:\\n                result.append(\\'\\'.join(current))\\n    return result', \"def separate_paren_groups(paren_string: str) -> List[str]:\\n    result = []\\n    current = []\\n    depth = 0\\n    \\n    for ch in paren_string:\\n        if ch == ' ':\\n            continue\\n        elif ch == '(':\\n            if depth == 0:\\n                current = []\\n            current.append(ch)\\n            depth += 1\\n        elif ch == ')':\\n            current.append(ch)\\n            depth -= 1\\n            if depth == 0:\\n                result.append(''.join(current))\\n        else:\\n            # Ignore any unexpected characters\\n            pass\\n    return result\", \"def separate_paren_groups(paren_string: str) -> List[str]:\\n    result = []\\n    current = ''\\n    depth = 0\\n    for char in paren_string.replace(' ', ''):\\n        if char == '(':\\n            if depth == 0:\\n                current = ''\\n            current += char\\n            depth += 1\\n        elif char == ')':\\n            current += char\\n            depth -= 1\\n            if depth == 0:\\n                result.append(current)\\n    return result\", \"def separate_paren_groups(paren_string: str) -> List[str]:\\n    result = []\\n    current = ''\\n    depth = 0\\n    for char in paren_string.replace(' ', ''):\\n        if char == '(':\\n            if depth == 0:\\n                current = ''\\n            current += char\\n            depth += 1\\n        elif char == ')':\\n            current += char\\n            depth -= 1\\n            if depth == 0:\\n                result.append(current)\\n    return result\", 'def separate_paren_groups(paren_string: str) -> List[str]:\\n    cleaned_str = paren_string.replace(\" \", \"\")\\n    groups = []\\n    current_group = \\'\\'\\n    open_count = 0\\n\\n    for char in cleaned_str:\\n        if char == \\'(\\':\\n            open_count += 1\\n        if open_count > 0:\\n            current_group += char\\n        if char == \\')\\':\\n            open_count -= 1\\n            if open_count == 0:\\n                groups.append(current_group)\\n                current_group = \\'\\'\\n    return groups'], canonical_solution=\"    result = []\\n    current_string = []\\n    current_depth = 0\\n\\n    for c in paren_string:\\n        if c == '(':\\n            current_depth += 1\\n            current_string.append(c)\\n        elif c == ')':\\n            current_depth -= 1\\n            current_string.append(c)\\n\\n            if current_depth == 0:\\n                result.append(''.join(current_string))\\n                current_string.clear()\\n\\n    return result\\n\"), trace_id=None, run_duration=13.07907279196661, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=0.8, reason='Passed 4 out of 5 tests.\\n\\nFailing snippets:\\nfailed: ', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=0.8, reason='**Generation 1:**  \\nSCORE: 1.0  \\nREASON: Clear, concise, and correct. Good naming, algorithm is efficient. Matches Python best practice, handles the problem as described.\\n\\n---\\n\\n**Generation 2:**  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. High clarity and correctness. Best-practice implementation for the specified prompt.\\n\\n---\\n\\n**Generation 3:**  \\nSCORE: 0.0  \\nREASON: Introduces an unnecessary and unexplained 1e-9, which modifies the correct answer and is not justified by the problem. This can introduce precision bugs and is misleading.\\n\\n---\\n\\n**Generation 4:**  \\nSCORE: 1.0  \\nREASON: Same as Generation 1 and 2, clear and correct for the given description.\\n\\n---\\n\\n**Generation 5:**  \\nSCORE: 1.0  \\nREASON: Again, the same as Gen 1, 2, and 4. High quality: concise, readable, and matches Python best practice for this task.\\n\\n---\\n\\n**TOTAL SCORE: 4.0**', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='1e398714-0bac-40a4-ae4e-da3f75e03cf3', created_at='2025-09-30T17:14:43.818595+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate(3.5) == 0.5\\n    assert abs(candidate(1.33) - 0.33) < 1e-6\\n    assert abs(candidate(123.456) - 0.456) < 1e-6\\n\", prompt='\\n\\ndef truncate_number(number: float) -> float:\\n    \"\"\" Given a positive floating point number, it can be decomposed into\\n    and integer part (largest integer smaller than given number) and decimals\\n    (leftover part always smaller than 1).\\n\\n    Return the decimal part of the number.\\n    >>> truncate_number(3.5)\\n    0.5\\n    \"\"\"\\n', task_id='HumanEval/2', entry_point='truncate_number', generated_codes=['def truncate_number(number: float) -> float:\\n    return number - int(number)', 'def truncate_number(number: float) -> float:\\n    return number - int(number)', 'def truncate_number(number: float) -> float:\\n    return number - int(number) + 1e-9', 'def truncate_number(number: float) -> float:\\n    return number - int(number)', 'def truncate_number(number: float) -> float:\\n    return number - int(number)'], canonical_solution='    return number % 1.0\\n'), trace_id=None, run_duration=10.28160200000275, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1:  \\nSCORE: 1.0  \\nREASON: Clear, concise, uses descriptive variable names. Algorithm is efficient (O(n) time), structure is straightforward, and matches the problem requirements. No major best practice violations.\\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. Clear, correct, readable, efficient, and follows Python best practices.\\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Identical to previous generations. Readable, correct, efficient, and follows best practices.\\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: Identical to previous generations. Clear code, appropriate naming, efficient, and correct.\\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Also identical to earlier generations. Follows good style, meets requirements, efficient and readable.\\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='1a7ad540-9f80-4540-b035-399471045dea', created_at='2025-09-30T17:14:43.818615+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([]) == False\\n    assert candidate([1, 2, -3, 1, 2, -3]) == False\\n    assert candidate([1, 2, -4, 5, 6]) == True\\n    assert candidate([1, -1, 2, -2, 5, -5, 4, -4]) == False\\n    assert candidate([1, -1, 2, -2, 5, -5, 4, -5]) == True\\n    assert candidate([1, -2, 2, -2, 5, -5, 4, -4]) == True\\n\", prompt='from typing import List\\n\\n\\ndef below_zero(operations: List[int]) -> bool:\\n    \"\"\" You\\'re given a list of deposit and withdrawal operations on a bank account that starts with\\n    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\\n    at that point function should return True. Otherwise it should return False.\\n    >>> below_zero([1, 2, 3])\\n    False\\n    >>> below_zero([1, 2, -4, 5])\\n    True\\n    \"\"\"\\n', task_id='HumanEval/3', entry_point='below_zero', generated_codes=['def below_zero(operations: List[int]) -> bool:\\n    balance = 0\\n    for op in operations:\\n        balance += op\\n        if balance < 0:\\n            return True\\n    return False', 'def below_zero(operations: List[int]) -> bool:\\n    balance = 0\\n    for op in operations:\\n        balance += op\\n        if balance < 0:\\n            return True\\n    return False', 'def below_zero(operations: List[int]) -> bool:\\n    balance = 0\\n    for op in operations:\\n        balance += op\\n        if balance < 0:\\n            return True\\n    return False', 'def below_zero(operations: List[int]) -> bool:\\n    balance = 0\\n    for op in operations:\\n        balance += op\\n        if balance < 0:\\n            return True\\n    return False', 'def below_zero(operations: List[int]) -> bool:\\n    balance = 0\\n    for op in operations:\\n        balance += op\\n        if balance < 0:\\n            return True\\n    return False'], canonical_solution='    balance = 0\\n\\n    for op in operations:\\n        balance += op\\n        if balance < 0:\\n            return True\\n\\n    return False\\n'), trace_id=None, run_duration=5.113511833013035, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=0.9, reason='Generation 1:\\nSCORE: 1.0  \\nREASON: Clear variable names and structure, concise, handles empty input, uses list comprehension for clarity, follows Python best practices.\\n\\nGeneration 2:\\nSCORE: 1.0  \\nREASON: Uses generator expression (efficient memory), handles empty input, clear naming and logic, maintains readability and best practices.\\n\\nGeneration 3:\\nSCORE: 1.0  \\nREASON: Good structure, readable variable names, input validation, logic clear; use of intermediate variable (n) for length improves clarity.\\n\\nGeneration 4:\\nSCORE: 1.0  \\nREASON: Follows same approach as Gen 2, with slightly different naming for total; clear, efficient, input validated, readable.\\n\\nGeneration 5:\\nSCORE: 0.5  \\nREASON: Fails to handle empty input—possible division by zero. Variable naming is still clear, logic correct otherwise, but lack of robustness is a significant drawback.\\n\\nTOTAL SCORE: 4.5', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='e11cc12e-ad62-4f3b-8127-25379de524dd', created_at='2025-09-30T17:14:43.818636+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert abs(candidate([1.0, 2.0, 3.0]) - 2.0/3.0) < 1e-6\\n    assert abs(candidate([1.0, 2.0, 3.0, 4.0]) - 1.0) < 1e-6\\n    assert abs(candidate([1.0, 2.0, 3.0, 4.0, 5.0]) - 6.0/5.0) < 1e-6\\n\\n\", prompt='from typing import List\\n\\n\\ndef mean_absolute_deviation(numbers: List[float]) -> float:\\n    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\\n    around the mean of this dataset.\\n    Mean Absolute Deviation is the average absolute difference between each\\n    element and a centerpoint (mean in this case):\\n    MAD = average | x - x_mean |\\n    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\\n    1.0\\n    \"\"\"\\n', task_id='HumanEval/4', entry_point='mean_absolute_deviation', generated_codes=['def mean_absolute_deviation(numbers: List[float]) -> float:\\n    if not numbers:\\n        return 0.0\\n    mean_value = sum(numbers) / len(numbers)\\n    deviations = [abs(x - mean_value) for x in numbers]\\n    return sum(deviations) / len(deviations)', 'def mean_absolute_deviation(numbers: List[float]) -> float:\\n    if not numbers:\\n        return 0.0\\n    mean_value = sum(numbers) / len(numbers)\\n    total_deviation = sum(abs(x - mean_value) for x in numbers)\\n    return total_deviation / len(numbers)', 'def mean_absolute_deviation(numbers: List[float]) -> float:\\n    n = len(numbers)\\n    if n == 0:\\n        return 0.0\\n    mean_value = sum(numbers) / n\\n    absolute_differences = [abs(x - mean_value) for x in numbers]\\n    return sum(absolute_differences) / n', 'def mean_absolute_deviation(numbers: List[float]) -> float:\\n    if not numbers:\\n        return 0.0\\n    mean_value = sum(numbers) / len(numbers)\\n    total_absolute_diff = sum(abs(x - mean_value) for x in numbers)\\n    return total_absolute_diff / len(numbers)', 'def mean_absolute_deviation(numbers: List[float]) -> float:\\n    x_mean = sum(numbers) / len(numbers)\\n    return sum(abs(x - x_mean) for x in numbers) / len(numbers)'], canonical_solution='    mean = sum(numbers) / len(numbers)\\n    return sum(abs(x - mean) for x in numbers) / len(numbers)\\n'), trace_id=None, run_duration=6.594942416995764, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1  \\nSCORE: 1.0  \\nREASON: Clear variable names, concise logic, checks for empty input, maintains order, and efficiently inserts delimiters. Follows Python best practices.\\n\\nGeneration 2  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. Clear, efficient, and correct; follows all best practices.\\n\\nGeneration 3  \\nSCORE: 1.0  \\nREASON: Uses enumerate for clarity, logic is clean and readable. Minimal and pythonic code. Handles edge cases correctly.\\n\\nGeneration 4  \\nSCORE: 1.0  \\nREASON: Checks for empty input, clear structure, appends delimiter between elements, good readability. Slightly more verbose but still correct and idiomatic.\\n\\nGeneration 5  \\nSCORE: 1.0  \\nREASON: Logic is clear, explicitly iterates to n-1 elements then appends last item, correctly handles empty input, and is easy to read and maintain.\\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='c3e64a31-1c6c-44a5-86b6-d1a7913402ed', created_at='2025-09-30T17:14:43.818655+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([], 7) == []\\n    assert candidate([5, 6, 3, 2], 8) == [5, 8, 6, 8, 3, 8, 2]\\n    assert candidate([2, 2, 2], 2) == [2, 2, 2, 2, 2]\\n\", prompt='from typing import List\\n\\n\\ndef intersperse(numbers: List[int], delimeter: int) -> List[int]:\\n    \"\"\" Insert a number \\'delimeter\\' between every two consecutive elements of input list `numbers\\'\\n    >>> intersperse([], 4)\\n    []\\n    >>> intersperse([1, 2, 3], 4)\\n    [1, 4, 2, 4, 3]\\n    \"\"\"\\n', task_id='HumanEval/5', entry_point='intersperse', generated_codes=['def intersperse(numbers: List[int], delimeter: int) -> List[int]:\\n    if not numbers:\\n        return []\\n    result = [numbers[0]]\\n    for num in numbers[1:]:\\n        result.append(delimeter)\\n        result.append(num)\\n    return result', 'def intersperse(numbers: List[int], delimeter: int) -> List[int]:\\n    if not numbers:\\n        return []\\n    result = [numbers[0]]\\n    for num in numbers[1:]:\\n        result.append(delimeter)\\n        result.append(num)\\n    return result', 'def intersperse(numbers: List[int], delimeter: int) -> List[int]:\\n    result = []\\n    for i, num in enumerate(numbers):\\n        if i > 0:\\n            result.append(delimeter)\\n        result.append(num)\\n    return result', 'def intersperse(numbers: List[int], delimeter: int) -> List[int]:\\n    if not numbers:\\n        return []\\n    result = []\\n    for i, num in enumerate(numbers):\\n        result.append(num)\\n        if i < len(numbers) - 1:\\n            result.append(delimeter)\\n    return result', 'def intersperse(numbers: List[int], delimeter: int) -> List[int]:\\n    if not numbers:\\n        return []\\n    result = []\\n    for i in range(len(numbers) - 1):\\n        result.append(numbers[i])\\n        result.append(delimeter)\\n    result.append(numbers[-1])\\n    return result'], canonical_solution='    if not numbers:\\n        return []\\n\\n    result = []\\n\\n    for n in numbers[:-1]:\\n        result.append(n)\\n        result.append(delimeter)\\n\\n    result.append(numbers[-1])\\n\\n    return result\\n'), trace_id=None, run_duration=5.933873959002085, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=0.5, reason='Generation 1  \\nSCORE: 0.5  \\nREASON: The code is readable, uses good variable naming, and achieves the intended behavior efficiently. However, it lacks input validation (does not check for unbalanced parentheses) and code comments, and could be improved by handling potential errors (such as decrementing below zero for current_depth). Edge cases and robustness are not fully considered.\\n\\nGeneration 2  \\nSCORE: 0.5  \\nREASON: Identical to Generation 1. The same strengths (clarity, efficiency) and weaknesses (no validation of input, missing comments, potential robustness issues with malformed input).\\n\\nGeneration 3  \\nSCORE: 0.5  \\nREASON: Identical logic and code to previous generations. Same issues and merits apply.\\n\\nGeneration 4  \\nSCORE: 0.5  \\nREASON: This is again an exact duplicate. The same reasoning applies: generally good code but missing error handling and best-practices-level commentary.\\n\\nGeneration 5  \\nSCORE: 0.5  \\nREASON: Once more, identical to all previous generations; the rating and issues remain the same.\\n\\nTOTAL SCORE: 2.5', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='67d3f213-e4a8-4115-8a96-0dee1317864f', created_at='2025-09-30T17:14:43.818671+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('(()()) ((())) () ((())()())') == [2, 3, 1, 3]\\n    assert candidate('() (()) ((())) (((())))') == [1, 2, 3, 4]\\n    assert candidate('(()(())((())))') == [4]\\n\", prompt='from typing import List\\n\\n\\ndef parse_nested_parens(paren_string: str) -> List[int]:\\n    \"\"\" Input to this function is a string represented multiple groups for nested parentheses separated by spaces.\\n    For each of the group, output the deepest level of nesting of parentheses.\\n    E.g. (()()) has maximum two levels of nesting while ((())) has three.\\n\\n    >>> parse_nested_parens(\\'(()()) ((())) () ((())()())\\')\\n    [2, 3, 1, 3]\\n    \"\"\"\\n', task_id='HumanEval/6', entry_point='parse_nested_parens', generated_codes=[\"def parse_nested_parens(paren_string: str) -> List[int]:\\n    result = []\\n    groups = paren_string.split()\\n    for group in groups:\\n        max_depth = 0\\n        current_depth = 0\\n        for char in group:\\n            if char == '(':\\n                current_depth += 1\\n                if current_depth > max_depth:\\n                    max_depth = current_depth\\n            elif char == ')':\\n                current_depth -= 1\\n        result.append(max_depth)\\n    return result\", \"def parse_nested_parens(paren_string: str) -> List[int]:\\n    result = []\\n    groups = paren_string.split()\\n    for group in groups:\\n        max_depth = 0\\n        current_depth = 0\\n        for char in group:\\n            if char == '(':\\n                current_depth += 1\\n                if current_depth > max_depth:\\n                    max_depth = current_depth\\n            elif char == ')':\\n                current_depth -= 1\\n        result.append(max_depth)\\n    return result\", \"def parse_nested_parens(paren_string: str) -> List[int]:\\n    result = []\\n    groups = paren_string.split()\\n    for group in groups:\\n        max_depth = 0\\n        current_depth = 0\\n        for char in group:\\n            if char == '(':\\n                current_depth += 1\\n                if current_depth > max_depth:\\n                    max_depth = current_depth\\n            elif char == ')':\\n                current_depth -= 1\\n        result.append(max_depth)\\n    return result\", \"def parse_nested_parens(paren_string: str) -> List[int]:\\n    result = []\\n    groups = paren_string.split()\\n    for group in groups:\\n        max_depth = 0\\n        current_depth = 0\\n        for char in group:\\n            if char == '(':\\n                current_depth += 1\\n                if current_depth > max_depth:\\n                    max_depth = current_depth\\n            elif char == ')':\\n                current_depth -= 1\\n        result.append(max_depth)\\n    return result\", \"def parse_nested_parens(paren_string: str) -> List[int]:\\n    result = []\\n    groups = paren_string.split()\\n    for group in groups:\\n        max_depth = 0\\n        current_depth = 0\\n        for char in group:\\n            if char == '(':\\n                current_depth += 1\\n                if current_depth > max_depth:\\n                    max_depth = current_depth\\n            elif char == ')':\\n                current_depth -= 1\\n        result.append(max_depth)\\n    return result\"], canonical_solution=\"    def parse_paren_group(s):\\n        depth = 0\\n        max_depth = 0\\n        for c in s:\\n            if c == '(':\\n                depth += 1\\n                max_depth = max(depth, max_depth)\\n            else:\\n                depth -= 1\\n\\n        return max_depth\\n\\n    return [parse_paren_group(x) for x in paren_string.split(' ') if x]\\n\"), trace_id=None, run_duration=7.592877750052139, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1:  \\nSCORE: 1.0  \\nREASON: Simple, clear, and efficient list comprehension, matches specification, proper naming, respects typing.  \\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: Identical to Gen 1, same strengths: clear, concise, efficient, and follows best practices for this simple task.  \\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Same as previous generations; excellent readability, appropriate naming, matches the docstring expectations, and is idiomatic Python.  \\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: Again, identical to previous generations; all best practices followed for this problem’s simplicity.  \\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Exact same implementation as above, all strengths maintained: concise, readable, efficient, and maintainable.  \\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='ff32f5c4-8e2c-458c-ba0c-8a4e3d289881', created_at='2025-09-30T17:14:43.818686+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([], 'john') == []\\n    assert candidate(['xxx', 'asd', 'xxy', 'john doe', 'xxxAAA', 'xxx'], 'xxx') == ['xxx', 'xxxAAA', 'xxx']\\n    assert candidate(['xxx', 'asd', 'aaaxxy', 'john doe', 'xxxAAA', 'xxx'], 'xx') == ['xxx', 'aaaxxy', 'xxxAAA', 'xxx']\\n    assert candidate(['grunt', 'trumpet', 'prune', 'gruesome'], 'run') == ['grunt', 'prune']\\n\", prompt='from typing import List\\n\\n\\ndef filter_by_substring(strings: List[str], substring: str) -> List[str]:\\n    \"\"\" Filter an input list of strings only for ones that contain given substring\\n    >>> filter_by_substring([], \\'a\\')\\n    []\\n    >>> filter_by_substring([\\'abc\\', \\'bacd\\', \\'cde\\', \\'array\\'], \\'a\\')\\n    [\\'abc\\', \\'bacd\\', \\'array\\']\\n    \"\"\"\\n', task_id='HumanEval/7', entry_point='filter_by_substring', generated_codes=['def filter_by_substring(strings: List[str], substring: str) -> List[str]:\\n    return [s for s in strings if substring in s]', 'def filter_by_substring(strings: List[str], substring: str) -> List[str]:\\n    return [s for s in strings if substring in s]', 'def filter_by_substring(strings: List[str], substring: str) -> List[str]:\\n    return [s for s in strings if substring in s]', 'def filter_by_substring(strings: List[str], substring: str) -> List[str]:\\n    return [s for s in strings if substring in s]', 'def filter_by_substring(strings: List[str], substring: str) -> List[str]:\\n    return [s for s in strings if substring in s]'], canonical_solution='    return [x for x in strings if substring in x]\\n'), trace_id=None, run_duration=4.0898098750039935, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1:\\nSCORE: 1.0  \\nREASON: The code is clear, correct, uses descriptive variable names, implements an efficient O(n) solution, handles the empty list case correctly (sum([]) == 0, initial product = 1), and follows standard Python practices.\\n\\nGeneration 2:\\nSCORE: 1.0  \\nREASON: Like Generation 1, the code is clean, clear, and efficient. Minor difference in variable name (\"product\" vs. \"total_product\") does not affect clarity or quality.\\n\\nGeneration 3:\\nSCORE: 1.0  \\nREASON: This is essentially identical to Generation 1, maintaining the same standard of clarity, correct logic, and efficiency.\\n\\nGeneration 4:\\nSCORE: 1.0  \\nREASON: Again, this is functionally and stylistically equivalent to Generations 1 and 3. All criteria for high quality are present.\\n\\nGeneration 5:\\nSCORE: 1.0  \\nREASON: Structurally identical to previous generations, ensuring clarity, correctness, and excellent adherence to best practices.\\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='fd2924ff-0391-40b1-9a24-250ee6bbfb89', created_at='2025-09-30T17:14:43.818706+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([]) == (0, 1)\\n    assert candidate([1, 1, 1]) == (3, 1)\\n    assert candidate([100, 0]) == (100, 0)\\n    assert candidate([3, 5, 7]) == (3 + 5 + 7, 3 * 5 * 7)\\n    assert candidate([10]) == (10, 10)\\n\", prompt='from typing import List, Tuple\\n\\n\\ndef sum_product(numbers: List[int]) -> Tuple[int, int]:\\n    \"\"\" For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.\\n    Empty sum should be equal to 0 and empty product should be equal to 1.\\n    >>> sum_product([])\\n    (0, 1)\\n    >>> sum_product([1, 2, 3, 4])\\n    (10, 24)\\n    \"\"\"\\n', task_id='HumanEval/8', entry_point='sum_product', generated_codes=['def sum_product(numbers: List[int]) -> Tuple[int, int]:\\n    total_sum = sum(numbers)\\n    total_product = 1\\n    for num in numbers:\\n        total_product *= num\\n    return total_sum, total_product', 'def sum_product(numbers: List[int]) -> Tuple[int, int]:\\n    total_sum = sum(numbers)\\n    product = 1\\n    for num in numbers:\\n        product *= num\\n    return (total_sum, product)', 'def sum_product(numbers: List[int]) -> Tuple[int, int]:\\n    total_sum = sum(numbers)\\n    total_product = 1\\n    for num in numbers:\\n        total_product *= num\\n    return (total_sum, total_product)', 'def sum_product(numbers: List[int]) -> Tuple[int, int]:\\n    total_sum = sum(numbers)\\n    total_product = 1\\n    for num in numbers:\\n        total_product *= num\\n    return (total_sum, total_product)', 'def sum_product(numbers: List[int]) -> Tuple[int, int]:\\n    total_sum = sum(numbers)\\n    total_product = 1\\n    for num in numbers:\\n        total_product *= num\\n    return (total_sum, total_product)'], canonical_solution='    sum_value = 0\\n    prod_value = 1\\n\\n    for n in numbers:\\n        sum_value += n\\n        prod_value *= n\\n    return sum_value, prod_value\\n'), trace_id=None, run_duration=5.903886374901049, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='**Generation 1:**  \\nSCORE: 1.0  \\nREASON: Good variable naming, excellent readability, efficiently solves the problem, and robust for all integer input. Maintains clarity and Pythonic structure.\\n\\n---\\n\\n**Generation 2:**  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1: clear, efficient, robust, and fully readable.\\n\\n---\\n\\n**Generation 3:**  \\nSCORE: 1.0  \\nREASON: Identical implementation; same correctness, clarity, and efficiency.\\n\\n---\\n\\n**Generation 4:**  \\nSCORE: 1.0  \\nREASON: Identical implementation to above; same strengths.\\n\\n---\\n\\n**Generation 5:**  \\nSCORE: 1.0  \\nREASON: Only minor variable name change; the code is clear, efficient, readable, and robust.\\n\\n---\\n\\n**TOTAL SCORE:** 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='6eac070f-8ff6-4afa-ad61-8739ed94fc56', created_at='2025-09-30T17:14:43.818724+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([]) == []\\n    assert candidate([1, 2, 3, 4]) == [1, 2, 3, 4]\\n    assert candidate([4, 3, 2, 1]) == [4, 4, 4, 4]\\n    assert candidate([3, 2, 3, 100, 3]) == [3, 3, 3, 100, 100]\\n\", prompt='from typing import List, Tuple\\n\\n\\ndef rolling_max(numbers: List[int]) -> List[int]:\\n    \"\"\" From a given list of integers, generate a list of rolling maximum element found until given moment\\n    in the sequence.\\n    >>> rolling_max([1, 2, 3, 2, 3, 4, 2])\\n    [1, 2, 3, 3, 3, 4, 4]\\n    \"\"\"\\n', task_id='HumanEval/9', entry_point='rolling_max', generated_codes=[\"def rolling_max(numbers: List[int]) -> List[int]:\\n    max_so_far = float('-inf')\\n    result = []\\n    for num in numbers:\\n        if num > max_so_far:\\n            max_so_far = num\\n        result.append(max_so_far)\\n    return result\", \"def rolling_max(numbers: List[int]) -> List[int]:\\n    max_so_far = float('-inf')\\n    result = []\\n    for num in numbers:\\n        if num > max_so_far:\\n            max_so_far = num\\n        result.append(max_so_far)\\n    return result\", \"def rolling_max(numbers: List[int]) -> List[int]:\\n    max_so_far = float('-inf')\\n    result = []\\n    for num in numbers:\\n        if num > max_so_far:\\n            max_so_far = num\\n        result.append(max_so_far)\\n    return result\", \"def rolling_max(numbers: List[int]) -> List[int]:\\n    max_so_far = float('-inf')\\n    result = []\\n    for num in numbers:\\n        if num > max_so_far:\\n            max_so_far = num\\n        result.append(max_so_far)\\n    return result\", \"def rolling_max(numbers: List[int]) -> List[int]:\\n    result = []\\n    current_max = float('-inf')\\n    for num in numbers:\\n        if num > current_max:\\n            current_max = num\\n        result.append(current_max)\\n    return result\"], canonical_solution='    running_max = None\\n    result = []\\n\\n    for n in numbers:\\n        if running_max is None:\\n            running_max = n\\n        else:\\n            running_max = max(running_max, n)\\n\\n        result.append(running_max)\\n\\n    return result\\n'), trace_id=None, run_duration=4.1544419169658795, evaluation_cost=None),\n",
       " ScoringResult(success=False, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=0.8, reason='Passed 4 out of 5 tests.\\n\\nFailing snippets:\\nfailed: ', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=False, score=0.4, reason=\"Generation 1  \\nSCORE: 0.5  \\nREASON: Code is readable and mostly correct, but has redundant code (last return branch is unreachable because the palindrome in the last iteration will always match string[-1:], which is always a palindrome). Does not handle empty strings explicitly except as early return; lacks comments.\\n\\n---\\n\\nGeneration 2  \\nSCORE: 0.5  \\nREASON: Identical to Generation 1. Same reasoning and issues; redundancy and unreachable code, otherwise readable.\\n\\n---\\n\\nGeneration 3  \\nSCORE: 0.0  \\nREASON: The for-loop ranges incorrectly; it checks from n down to -1 (including n), so string[n:] is always '', which is a palindrome, so the first iteration always returns the original string unchanged (even if it's not a palindrome). Fails for non-palindromes, does not solve the original problem.\\n\\n---\\n\\nGeneration 4  \\nSCORE: 0.5  \\nREASON: Logic is nearly correct, but the fallback branch (`return string + string[::-1]`) does not match the intended algorithm (should reverse only the prefix before the palindromic suffix). Readable but does not strictly follow the described algorithm.\\n\\n---\\n\\nGeneration 5  \\nSCORE: 0.5  \\nREASON: Identical to Generations 1 and 2; good readability, but has unreachable code. Handles most cases, but could use clean-up and comments.\\n\\n---\\n\\nTOTAL SCORE: 2.0\", strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='63b826c3-8a4a-42c1-adc6-c9c85079e4ac', created_at='2025-09-30T17:14:43.818739+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('') == ''\\n    assert candidate('x') == 'x'\\n    assert candidate('xyz') == 'xyzyx'\\n    assert candidate('xyx') == 'xyx'\\n    assert candidate('jerry') == 'jerryrrej'\\n\", prompt='\\n\\ndef is_palindrome(string: str) -> bool:\\n    \"\"\" Test if given string is a palindrome \"\"\"\\n    return string == string[::-1]\\n\\n\\ndef make_palindrome(string: str) -> str:\\n    \"\"\" Find the shortest palindrome that begins with a supplied string.\\n    Algorithm idea is simple:\\n    - Find the longest postfix of supplied string that is a palindrome.\\n    - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.\\n    >>> make_palindrome(\\'\\')\\n    \\'\\'\\n    >>> make_palindrome(\\'cat\\')\\n    \\'catac\\'\\n    >>> make_palindrome(\\'cata\\')\\n    \\'catac\\'\\n    \"\"\"\\n', task_id='HumanEval/10', entry_point='make_palindrome', generated_codes=[\"def make_palindrome(string: str) -> str:\\n    if not string:\\n        return ''\\n    for i in range(len(string)):\\n        if is_palindrome(string[i:]):\\n            return string + string[:i][::-1]\\n    return string + string[:-1][::-1]\", \"def make_palindrome(string: str) -> str:\\n    if not string:\\n        return ''\\n    for i in range(len(string)):\\n        if is_palindrome(string[i:]):\\n            return string + string[:i][::-1]\\n    return string + string[:-1][::-1]\", 'def make_palindrome(string: str) -> str:\\n    n = len(string)\\n    for i in range(n, -1, -1):\\n        if is_palindrome(string[i:]):\\n            return string + string[:i][::-1]', 'def make_palindrome(string: str) -> str:\\n    for i in range(len(string)):\\n        if is_palindrome(string[i:]):\\n            return string + string[:i][::-1]\\n    return string + string[::-1]', 'def make_palindrome(string: str) -> str:\\n    for i in range(len(string)):\\n        if is_palindrome(string[i:]):\\n            return string + string[:i][::-1]\\n    return string + string[:-1][::-1]'], canonical_solution=\"    if not string:\\n        return ''\\n\\n    beginning_of_suffix = 0\\n\\n    while not is_palindrome(string[beginning_of_suffix:]):\\n        beginning_of_suffix += 1\\n\\n    return string + string[:beginning_of_suffix][::-1]\\n\"), trace_id=None, run_duration=7.833389000035822, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=0.5, reason=\"Generation 1:  \\nSCORE: 0.5  \\nREASON: Code is short, clear, and correct for input strings of equal length, but there is no input validation or error handling for unequal-length inputs (contradicting the docstring's implication), nor is there a docstring or comments. Assumes inputs are valid, which is not robust.\\n\\nGeneration 2:  \\nSCORE: 0.5  \\nREASON: Identical to Generation 1. Same clarity and shortcomings—good logic but lacks input validation, robustness, and any documentation.\\n\\nGeneration 3:  \\nSCORE: 0.5  \\nREASON: Same as previous generations—concise and follows the problem’s requirements only for equal-length binary strings, but lacks error handling and comments.\\n\\nGeneration 4:  \\nSCORE: 0.5  \\nREASON: Identical implementation, identical issues: clarity and succinctness, but missing input validation and best practice elements for production code.\\n\\nGeneration 5:  \\nSCORE: 0.5  \\nREASON: Code handles unequal-length inputs by zero-padding, which is a minor improvement in robustness. However, the chosen behavior is not specified in the problem, and there is still no type or content validation (e.g., for non-binary input). The logic is correct, minor improvement in handling, but still lacks full best practices.\\n\\nTOTAL SCORE: 2.5\", strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='85c1f404-09f8-4106-afb7-096bc8a0c276', created_at='2025-09-30T17:14:43.818755+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('111000', '101010') == '010010'\\n    assert candidate('1', '1') == '0'\\n    assert candidate('0101', '0000') == '0101'\\n\", prompt='from typing import List\\n\\n\\ndef string_xor(a: str, b: str) -> str:\\n    \"\"\" Input are two strings a and b consisting only of 1s and 0s.\\n    Perform binary XOR on these inputs and return result also as a string.\\n    >>> string_xor(\\'010\\', \\'110\\')\\n    \\'100\\'\\n    \"\"\"\\n', task_id='HumanEval/11', entry_point='string_xor', generated_codes=[\"def string_xor(a: str, b: str) -> str:\\n    return ''.join('1' if x != y else '0' for x, y in zip(a, b))\", \"def string_xor(a: str, b: str) -> str:\\n    return ''.join('1' if x != y else '0' for x, y in zip(a, b))\", \"def string_xor(a: str, b: str) -> str:\\n    return ''.join('1' if x != y else '0' for x, y in zip(a, b))\", \"def string_xor(a: str, b: str) -> str:\\n    return ''.join('1' if x != y else '0' for x, y in zip(a, b))\", \"def string_xor(a: str, b: str) -> str:\\n    max_len = max(len(a), len(b))\\n    a = a.zfill(max_len)\\n    b = b.zfill(max_len)\\n    result = ''.join('1' if x != y else '0' for x, y in zip(a, b))\\n    return result\"], canonical_solution=\"    def xor(i, j):\\n        if i == j:\\n            return '0'\\n        else:\\n            return '1'\\n\\n    return ''.join(xor(x, y) for x, y in zip(a, b))\\n\"), trace_id=None, run_duration=5.767317833960988, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=0.9, reason=\"Generation 1  \\nSCORE: 1.0  \\nREASON: The code is concise, uses clear naming, and leverages Python's built-in `max` function with the `key` argument, ensuring efficiency and readability. It handles the empty list case as required.\\n\\nGeneration 2  \\nSCORE: 0.5  \\nREASON: Loop is explicit and correct, but initializes `max_length` and `result` with the first string, then the loop checks all strings including the first again, making this slightly less efficient and non-idiomatic compared to using `max`. However, correctness and clarity are maintained.\\n\\nGeneration 3  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. Readable, efficient, follows best practices.\\n\\nGeneration 4  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1 and 3. Concise, efficient, clear, and meets all requirements.\\n\\nGeneration 5  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1, 3, and 4. All best practices followed, efficient, clear.\\n\\nTOTAL SCORE: 4.5\", strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='d07d5a63-3efe-4803-85f0-1cde896ce68e', created_at='2025-09-30T17:14:43.81877+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([]) == None\\n    assert candidate(['x', 'y', 'z']) == 'x'\\n    assert candidate(['x', 'yyy', 'zzzz', 'www', 'kkkk', 'abc']) == 'zzzz'\\n\", prompt='from typing import List, Optional\\n\\n\\ndef longest(strings: List[str]) -> Optional[str]:\\n    \"\"\" Out of list of strings, return the longest one. Return the first one in case of multiple\\n    strings of the same length. Return None in case the input list is empty.\\n    >>> longest([])\\n\\n    >>> longest([\\'a\\', \\'b\\', \\'c\\'])\\n    \\'a\\'\\n    >>> longest([\\'a\\', \\'bb\\', \\'ccc\\'])\\n    \\'ccc\\'\\n    \"\"\"\\n', task_id='HumanEval/12', entry_point='longest', generated_codes=['def longest(strings: List[str]) -> Optional[str]:\\n    if not strings:\\n        return None\\n    return max(strings, key=len)', 'def longest(strings: List[str]) -> Optional[str]:\\n    if not strings:\\n        return None\\n    max_length = len(strings[0])\\n    result = strings[0]\\n    for s in strings:\\n        if len(s) > max_length:\\n            max_length = len(s)\\n            result = s\\n    return result', 'def longest(strings: List[str]) -> Optional[str]:\\n    if not strings:\\n        return None\\n    return max(strings, key=len)', 'def longest(strings: List[str]) -> Optional[str]:\\n    if not strings:\\n        return None\\n    return max(strings, key=len)', 'def longest(strings: List[str]) -> Optional[str]:\\n    if not strings:\\n        return None\\n    return max(strings, key=len)'], canonical_solution='    if not strings:\\n        return None\\n\\n    maxlen = max(len(x) for x in strings)\\n    for s in strings:\\n        if len(s) == maxlen:\\n            return s\\n'), trace_id=None, run_duration=4.066764832939953, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1:  \\nSCORE: 1.0  \\nREASON: Clear and concise implementation of the classic Euclidean algorithm for GCD. Readable variable naming, efficient and correct approach. Function signature, docstring, and return type are appropriate. No significant issues with readability or best practices.  \\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1. Efficient, readable, and robust. All best practices are followed for such a function.\\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Identical to previous generations. Efficient, standard implementation with good readability and adherence to best practices.\\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: Identical code, so the assessment remains: the code is clear, idiomatic, efficient, and follows Python best practices.\\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Same as previous generations. The code is concise, readable, and implements the correct algorithm; appropriate for this problem.\\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='443fae08-e438-4f09-acf1-6179ae1b8cd8', created_at='2025-09-30T17:14:43.818787+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate(3, 7) == 1\\n    assert candidate(10, 15) == 5\\n    assert candidate(49, 14) == 7\\n    assert candidate(144, 60) == 12\\n\", prompt='\\n\\ndef greatest_common_divisor(a: int, b: int) -> int:\\n    \"\"\" Return a greatest common divisor of two integers a and b\\n    >>> greatest_common_divisor(3, 5)\\n    1\\n    >>> greatest_common_divisor(25, 15)\\n    5\\n    \"\"\"\\n', task_id='HumanEval/13', entry_point='greatest_common_divisor', generated_codes=['def greatest_common_divisor(a: int, b: int) -> int:\\n    while b:\\n        a, b = b, a % b\\n    return a', 'def greatest_common_divisor(a: int, b: int) -> int:\\n    while b:\\n        a, b = b, a % b\\n    return a', 'def greatest_common_divisor(a: int, b: int) -> int:\\n    while b:\\n        a, b = b, a % b\\n    return a', 'def greatest_common_divisor(a: int, b: int) -> int:\\n    while b:\\n        a, b = b, a % b\\n    return a', 'def greatest_common_divisor(a: int, b: int) -> int:\\n    while b:\\n        a, b = b, a % b\\n    return a'], canonical_solution='    while b:\\n        a, b = b, a % b\\n    return a\\n'), trace_id=None, run_duration=3.9564600420417264, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1:  \\nSCORE: 1.0  \\nREASON: The solution is concise, clear, uses list comprehensions effectively, follows Pythonic conventions, and matches the docstring and sample output. No error-prone constructs are present and the code is readily maintainable.\\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1; maintains high readability, efficiency, and best practices.\\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Same as above—clear, efficient, idiomatic, and meets requirements.\\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: Same as previous generations: readable, efficient, and correct.\\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Same as above; the code is concise, readable, maintains best practices, and works correctly.\\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='510261bc-ff9b-4b8f-a92d-0487fd91608b', created_at='2025-09-30T17:14:43.818802+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('') == []\\n    assert candidate('asdfgh') == ['a', 'as', 'asd', 'asdf', 'asdfg', 'asdfgh']\\n    assert candidate('WWW') == ['W', 'WW', 'WWW']\\n\", prompt='from typing import List\\n\\n\\ndef all_prefixes(string: str) -> List[str]:\\n    \"\"\" Return list of all prefixes from shortest to longest of the input string\\n    >>> all_prefixes(\\'abc\\')\\n    [\\'a\\', \\'ab\\', \\'abc\\']\\n    \"\"\"\\n', task_id='HumanEval/14', entry_point='all_prefixes', generated_codes=['def all_prefixes(string: str) -> List[str]:\\n    return [string[:i] for i in range(1, len(string) + 1)]', 'def all_prefixes(string: str) -> List[str]:\\n    return [string[:i] for i in range(1, len(string) + 1)]', 'def all_prefixes(string: str) -> List[str]:\\n    return [string[:i] for i in range(1, len(string) + 1)]', 'def all_prefixes(string: str) -> List[str]:\\n    return [string[:i] for i in range(1, len(string) + 1)]', 'def all_prefixes(string: str) -> List[str]:\\n    return [string[:i] for i in range(1, len(string) + 1)]'], canonical_solution='    result = []\\n\\n    for i in range(len(string)):\\n        result.append(string[:i+1])\\n    return result\\n'), trace_id=None, run_duration=3.627754708053544, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1:  \\nSCORE: 1.0  \\nREASON: Clear, concise solution using a generator expression and join method; readable and efficient; follows Python best practices for such a simple function.  \\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 1; all best practice standards met, including clarity and robustness for valid input.  \\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Same as above; code is clean, efficient, and idiomatic for this small utility.  \\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: Same implementation as previous generations; fulfills requirements without unnecessary complexity.  \\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Matches other generations; uses appropriate methods and provides full clarity and maintainability for intended use.  \\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='486a6253-0dcd-4266-b8b1-9e5655f6b250', created_at='2025-09-30T17:14:43.818818+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate(0) == '0'\\n    assert candidate(3) == '0 1 2 3'\\n    assert candidate(10) == '0 1 2 3 4 5 6 7 8 9 10'\\n\", prompt='\\n\\ndef string_sequence(n: int) -> str:\\n    \"\"\" Return a string containing space-delimited numbers starting from 0 upto n inclusive.\\n    >>> string_sequence(0)\\n    \\'0\\'\\n    >>> string_sequence(5)\\n    \\'0 1 2 3 4 5\\'\\n    \"\"\"\\n', task_id='HumanEval/15', entry_point='string_sequence', generated_codes=[\"def string_sequence(n: int) -> str:\\n    return ' '.join(str(i) for i in range(n + 1))\", \"def string_sequence(n: int) -> str:\\n    return ' '.join(str(i) for i in range(n + 1))\", \"def string_sequence(n: int) -> str:\\n    return ' '.join(str(i) for i in range(n + 1))\", \"def string_sequence(n: int) -> str:\\n    return ' '.join(str(i) for i in range(n + 1))\", \"def string_sequence(n: int) -> str:\\n    return ' '.join(str(i) for i in range(n + 1))\"], canonical_solution=\"    return ' '.join([str(x) for x in range(n + 1)])\\n\"), trace_id=None, run_duration=3.808459833962843, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1  \\nSCORE: 1.0  \\nREASON: The code uses a generator expression to convert each character to lowercase and collects into a set, ensuring distinct characters regardless of case. The function is concise, readable, and correctly handles the problem as specified. Naming and structure are clear and in accordance with Python best practices.\\n\\nGeneration 2  \\nSCORE: 1.0  \\nREASON: By converting the entire string to lowercase first, then creating a set, this solution is clear, efficient, and equivalent in output to Generation 1. It is concise and follows best practices.\\n\\nGeneration 3  \\nSCORE: 1.0  \\nREASON: Identical to Generation 2; same reasoning applies — clear, efficient, and adheres to Python best practices.\\n\\nGeneration 4  \\nSCORE: 1.0  \\nREASON: Identical to Generations 2 and 3; same strengths as above.\\n\\nGeneration 5  \\nSCORE: 1.0  \\nREASON: Functionally a renamed version of Generation 1, uses a generator expression with clear variable naming. Concise, correct, and matches best practices.\\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='049d9c3b-f6c5-44c6-9a4a-f8512a089904', created_at='2025-09-30T17:14:43.818832+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('') == 0\\n    assert candidate('abcde') == 5\\n    assert candidate('abcde' + 'cade' + 'CADE') == 5\\n    assert candidate('aaaaAAAAaaaa') == 1\\n    assert candidate('Jerry jERRY JeRRRY') == 5\\n\", prompt='\\n\\ndef count_distinct_characters(string: str) -> int:\\n    \"\"\" Given a string, find out how many distinct characters (regardless of case) does it consist of\\n    >>> count_distinct_characters(\\'xyzXYZ\\')\\n    3\\n    >>> count_distinct_characters(\\'Jerry\\')\\n    4\\n    \"\"\"\\n', task_id='HumanEval/16', entry_point='count_distinct_characters', generated_codes=['def count_distinct_characters(string: str) -> int:\\n    return len(set(c.lower() for c in string))', 'def count_distinct_characters(string: str) -> int:\\n    return len(set(string.lower()))', 'def count_distinct_characters(string: str) -> int:\\n    return len(set(string.lower()))', 'def count_distinct_characters(string: str) -> int:\\n    return len(set(string.lower()))', 'def count_distinct_characters(string: str) -> int:\\n    return len(set(char.lower() for char in string))'], canonical_solution='    return len(set(string.lower()))\\n'), trace_id=None, run_duration=4.9492494580335915, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=0.8, reason=\"Generation 1:  \\nSCORE: 1.0  \\nREASON: Clear, concise, readable code with appropriate variable names and direct mapping from input to output. It follows best practices and behaves as expected per the docstring. The lack of input validation is acceptable given the fixed input domain.\\n\\n---\\n\\nGeneration 2:  \\nSCORE: 0.0  \\nREASON: Unnecessarily complex logic with a while loop and token lookahead, handling cases that never occur (splitting always keeps 'o|' together). Some branches are unreachable/misguided, and the design is less clear or maintainable. It also silently ignores unrecognized tokens without feedback.\\n\\n---\\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Clear, concise, appropriate variable names, logic closely matches the requirements, and is easy to read and maintain. Lacks comments but otherwise solid for this simple function.\\n\\n---\\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: Identical to Generation 3. Clear, succinct, matches requirements exactly, readable and maintainable, and idiomatic Python.\\n\\n---\\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Same structure as Generations 1, 3, and 4. Clear, easy to read, directly implements the problem as described, and uses meaningful variable names.\\n\\n---\\n\\nTOTAL SCORE: 4.0\", strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='8309772c-009a-43fc-a9f0-91dcce6d1fbd', created_at='2025-09-30T17:14:43.818846+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('') == []\\n    assert candidate('o o o o') == [4, 4, 4, 4]\\n    assert candidate('.| .| .| .|') == [1, 1, 1, 1]\\n    assert candidate('o| o| .| .| o o o o') == [2, 2, 1, 1, 4, 4, 4, 4]\\n    assert candidate('o| .| o| .| o o| o o|') == [2, 1, 2, 1, 4, 2, 4, 2]\\n\", prompt='from typing import List\\n\\n\\ndef parse_music(music_string: str) -> List[int]:\\n    \"\"\" Input to this function is a string representing musical notes in a special ASCII format.\\n    Your task is to parse this string and return list of integers corresponding to how many beats does each\\n    not last.\\n\\n    Here is a legend:\\n    \\'o\\' - whole note, lasts four beats\\n    \\'o|\\' - half note, lasts two beats\\n    \\'.|\\' - quater note, lasts one beat\\n\\n    >>> parse_music(\\'o o| .| o| o| .| .| .| .| o o\\')\\n    [4, 2, 1, 2, 2, 1, 1, 1, 1, 4, 4]\\n    \"\"\"\\n', task_id='HumanEval/17', entry_point='parse_music', generated_codes=[\"def parse_music(music_string: str) -> List[int]:\\n    notes = music_string.split()\\n    result = []\\n    for note in notes:\\n        if note == 'o':\\n            result.append(4)\\n        elif note == 'o|':\\n            result.append(2)\\n        elif note == '.|':\\n            result.append(1)\\n    return result\", \"def parse_music(music_string: str) -> List[int]:\\n    notes = music_string.split()\\n    result = []\\n    i = 0\\n    while i < len(notes):\\n        note = notes[i]\\n        if note == 'o':\\n            result.append(4)\\n            i += 1\\n        elif note == 'o|':\\n            result.append(2)\\n            i += 1\\n        elif note == '.|':\\n            result.append(1)\\n            i += 1\\n        elif note == 'o' and i + 1 < len(notes) and notes[i + 1] == '|':\\n            result.append(2)\\n            i += 2\\n        elif note == '.' and i + 1 < len(notes) and notes[i + 1] == '|':\\n            result.append(1)\\n            i += 2\\n        else:\\n            # For safety, treat unrecognized tokens as 0\\n            i += 1\\n    return result\", \"def parse_music(music_string: str) -> List[int]:\\n    tokens = music_string.split()\\n    result = []\\n    for token in tokens:\\n        if token == 'o':\\n            result.append(4)\\n        elif token == 'o|':\\n            result.append(2)\\n        elif token == '.|':\\n            result.append(1)\\n    return result\", \"def parse_music(music_string: str) -> List[int]:\\n    tokens = music_string.split()\\n    result = []\\n    for token in tokens:\\n        if token == 'o':\\n            result.append(4)\\n        elif token == 'o|':\\n            result.append(2)\\n        elif token == '.|':\\n            result.append(1)\\n    return result\", \"def parse_music(music_string: str) -> List[int]:\\n    notes = music_string.split()\\n    result = []\\n    for note in notes:\\n        if note == 'o':\\n            result.append(4)\\n        elif note == 'o|':\\n            result.append(2)\\n        elif note == '.|':\\n            result.append(1)\\n    return result\"], canonical_solution=\"    note_map = {'o': 4, 'o|': 2, '.|': 1}\\n    return [note_map[x] for x in music_string.split(' ') if x]\\n\"), trace_id=None, run_duration=7.668064167024568, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason=\"Let's review each code generation:\\n\\n---\\n\\n**Generation 1**\\n\\n**SCORE:** 1.0  \\n**REASON:** The implementation is concise and readable, correctly handles overlapping substrings, and uses well-named variables. Follows Python best practices, and the logic matches the docstring examples.\\n\\n---\\n\\n**Generation 2**\\n\\n**SCORE:** 1.0  \\n**REASON:** This is identical to Generation 1. It repeats the same readable, correct approach and meets all best practices.\\n\\n---\\n\\n**Generation 3**\\n\\n**SCORE:** 1.0  \\n**REASON:** The logic is equivalent to Generations 1 and 2, with a minor naming difference (`index` instead of `start` for the result of `find`). This arguably makes the code slightly clearer by separating the index found from the starting index. Still, it is clear, robust, and follows best practices.\\n\\n---\\n\\n**Generation 4**\\n\\n**SCORE:** 1.0  \\n**REASON:** Identical to Generations 1 and 2. No issues; the approach is standard, clear, and aligns with best practices.\\n\\n---\\n\\n**Generation 5**\\n\\n**SCORE:** 1.0  \\n**REASON:** Again, this is the same as Generations 1, 2, and 4, clear, efficient, and correct.\\n\\n---\\n\\n### TOTAL SCORE: 5.0\\n\\n**Summary:**  \\nAll generations are functionally identical or equivalent and demonstrate high-quality, clear, and idiomatic Python code. All merit the highest rating.\", strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='252311db-3675-47f3-85a9-d99c0d836159', created_at='2025-09-30T17:14:43.818864+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('', 'x') == 0\\n    assert candidate('xyxyxyx', 'x') == 4\\n    assert candidate('cacacacac', 'cac') == 4\\n    assert candidate('john doe', 'john') == 1\\n\", prompt='\\n\\ndef how_many_times(string: str, substring: str) -> int:\\n    \"\"\" Find how many times a given substring can be found in the original string. Count overlaping cases.\\n    >>> how_many_times(\\'\\', \\'a\\')\\n    0\\n    >>> how_many_times(\\'aaa\\', \\'a\\')\\n    3\\n    >>> how_many_times(\\'aaaa\\', \\'aa\\')\\n    3\\n    \"\"\"\\n', task_id='HumanEval/18', entry_point='how_many_times', generated_codes=['def how_many_times(string: str, substring: str) -> int:\\n    count = 0\\n    start = 0\\n    while True:\\n        start = string.find(substring, start)\\n        if start == -1:\\n            break\\n        count += 1\\n        start += 1\\n    return count', 'def how_many_times(string: str, substring: str) -> int:\\n    count = 0\\n    start = 0\\n    while True:\\n        start = string.find(substring, start)\\n        if start == -1:\\n            break\\n        count += 1\\n        start += 1\\n    return count', 'def how_many_times(string: str, substring: str) -> int:\\n    count = 0\\n    start = 0\\n    while True:\\n        index = string.find(substring, start)\\n        if index == -1:\\n            break\\n        count += 1\\n        start = index + 1\\n    return count', 'def how_many_times(string: str, substring: str) -> int:\\n    count = 0\\n    start = 0\\n    while True:\\n        start = string.find(substring, start)\\n        if start == -1:\\n            break\\n        count += 1\\n        start += 1\\n    return count', 'def how_many_times(string: str, substring: str) -> int:\\n    count = 0\\n    start = 0\\n    while True:\\n        start = string.find(substring, start)\\n        if start == -1:\\n            break\\n        count += 1\\n        start += 1\\n    return count'], canonical_solution='    times = 0\\n\\n    for i in range(len(string) - len(substring) + 1):\\n        if string[i:i+len(substring)] == substring:\\n            times += 1\\n\\n    return times\\n'), trace_id=None, run_duration=5.734770499984734, evaluation_cost=None),\n",
       " ScoringResult(success=True, scorers_data=[ScorerData(id=None, name='Pass@1 for 5 generations', threshold=0.5, success=True, score=1.0, reason='Passed 5 out of 5 tests.\\n\\nFailing snippets:\\n', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None), ScorerData(id=None, name='Code Quality for 5 generations', threshold=0.5, success=True, score=1.0, reason='Generation 1:  \\nSCORE: 1.0  \\nREASON: Clear variable naming, concise structure, efficient use of sorting with a key. Mapping is readable, and code is easy to follow. Handles all valid cases as specified, though error handling could be improved (for invalid words), but problem statement suggests inputs are valid.\\n\\nGeneration 2:  \\nSCORE: 1.0  \\nREASON: Clear, readable, and efficient. Follows Python best practices, variable names are appropriate, and code is maintainable. Same as Generation 1 with slight naming differences.\\n\\nGeneration 3:  \\nSCORE: 1.0  \\nREASON: Clean and well-organized; variable naming (numeral_map, inverse_map) is descriptive. Algorithm is efficient, with clear logic and good maintainability.\\n\\nGeneration 4:  \\nSCORE: 1.0  \\nREASON: High readability, well-structured, clear intent. Variable and function names are appropriate. Implementation is efficient and maintainable.\\n\\nGeneration 5:  \\nSCORE: 1.0  \\nREASON: Uses clear and descriptive variable names, with efficient logic and good readability. Follows best practices.\\n\\nTOTAL SCORE: 5.0', strict_mode=False, evaluation_model=None, error=None, additional_metadata=None)], name=None, data_object=Example(example_id='0f45d1cc-e0ec-4780-87df-0106bf2c58d7', created_at='2025-09-30T17:14:43.818879+00:00', name=None, dataset_id='821d25f1-10ab-48ed-b6eb-f9f409e4cbe4', test=\"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('') == ''\\n    assert candidate('three') == 'three'\\n    assert candidate('three five nine') == 'three five nine'\\n    assert candidate('five zero four seven nine eight') == 'zero four five seven eight nine'\\n    assert candidate('six five four three two one zero') == 'zero one two three four five six'\\n\", prompt='from typing import List\\n\\n\\ndef sort_numbers(numbers: str) -> str:\\n    \"\"\" Input is a space-delimited string of numberals from \\'zero\\' to \\'nine\\'.\\n    Valid choices are \\'zero\\', \\'one\\', \\'two\\', \\'three\\', \\'four\\', \\'five\\', \\'six\\', \\'seven\\', \\'eight\\' and \\'nine\\'.\\n    Return the string with numbers sorted from smallest to largest\\n    >>> sort_numbers(\\'three one five\\')\\n    \\'one three five\\'\\n    \"\"\"\\n', task_id='HumanEval/19', entry_point='sort_numbers', generated_codes=[\"def sort_numbers(numbers: str) -> str:\\n    mapping = {\\n        'zero': 0,\\n        'one': 1,\\n        'two': 2,\\n        'three': 3,\\n        'four': 4,\\n        'five': 5,\\n        'six': 6,\\n        'seven': 7,\\n        'eight': 8,\\n        'nine': 9\\n    }\\n    reverse_mapping = {v: k for k, v in mapping.items()}\\n    nums = numbers.split()\\n    sorted_nums = sorted(nums, key=lambda x: mapping[x])\\n    return ' '.join(sorted_nums)\", \"def sort_numbers(numbers: str) -> str:\\n    num_map = {\\n        'zero': 0,\\n        'one': 1,\\n        'two': 2,\\n        'three': 3,\\n        'four': 4,\\n        'five': 5,\\n        'six': 6,\\n        'seven': 7,\\n        'eight': 8,\\n        'nine': 9\\n    }\\n    reverse_map = {v: k for k, v in num_map.items()}\\n    num_list = numbers.split()\\n    sorted_nums = sorted(num_list, key=lambda x: num_map[x])\\n    return ' '.join(sorted_nums)\", \"def sort_numbers(numbers: str) -> str:\\n    numeral_map = {\\n        'zero': 0,\\n        'one': 1,\\n        'two': 2,\\n        'three': 3,\\n        'four': 4,\\n        'five': 5,\\n        'six': 6,\\n        'seven': 7,\\n        'eight': 8,\\n        'nine': 9\\n    }\\n    inverse_map = {v: k for k, v in numeral_map.items()}\\n    num_list = numbers.split()\\n    sorted_nums = sorted(num_list, key=lambda x: numeral_map[x])\\n    return ' '.join(sorted_nums)\", \"def sort_numbers(numbers: str) -> str:\\n    num_map = {\\n        'zero': 0,\\n        'one': 1,\\n        'two': 2,\\n        'three': 3,\\n        'four': 4,\\n        'five': 5,\\n        'six': 6,\\n        'seven': 7,\\n        'eight': 8,\\n        'nine': 9\\n    }\\n    reverse_map = {v: k for k, v in num_map.items()}\\n    number_list = numbers.split()\\n    sorted_numbers = sorted(number_list, key=lambda x: num_map[x])\\n    return ' '.join(sorted_numbers)\", \"def sort_numbers(numbers: str) -> str:\\n    word_to_num = {\\n        'zero': 0,\\n        'one': 1,\\n        'two': 2,\\n        'three': 3,\\n        'four': 4,\\n        'five': 5,\\n        'six': 6,\\n        'seven': 7,\\n        'eight': 8,\\n        'nine': 9\\n    }\\n    num_to_word = {v: k for k, v in word_to_num.items()}\\n    words = numbers.split()\\n    sorted_nums = sorted(words, key=lambda w: word_to_num[w])\\n    return ' '.join(sorted_nums)\"], canonical_solution=\"    value_map = {\\n        'zero': 0,\\n        'one': 1,\\n        'two': 2,\\n        'three': 3,\\n        'four': 4,\\n        'five': 5,\\n        'six': 6,\\n        'seven': 7,\\n        'eight': 8,\\n        'nine': 9\\n    }\\n    return ' '.join(sorted([x for x in numbers.split(' ') if x], key=lambda x: value_map[x]))\\n\"), trace_id=None, run_duration=5.411400791956112, evaluation_cost=None)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n⚡ Running evaluation...\")\n",
    "judgment.run_evaluation(\n",
    "    examples=dataset.examples,\n",
    "    scorers=[CodeExecutionScorer(k=1, n=num_generations), CodeQualityScorer(n=num_generations)],\n",
    "    project_name=\"humaneval-project\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a58b3",
   "metadata": {},
   "source": [
    "Click **View Results** to open the dashboard. You should see something like this — be sure to explore the **Tests** page and the analytics panels to view detailed results and insights.  \n",
    "\n",
    "\n",
    "![Dashboard Screenshot](./assets/offline_tests.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

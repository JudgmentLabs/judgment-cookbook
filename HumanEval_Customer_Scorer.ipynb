{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f43c5a36",
   "metadata": {},
   "source": [
    "# Custom Scorers with HumanEval\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JudgmentLabs/judgment-cookbook/blob/refactor/HumanEval_Custom_Scorer.ipynb)\n",
    "[![Docs](https://img.shields.io/badge/Documentation-blue)](https://docs.judgmentlabs.ai/documentation)\n",
    "\n",
    "In this notebook, you will learn how to evaluate code generation on OpenAI's [HumanEval](https://github.com/openai/human-eval) benchmark and create **custom scorers** that are code-based and LLM-as-a-Judge using the [`judgeval`](https://github.com/JudgmentLabs/judgeval) library. \n",
    "\n",
    "1. **Code Execution Scorer**: Uses sandboxed code execution to evaluate code correctness\n",
    "2. **LLM-as-a-Judge Scorer**: Uses language models to evaluate code quality\n",
    "\n",
    "You will generate code using LLMs, create a custom scorers that leverages OpenAI's sandboxed environment and LLM-as-a-Judge, and evaluate it on the HumanEval benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17740081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "# Installations\n",
    "!pip install human-eval datasets openai judgeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0220c8",
   "metadata": {},
   "source": [
    "To run this notebook, select **Runtime* -> Run All*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7767e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "str expected, not ellipsis",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mOPENAI_API_KEY\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m = ...\n\u001b[32m      6\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mJUDGMENT_API_KEY\u001b[39m\u001b[33m\"\u001b[39m] = ...\n\u001b[32m      7\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mJUDGMENT_ORG_ID\u001b[39m\u001b[33m\"\u001b[39m] = ...\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:719\u001b[39m, in \u001b[36m__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:793\u001b[39m, in \u001b[36mencode\u001b[39m\u001b[34m(value)\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: str expected, not ellipsis"
     ]
    }
   ],
   "source": [
    "# set api keys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ...\n",
    "os.environ[\"JUDGMENT_API_KEY\"] = ...\n",
    "os.environ[\"JUDGMENT_ORG_ID\"] = ...\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624525fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 12:58:22 - judgeval - WARNING - UPDATE AVAILABLE: You are using 'judgeval==0.11.0', but the latest version is '0.12.0'. While this version is still supported, we recommend upgrading to avoid potential issues or missing features: `pip install --upgrade judgeval`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewli/judgment-cookbook/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from judgeval import JudgmentClient\n",
    "from judgeval.dataset import Dataset\n",
    "from judgeval.scorers.example_scorer import ExampleScorer\n",
    "from judgeval.data import Example\n",
    "from datasets import load_dataset\n",
    "from human_eval.execution import check_correctness\n",
    "from openai import AsyncOpenAI\n",
    "from typing import Dict, Any\n",
    "import asyncio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd9cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "judgment = JudgmentClient()\n",
    "client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f5296",
   "metadata": {},
   "source": [
    "## 1. Understanding HumanEval\n",
    "\n",
    "HumanEval is a benchmark dataset created by OpenAI for evaluating code generation models. Introduced in the paper [\"Evaluating Large Language Models Trained on Code\"](https://arxiv.org/abs/2107.03374), it contains 164 Python programming problems designed to test functional correctness.\n",
    "\n",
    "### What HumanEval Contains\n",
    "Each problem includes:\n",
    "- **Function signature and docstring**: The problem description\n",
    "- **Test cases**: Automated tests to verify correctness  \n",
    "- **Canonical solution**: Reference implementation\n",
    "- **Entry point**: Function name to test\n",
    "\n",
    "### How HumanEval Evaluates Code\n",
    "HumanEval evaluates model outputs by dynamically building a Python program that stitches together the **Function signature and docstring**, the **model’s generated solution**, and the **test cases**. This combined program is then executed in a sandbox to verify whether the generated code passes all test cases.\n",
    "\n",
    "The ```check_correctness``` function orchestrates this process: assembles the prompt, generated solution, and tests into a single program, and executes the script in a sandboxed environment.\n",
    "\n",
    "```python\n",
    "# Construct the check program and run it.\n",
    "check_program = (\n",
    "    problem[\"prompt\"] +      # Function signature + docstring\n",
    "    completion +            # Generated code\n",
    "    \"\\n\" +\n",
    "    problem[\"test\"] +       # Test cases\n",
    "    \"\\n\" +\n",
    "    f\"check({problem['entry_point']})\"  # Call the test function\n",
    ")\n",
    "\n",
    "...\n",
    "\n",
    "# WARNING: This executes untrusted model-generated code\n",
    "exec(check_program, exec_globals)\n",
    "```\n",
    "\n",
    "The evaluation is **pass/fail**: if all test cases pass without exceptions, the code is correct. If any test fails or the code crashes, it's incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8775f3",
   "metadata": {},
   "source": [
    "## Custom Code Execution Scorer\n",
    "\n",
    "We'll create a custom scorer using `judgeval` that integrates HumanEval's sandboxed code execution. The scorer uses the `check_correctness` function to evaluate whether generated code passes the test cases.\n",
    "\n",
    "Instead of stopping at a simple pass/fail, we extend this to compute the **Pass@k** statistic. Pass@k measures the probability that at least one of the top-k generated solutions is correct, given:\n",
    "\n",
    "- **n** = total number of generated solutions  \n",
    "- **c** = number of correct solutions among them  \n",
    "- **k** = how many solutions we sample  \n",
    "\n",
    "For example, if we generate **n = 5** completions and some are correct, we can compute **Pass@1** (the chance a single sampled solution is correct) and **Pass@3** (the chance at least one out of three sampled solutions is correct). This provides a more practical view of model performance, since in real use cases we usually sample multiple completions and care whether any of them solves the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1abd3b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator(n: int, c: int, k: int) -> float:\n",
    "    if n - c < k:\n",
    "        return 1.0\n",
    "    return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a74a3",
   "metadata": {},
   "source": [
    "Next, we'll integrate the `estimator` function with the `check_correctness` function to build a custom scorer called `CodeExecutionScorer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8af9e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeExecutionScorer(ExampleScorer):\n",
    "    \"\"\"\n",
    "    A scorer for evaluating code generation using check_correctness\n",
    "    and Pass@k statistics.\n",
    "    \"\"\"\n",
    "    #default values\n",
    "    k: int = 1\n",
    "    n: int = 1\n",
    "    name: str = f\"Pass@{k} for {n} generations\"\n",
    "\n",
    "    async def a_score_example(self, example: Example) -> None:\n",
    "        \"\"\"\n",
    "        Score an example by running the generated code against test cases.\n",
    "        \n",
    "        This method uses check_correctness to execute the generated code\n",
    "        in a sandboxed environment and check if it passes all test cases.\n",
    "        \n",
    "        Args:\n",
    "            example (HumanEvalExample): The example containing the problem and generated code\n",
    "            \n",
    "        Returns:\n",
    "            float: The score (1.0 if all tests pass, 0.0 otherwise)\n",
    "        \"\"\"\n",
    "        # Create problem dict in the format expected by check_correctness\n",
    "        problem = {\n",
    "            \"task_id\": example.task_id,\n",
    "            \"prompt\": example.prompt,\n",
    "            \"test\": example.test,\n",
    "            \"entry_point\": example.entry_point\n",
    "        }\n",
    "\n",
    "        \n",
    "        # Use check_correctness to evaluate the generated code\n",
    "        self.n = len(example.generated_codes)\n",
    "        failed_results = []\n",
    "\n",
    "        for i in range(self.n):\n",
    "            result = check_correctness(\n",
    "                problem=problem,\n",
    "                completion=example.generated_codes[i],\n",
    "                timeout=3.0\n",
    "            )\n",
    "\n",
    "            if not result[\"passed\"]:\n",
    "                failed_results.append(result['result'])\n",
    "\n",
    "        c = self.n - len(failed_results)\n",
    "\n",
    "        pass_k = estimator(self.n, c, self.k)\n",
    "        \n",
    "        self.score = pass_k\n",
    "        self.reason = (\n",
    "            f\"Passed {self.n - len(failed_results)} out of {self.n} tests.\\n\\n\"\n",
    "            \"Failing snippets:\\n\"\n",
    "            + \"\\n---\\n\".join(failed_results)\n",
    "        )        \n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631d91b",
   "metadata": {},
   "source": [
    "## Custom LLM-as-a-Judge Scorer\n",
    "\n",
    "\n",
    "We'll then create another custom scorer with `judgeval` that integrates **LLM-as-a-Judge**, using a language model to assess generated code against a clear rubric emphasizing **readability**, **efficiency**, and **adherence to best practices**.\n",
    "\n",
    "Unlike the pass/fail execution check, this scorer evaluates multiple generations and returns a **0–n** score (where n = number of generations), with each generation rated as:\n",
    "- **High Quality (1.0)**: Excellent readability, efficiency, and best practices\n",
    "- **Medium Quality (0.5)**: Good overall with minor issues\n",
    "- **Low Quality (0.0)**: Significant problems\n",
    "\n",
    "The total score is the sum across all generations, giving you both individual generation quality and overall performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08900129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeQualityScorer(ExampleScorer):\n",
    "    \"\"\"\n",
    "    A scorer for evaluating code generation using LLM-as-a-Judge.\n",
    "    \"\"\"\n",
    "\n",
    "    #default values\n",
    "    n: int = 1\n",
    "    name: str = f\"Code Quality out of {n} generations\"\n",
    "\n",
    "    async def a_score_example(self, example: Example) -> None:\n",
    "        \"\"\"\n",
    "        Score an example by running the generated code against LLM-as-a-Judge.\n",
    "        \n",
    "        Args:\n",
    "            example (HumanEvalExample): The example containing the problem and generated code\n",
    "            \n",
    "        Returns:\n",
    "            float: The score is the sum of individual generation ratings (1.0 for high quality, 0.5 for medium quality, 0.0 for low quality) across n generations        \"\"\"\n",
    "\n",
    "\n",
    "        generations_text = \"\\n\".join([\n",
    "            f\"Generation {i+1}:\\n{example.generated_codes[i]}\\n\" \n",
    "            for i in range(self.n)\n",
    "        ])\n",
    "\n",
    "        prompt = f\"\"\"You are an expert code reviewer. Evaluate each code generation below.\n",
    "\n",
    "        Rate each as:\n",
    "        - **HIGH QUALITY (1.0)**: Excellent readability (clear naming, structure, comments), efficient algorithms, follows best practices (input validation, robustness, maintainability)\n",
    "        - **MEDIUM QUALITY (0.5)**: Good overall with minor issues in readability, efficiency, or best practices  \n",
    "        - **LOW QUALITY (0.0)**: Significant problems with readability, efficiency, or best practices\n",
    "\n",
    "        Consider: Code clarity, naming, algorithm efficiency, error handling, organization, Python best practices.\n",
    "\n",
    "        Problem: {example.prompt}\n",
    "\n",
    "        {generations_text}\n",
    "\n",
    "        Output only the total quality score as a number (sum of all generation ratings, e.g., 4.5):\"\"\"\n",
    "\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert code reviewer. Evaluate each code generation below and return only the total quality score as a number (sum of all generation ratings, e.g., 4.5).\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        total_score = int(response.choices[0].message.content) / self.n\n",
    "\n",
    "        self.score = total_score\n",
    "        self.reason = f\"Total quality score: {total_score}\"\n",
    "        \n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35205b8",
   "metadata": {},
   "source": [
    "## Code Generation Function\n",
    "\n",
    "Next, we’ll implement a function that, given a HumanEval problem, queries an LLM to produce a candidate implementation. The function is written with `async/await` so multiple problems can be evaluated in parallel, significantly reducing total runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd2f6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_code(problem: Dict[str, Any]) -> str:\n",
    "    \"\"\"Generate code using LLM for a given HumanEval problem.\"\"\"\n",
    "    prompt = problem[\"prompt\"]\n",
    "    \n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Python programmer. Write ONLY the Python function code that solves the given problem. Do not include any markdown formatting, explanations, or code blocks. Return only the raw Python code.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    \n",
    "    generated_code = response.choices[0].message.content\n",
    "    \n",
    "    return generated_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971f166",
   "metadata": {},
   "source": [
    "## Load HumanEval Dataset\n",
    "\n",
    "Now let's load the HumanEval [dataset](https://huggingface.co/datasets/openai/openai_humaneval) from Hugging Face and examine its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dee8c284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading HumanEval dataset...\n",
      "   Found 164 problems\n",
      "\n",
      "📋 Example problem structure:\n",
      "   Task ID: HumanEval/0\n",
      "   Entry Point: has_close_elements\n",
      "   Prompt length: 348 characters\n",
      "   Test length: 531 characters\n",
      "\n",
      "📝 Sample prompt:\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given thr...\n"
     ]
    }
   ],
   "source": [
    "# Load the HumanEval dataset\n",
    "print(\"📊 Loading HumanEval dataset...\")\n",
    "dataset = load_dataset(\"openai/openai_humaneval\")\n",
    "print(f\"   Found {len(dataset['test'])} problems\")\n",
    "\n",
    "# Examine the structure of a single problem\n",
    "example_problem = dataset[\"test\"][0]\n",
    "print(\"\\n📋 Example problem structure:\")\n",
    "print(f\"   Task ID: {example_problem['task_id']}\")\n",
    "print(f\"   Entry Point: {example_problem['entry_point']}\")\n",
    "print(f\"   Prompt length: {len(example_problem['prompt'])} characters\")\n",
    "print(f\"   Test length: {len(example_problem['test'])} characters\")\n",
    "\n",
    "print(\"\\n📝 Sample prompt:\")\n",
    "print(example_problem['prompt'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8253bf72",
   "metadata": {},
   "source": [
    "Generate code responses for each problem in the HumanEval benchmark and create example objects to upload into `judgeval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2856d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Generating code...\n",
      "   Problem 1/5: HumanEval/0\n",
      "   Problem 2/5: HumanEval/1\n",
      "   Problem 3/5: HumanEval/2\n",
      "   Problem 4/5: HumanEval/3\n",
      "   Problem 5/5: HumanEval/4\n",
      "   Problem 6/5: HumanEval/5\n",
      "   Problem 7/5: HumanEval/6\n",
      "   Problem 8/5: HumanEval/7\n",
      "   Problem 9/5: HumanEval/8\n",
      "   Problem 10/5: HumanEval/9\n",
      "   Problem 11/5: HumanEval/10\n",
      "   Problem 12/5: HumanEval/11\n",
      "   Problem 13/5: HumanEval/12\n",
      "   Problem 14/5: HumanEval/13\n",
      "   Problem 15/5: HumanEval/14\n",
      "   Problem 16/5: HumanEval/15\n",
      "   Problem 17/5: HumanEval/16\n",
      "   Problem 18/5: HumanEval/17\n",
      "   Problem 19/5: HumanEval/18\n",
      "   Problem 20/5: HumanEval/19\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🤖 Generating code...\")\n",
    "problems = list(dataset[\"test\"].select(range(20)))\n",
    "\n",
    "num_generations = 5\n",
    "\n",
    "# Generate all code in parallel\n",
    "generations = [\n",
    "    await asyncio.gather(*[generate_code(problem) for _ in range(num_generations)])\n",
    "    for problem in problems\n",
    "]\n",
    "\n",
    "# Create examples\n",
    "examples = []\n",
    "for i, (problem, generated_codes) in enumerate(zip(problems, generations)):\n",
    "    print(f\"   Problem {i+1}/5: {problem['task_id']}\")\n",
    "    \n",
    "    example = Example(\n",
    "        task_id=problem[\"task_id\"],\n",
    "        prompt=problem[\"prompt\"],\n",
    "        canonical_solution=problem[\"canonical_solution\"],\n",
    "        test=problem[\"test\"],\n",
    "        entry_point=problem[\"entry_point\"],\n",
    "        generated_codes=generated_codes\n",
    "    )\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a0bec",
   "metadata": {},
   "source": [
    "We use `Dataset.create()` to create a new dataset and upload it to the [Judgment](https://app.judgmentlabs.ai/app) platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "487e2b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 12:59:10 - judgeval - INFO - Successfully created dataset humaneval-dataset!\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.create(\n",
    "    name=\"humaneval-dataset\", \n",
    "    project_name=\"humaneval-project\", \n",
    "    examples=examples,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef3345",
   "metadata": {},
   "source": [
    "We use `Dataset.get()` to retrieve an existing dataset from the [Judgment](https://app.judgmentlabs.ai/app) platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94ea79b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 12:59:11 - judgeval - INFO - Successfully retrieved dataset humaneval-dataset!\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.get(\n",
    "    name=\"humaneval-dataset\",\n",
    "    project_name=\"humaneval-project\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aac9879",
   "metadata": {},
   "source": [
    "## Running Evaluation\n",
    "\n",
    "We then use our custom scorer and the judgment client to run evaluations asynchronously on our servers and display the results on the judgment platform for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49865caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚡ Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 20 example(s) in parallel: |██████████|100% (20/20) [Time Taken: 00:06,  3.08Example/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 13:08:12 - judgeval - ERROR - Failed to save evaluation results to DB: 500: Unexpected error logging custom evaluation results: {'message': 'duplicate key value violates unique constraint \"examples_pkey\"', 'code': '23505', 'hint': None, 'details': 'Key (example_id)=(2425cafe-1ad0-498b-9aaf-1132c48f85d2) already exists.'}\n"
     ]
    },
    {
     "ename": "JudgmentRuntimeError",
     "evalue": "An unexpected error occured during evaluation: Request failed while saving evaluation results to DB: 500: Unexpected error logging custom evaluation results: {'message': 'duplicate key value violates unique constraint \"examples_pkey\"', 'code': '23505', 'hint': None, 'details': 'Key (example_id)=(2425cafe-1ad0-498b-9aaf-1132c48f85d2) already exists.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJudgmentAPIError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/judgment-cookbook/.venv/lib/python3.12/site-packages/judgeval/evaluation/__init__.py:69\u001b[39m, in \u001b[36mlog_evaluation_results\u001b[39m\u001b[34m(scoring_results, run)\u001b[39m\n\u001b[32m     68\u001b[39m api_client = JudgmentSyncClient(JUDGMENT_API_KEY, JUDGMENT_ORG_ID)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m response = \u001b[43mapi_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_eval_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresults\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwarnings\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m url = response.get(\u001b[33m\"\u001b[39m\u001b[33mui_results_url\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/judgment-cookbook/.venv/lib/python3.12/site-packages/judgeval/api/__init__.py:101\u001b[39m, in \u001b[36mJudgmentSyncClient.log_eval_results\u001b[39m\u001b[34m(self, payload)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_eval_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, payload: EvalResults) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_for\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/log_eval_results/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/judgment-cookbook/.venv/lib/python3.12/site-packages/judgeval/api/__init__.py:58\u001b[39m, in \u001b[36mJudgmentSyncClient._request\u001b[39m\u001b[34m(self, method, url, payload, params)\u001b[39m\n\u001b[32m     51\u001b[39m     r = \u001b[38;5;28mself\u001b[39m.client.request(\n\u001b[32m     52\u001b[39m         method,\n\u001b[32m     53\u001b[39m         url,\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m         headers=_headers(\u001b[38;5;28mself\u001b[39m.api_key, \u001b[38;5;28mself\u001b[39m.organization_id),\n\u001b[32m     57\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_handle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/judgment-cookbook/.venv/lib/python3.12/site-packages/judgeval/api/__init__.py:24\u001b[39m, in \u001b[36m_handle_response\u001b[39m\u001b[34m(r)\u001b[39m\n\u001b[32m     23\u001b[39m         detail = r.text\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JudgmentAPIError(r.status_code, detail, r)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r.json()\n",
      "\u001b[31mJudgmentAPIError\u001b[39m: 500: Unexpected error logging custom evaluation results: {'message': 'duplicate key value violates unique constraint \"examples_pkey\"', 'code': '23505', 'hint': None, 'details': 'Key (example_id)=(2425cafe-1ad0-498b-9aaf-1132c48f85d2) already exists.'}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mJudgmentRuntimeError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/judgment-cookbook/.venv/lib/python3.12/site-packages/judgeval/__init__.py:57\u001b[39m, in \u001b[36mJudgmentClient.run_evaluation\u001b[39m\u001b[34m(self, examples, scorers, project_name, eval_run_name, model, assert_test)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28meval\u001b[39m = ExampleEvaluationRun(\n\u001b[32m     50\u001b[39m     project_name=project_name,\n\u001b[32m     51\u001b[39m     eval_name=eval_run_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m     model=model,\n\u001b[32m     55\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m results = \u001b[43mrun_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m assert_test:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/judgment-cookbook/.venv/lib/python3.12/site-packages/judgeval/evaluation/__init__.py:270\u001b[39m, in \u001b[36mrun_eval\u001b[39m\u001b[34m(evaluation_run)\u001b[39m\n\u001b[32m    267\u001b[39m     send_results = [\n\u001b[32m    268\u001b[39m         scoring_result.model_dump(warnings=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m scoring_result \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    269\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     url = \u001b[43mlog_evaluation_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43msend_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m rprint(\n\u001b[32m    272\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔍 You can view your evaluation results here: [rgb(106,0,255)][link=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]View Results[/link]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    273\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/judgment-cookbook/.venv/lib/python3.12/site-packages/judgeval/evaluation/__init__.py:80\u001b[39m, in \u001b[36mlog_evaluation_results\u001b[39m\u001b[34m(scoring_results, run)\u001b[39m\n\u001b[32m     79\u001b[39m judgeval_logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to save evaluation results to DB: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m JudgmentRuntimeError(\n\u001b[32m     81\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequest failed while saving evaluation results to DB: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     82\u001b[39m )\n",
      "\u001b[31mJudgmentRuntimeError\u001b[39m: Request failed while saving evaluation results to DB: 500: Unexpected error logging custom evaluation results: {'message': 'duplicate key value violates unique constraint \"examples_pkey\"', 'code': '23505', 'hint': None, 'details': 'Key (example_id)=(2425cafe-1ad0-498b-9aaf-1132c48f85d2) already exists.'}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mJudgmentRuntimeError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m⚡ Running evaluation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mjudgment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCodeExecutionScorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_generations\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhumaneval-project\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/judgment-cookbook/.venv/lib/python3.12/site-packages/judgeval/__init__.py:70\u001b[39m, in \u001b[36mJudgmentClient.run_evaluation\u001b[39m\u001b[34m(self, examples, scorers, project_name, eval_run_name, model, assert_test)\u001b[39m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     67\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease check your EvaluationRun object, one or more fields are invalid: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m     )\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JudgmentRuntimeError(\n\u001b[32m     71\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAn unexpected error occured during evaluation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mJudgmentRuntimeError\u001b[39m: An unexpected error occured during evaluation: Request failed while saving evaluation results to DB: 500: Unexpected error logging custom evaluation results: {'message': 'duplicate key value violates unique constraint \"examples_pkey\"', 'code': '23505', 'hint': None, 'details': 'Key (example_id)=(2425cafe-1ad0-498b-9aaf-1132c48f85d2) already exists.'}"
     ]
    }
   ],
   "source": [
    "print(\"\\n⚡ Running evaluation...\")\n",
    "judgment.run_evaluation(\n",
    "    examples=dataset.examples,\n",
    "    scorers=[CodeExecutionScorer(k=1, n=num_generations)],\n",
    "    project_name=\"humaneval-project\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa96b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

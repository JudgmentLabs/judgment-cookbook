{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f43c5a36",
   "metadata": {},
   "source": [
    "# Code Execution Scorer\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]()\n",
    "\n",
    "In this notebook, you will learn how to create **custom code-based scorers** using the **judgeval** library. This implementation demonstrates how to evaluate code generation using OpenAI's HumanEval benchmark and their sandboxed execution environment.\n",
    "\n",
    "You will generate code using LLMs, create a custom scorer that leverages OpenAI's sandboxed environment, and evaluate it against the HumanEval benchmark dataset using functional correctness testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17740081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installations\n",
    "!pip install human-eval datasets openai judgeval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0220c8",
   "metadata": {},
   "source": [
    "To run this notebook and train a WikiRacer Agent, select **Runtime* -> Run All*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7767e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set api keys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] =\n",
    "os.environ[\"JUDGMENT_API_KEY\"] =\n",
    "os.environ[\"JUDGMENT_ORG_ID\"] =\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624525fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from judgeval import JudgmentClient\n",
    "from judgeval.dataset import Dataset\n",
    "from judgeval.scorers.example_scorer import ExampleScorer\n",
    "from judgeval.data import Example\n",
    "from datasets import load_dataset\n",
    "from human_eval.execution import check_correctness\n",
    "from openai import AsyncOpenAI\n",
    "from typing import Dict, Any\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "judgment = JudgmentClient()\n",
    "client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f5296",
   "metadata": {},
   "source": [
    "## 1. Understanding HumanEval\n",
    "\n",
    "HumanEval is a benchmark dataset created by OpenAI for evaluating code generation models. Introduced in the paper [\"Evaluating Large Language Models Trained on Code\"](https://arxiv.org/abs/2107.03374), it contains 164 Python programming problems designed to test functional correctness.\n",
    "\n",
    "### What HumanEval Contains\n",
    "Each problem includes:\n",
    "- **Function signature and docstring**: The problem description\n",
    "- **Test cases**: Automated tests to verify correctness  \n",
    "- **Canonical solution**: Reference implementation\n",
    "- **Entry point**: Function name to test\n",
    "\n",
    "### How HumanEval Evaluates Code\n",
    "HumanEval evaluates model outputs by dynamically building a Python program that stitches together the **Function signature and docstring**, the **model‚Äôs generated solution**, and the **test cases**. This combined program is then executed in a sandbox to verify whether the generated code passes all test cases.\n",
    "\n",
    "The ```check_correctness``` function orchestrates this process: assembles the prompt, generated solution, and tests into a single program, and executes the script in a sandboxed environment.\n",
    "\n",
    "```python\n",
    "# Construct the check program and run it.\n",
    "check_program = (\n",
    "    problem[\"prompt\"] +      # Function signature + docstring\n",
    "    completion +            # Generated code\n",
    "    \"\\n\" +\n",
    "    problem[\"test\"] +       # Test cases\n",
    "    \"\\n\" +\n",
    "    f\"check({problem['entry_point']})\"  # Call the test function\n",
    ")\n",
    "\n",
    "...\n",
    "\n",
    "# WARNING: This executes untrusted model-generated code\n",
    "exec(check_program, exec_globals)\n",
    "```\n",
    "\n",
    "The evaluation is **pass/fail**: if all test cases pass without exceptions, the code is correct. If any test fails or the code crashes, it's incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a69bca",
   "metadata": {},
   "source": [
    "## Code Generation Function\n",
    "\n",
    "Next, we‚Äôll implement a function that, given a HumanEval problem, queries an LLM to produce a candidate implementation. The function is written with `async/await` so multiple problems can be evaluated in parallel, significantly reducing total runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd53d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_code(problem: Dict[str, Any]) -> str:\n",
    "    \"\"\"Generate code using LLM for a given HumanEval problem.\"\"\"\n",
    "    prompt = problem[\"prompt\"]\n",
    "    \n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Python programmer. Write ONLY the Python function code that solves the given problem. Do not include any markdown formatting, explanations, or code blocks. Return only the raw Python code.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    generated_code = response.choices[0].message.content\n",
    "    \n",
    "    return generated_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8775f3",
   "metadata": {},
   "source": [
    "## Custom Code Execution Scorer\n",
    "\n",
    "We'll create a custom scorer using judgeval that integrates HumanEval's sandboxed code execution. The scorer uses the `check_correctness` function to evaluate whether generated code passes the test cases.\n",
    "\n",
    "The scorer runs the generated code in a sandboxed environment and checks if all tests pass. If they do, it assigns a score of 1.0. If any test fails or the code crashes, it assigns a score of 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af9e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanEvalCodeExecutionScorer(ExampleScorer):\n",
    "    \"\"\"\n",
    "    A scorer for evaluating code generation using functional correctness.\n",
    "    \n",
    "    This scorer uses the human_eval.execution.check_correctness function\n",
    "    to run generated code against test cases and determine if it passes.\n",
    "    \n",
    "    Attributes:\n",
    "        name (str): The name of the scorer\n",
    "    \"\"\"\n",
    "    name: str = \"HumanEval Code Execution Scorer\"\n",
    "\n",
    "    async def a_score_example(self, example: Example) -> None:\n",
    "        \"\"\"\n",
    "        Score an example by running the generated code against test cases.\n",
    "        \n",
    "        This method uses check_correctness to execute the generated code\n",
    "        in a sandboxed environment and check if it passes all test cases.\n",
    "        \n",
    "        Args:\n",
    "            example (HumanEvalExample): The example containing the problem and generated code\n",
    "            \n",
    "        Returns:\n",
    "            float: The score (1.0 if all tests pass, 0.0 otherwise)\n",
    "        \"\"\"\n",
    "        # Create problem dict in the format expected by check_correctness\n",
    "        problem = {\n",
    "            \"task_id\": example.task_id,\n",
    "            \"prompt\": example.prompt,\n",
    "            \"test\": example.test,\n",
    "            \"entry_point\": example.entry_point\n",
    "        }\n",
    "        \n",
    "        # Use check_correctness to evaluate the generated code\n",
    "        result = check_correctness(\n",
    "            problem=problem,\n",
    "            completion=example.generated_code,\n",
    "            timeout=3.0\n",
    "        )\n",
    "        \n",
    "        # Set score based on whether tests passed\n",
    "        if result[\"passed\"]:\n",
    "            self.score = 1.0\n",
    "            self.reason = \"All test cases passed\"\n",
    "        else:\n",
    "            self.score = 0.0\n",
    "            self.reason = f\"Test failed: {result['result']}\"\n",
    "        \n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971f166",
   "metadata": {},
   "source": [
    "## Load HumanEval Dataset\n",
    "\n",
    "Now let's load the HumanEval dataset from Hugging Face and examine its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee8c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HumanEval dataset\n",
    "print(\"üìä Loading HumanEval dataset...\")\n",
    "dataset = load_dataset(\"openai/openai_humaneval\")\n",
    "print(f\"   Found {len(dataset['test'])} problems\")\n",
    "\n",
    "# Examine the structure of a single problem\n",
    "example_problem = dataset[\"test\"][0]\n",
    "print(\"\\nüìã Example problem structure:\")\n",
    "print(f\"   Task ID: {example_problem['task_id']}\")\n",
    "print(f\"   Entry Point: {example_problem['entry_point']}\")\n",
    "print(f\"   Prompt length: {len(example_problem['prompt'])} characters\")\n",
    "print(f\"   Test length: {len(example_problem['test'])} characters\")\n",
    "\n",
    "print(\"\\nüìù Sample prompt:\")\n",
    "print(example_problem['prompt'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8253bf72",
   "metadata": {},
   "source": [
    "Generate code responses for each problem in the HumanEval benchmark and create example objects to upload into judgeval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2856d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nü§ñ Generating code...\")\n",
    "problems = list(dataset[\"test\"].select(range(164)))\n",
    "\n",
    "# Generate all code in parallel\n",
    "generated_codes = await asyncio.gather(*[\n",
    "    generate_code(problem) \n",
    "    for problem in problems\n",
    "])\n",
    "\n",
    "# Create examples\n",
    "examples = []\n",
    "for i, (problem, generated_code) in enumerate(zip(problems, generated_codes)):\n",
    "    print(f\"   Problem {i+1}/5: {problem['task_id']}\")\n",
    "    \n",
    "    example = Example(\n",
    "        task_id=problem[\"task_id\"],\n",
    "        prompt=problem[\"prompt\"],\n",
    "        canonical_solution=problem[\"canonical_solution\"],\n",
    "        test=problem[\"test\"],\n",
    "        entry_point=problem[\"entry_point\"],\n",
    "        generated_code=generated_code\n",
    "    )\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a0bec",
   "metadata": {},
   "source": [
    "We use `Dataset.create()` to create a new dataset and upload it to the judgment platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e2b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.create(\n",
    "    name=\"humaneval-dataset\", \n",
    "    project_name=\"humaneval-project\", \n",
    "    examples=examples,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef3345",
   "metadata": {},
   "source": [
    "We use `Dataset.get()` to retrieve an existing dataset from the judgment platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.get(\n",
    "    name=\"humaneval-dataset\",\n",
    "    project_name=\"humaneval-project\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aac9879",
   "metadata": {},
   "source": [
    "## Running Evaluation\n",
    "\n",
    "We then use our custom scorer and the judgment client to run evaluations asynchronously on our servers and display the results on the judgment platform for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49865caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚ö° Running evaluation...\")\n",
    "judgment.run_evaluation(\n",
    "    examples=dataset.examples,\n",
    "    scorers=[HumanEvalCodeExecutionScorer()],\n",
    "    project_name=\"humaneval-project\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

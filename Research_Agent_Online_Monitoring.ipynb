{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3733896",
   "metadata": {},
   "source": [
    "# Online Monitoring for Agents\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JudgmentLabs/judgment-cookbook/blob/main/Report_Agent_Online_Monitoringipynb)\n",
    "[![Docs](https://img.shields.io/badge/Documentation-blue)](https://docs.judgmentlabs.ai/documentation)\n",
    "\n",
    "\n",
    "In this notebook, you will learn how to create a research agent that scours Wikimedia's [Wikipedia](https://huggingface.co/datasets/wikimedia/wikipedia) dataset with [ChromaDB](https://www.trychroma.com/), then monitor and evaluate its performance using **custom scorers** with the [`judgeval`](https://github.com/JudgmentLabs/judgeval) library for real-time AI agent monitoring.\n",
    "\n",
    "1. **Example Scorer (Output-Level)**: Uses LLM-as-a-judge to evaluate the research report relevance\n",
    "2. **Trajectory Scorer (Process-Level)**: Uses LLM-as-a-judge to validate if citations and document references actually exist in the database\n",
    "3. **Alerts**: Get notified via email, Slack, or PagerDuty when scorers dip below performance thresholds\n",
    "\n",
    "\n",
    "You will create a research agent using Wikipedia dataset and ChromaDB, then build custom scorers that evaluate both example-level and trajectory-level agent behavior with real-time alerts for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da60419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installations\n",
    "!pip install chromadb datasets openai judgeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173794c",
   "metadata": {},
   "source": [
    "To run this notebook, select **Runtime -> Run All**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beba64f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You can get your Judgment API key and Org ID for free on the [Judgment Labs Platform](https://app.judgmentlabs.ai/register).\n",
    "\n",
    "![Get Started](./assets/get_started.png)\n",
    "\n",
    "On the Judgment Platform and within your organization, create a project called `research-agent-project`.\n",
    "- Within Judgment's web app, navigate to `Projects` -> `New Project` in the top right corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad1912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set api keys\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ...  # Fill your API keys here\n",
    "os.environ[\"JUDGMENT_API_KEY\"] = ...\n",
    "os.environ[\"JUDGMENT_ORG_ID\"] = ...\n",
    "\n",
    "# Set tokenizers parallelism to avoid warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14fdff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trace and client\n",
    "from judgeval.tracer import Tracer, wrap\n",
    "from openai import OpenAI\n",
    "\n",
    "judgment = Tracer(project_name=\"research-agent-project\")\n",
    "client = wrap(OpenAI()) #automatically tracks all LLM API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54cf012",
   "metadata": {},
   "source": [
    "## Understanding the Wikipedia Dataset\n",
    "\n",
    "[Wikipedia](https://huggingface.co/datasets/wikimedia/wikipedia) is a dataset created by Wikimedia containing cleaned articles across 300+ languages. We'll be using the simplified English split, which contains 242k articles written in easier-to-understand language compared to standard Wikipedia articles, dating from November 1st, 2023.\n",
    "\n",
    "### What Wikipedia Contains\n",
    "Each article contains:\n",
    "- **id**: Article identifier\n",
    "- **url**: URL of the article\n",
    "- **title**: Title of the article\n",
    "- **text**: Text content of the article\n",
    "\n",
    "Let's take a look at an example article from the dataset to understand the data we're working with!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca127d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.simple\", split=\"train\")\n",
    "print(\"Title: \", dataset[0][\"title\"])\n",
    "print(\"URL: \", dataset[0][\"url\"])\n",
    "print(\"Text: \", dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef694c81",
   "metadata": {},
   "source": [
    "## Building the Research Agent\n",
    "\n",
    "Now that we've explored the Wikipedia dataset, we'll use this data to build an agent that searches through the database and generates reports based on user queries!\n",
    "\n",
    "Here are the steps we will follow:\n",
    "\n",
    "1. **Database Setup**: Initialize ChromaDB and populate with articles from Wikipedia\n",
    "\n",
    "2. **Build Custom Scorers**: Create both output-level and trajectory-level evaluation scorers to track report quality and agent behavior\n",
    "\n",
    "3. **Online Monitoring**: Set up online monitoring and integrate into the agent and tools (`search_documents`, `get_document`, `synthesize_report`)\n",
    "\n",
    "4. **Alerts**: Configure notifications when scorers cross thresholds via email, Slack, or PagerDuty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82d950b",
   "metadata": {},
   "source": [
    "### Database Setup\n",
    "\n",
    "ChromaDB is the open-source search and retrieval database for AI applications. It uses the [`sentence-transformers/all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model to embed texts and compares them to user queries to rank documents so you retrieve the most relevant documents given the query.\n",
    "\n",
    "Now, let's create our database and populate it with Wikipedia articles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8061d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(\"knowledge_base\")\n",
    "\n",
    "batch_size = 100\n",
    "print(f\"Adding {min(batch_size, len(dataset))} articles to ChromaDB...\")\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "for i in range(min(batch_size, len(dataset))):\n",
    "    article = dataset[i]\n",
    "    documents.append(article[\"text\"])\n",
    "    metadatas.append({\"url\": article[\"url\"]})\n",
    "    ids.append(article[\"title\"])\n",
    "\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "print(f\"ChromaDB initialized with {min(batch_size, len(dataset))} Wikipedia articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cacbc8",
   "metadata": {},
   "source": [
    "Great! Now we have added 100 articles into our database. Note that since Wikipedia is sorted in alphabetical order, the first 100 articles typically cover topics at the beginning of the alphabet. Hence, you won't find any articles relating to xylophones (unless you add more articles)!\n",
    "\n",
    "Let's see how we can query the database for articles relating to science!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad33bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collection.query(query_texts=[\"What are the different types of science?\"], n_results=5)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b334ec5",
   "metadata": {},
   "source": [
    "#### Define Your Custom Example Class\n",
    "\n",
    "Now that we understand how the database works, we need to store the information together. This is needed to pass into our custom scorers later on.\n",
    "\n",
    "In `judgeval`, all data passed into scorers is represented as an `Example`. The base `Example` object is an abstraction that standardizes how data is stored and accessed. By inheriting from it, you can define your own fields that describe the task you want to monitor.\n",
    "\n",
    "For our research agent, we'll create a `Report` that captures the fields needed to represent a query and its generated report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91505c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from judgeval.data import Example\n",
    "\n",
    "class Report(Example):\n",
    "    query: str\n",
    "    report: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8710da",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example = Report(query=\"What is the capital of France?\", report=\"The capital of France is Paris.\")\n",
    "print(\"The query is: \", one_example.query)\n",
    "print(\"The report is: \", one_example.report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a690c6",
   "metadata": {},
   "source": [
    "### Custom Scorers\n",
    "\n",
    "We'll create custom scorers using `judgeval` that utilize LLM-as-a-judge in two different ways:\n",
    "\n",
    "1. **Example-level scorer**: A custom `ReportRelevanceScorer` that evaluates only the final output and if the final report is relevant to the user query\n",
    "2. **Trajectory-level scorer**: A `TracePromptScorer` that evaluates the entire agent behavior and decision-making process throughout execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a3c1f",
   "metadata": {},
   "source": [
    "#### Example Level Scorer\n",
    "\n",
    "Let's first implement the `ReportRelevanceScorer` to check if the final report is relevant to the user query.\n",
    "\n",
    "In `judgeval`, the user must implement:\n",
    "\n",
    "`async def a_score_example(self, example: Example)`\n",
    "\n",
    "This method will asynchronously score each example, and the scorer should set three key fields:\n",
    "\n",
    "- **`self.name`**: scorer name shown in the Judgment Labs platform dashboard \n",
    "- **`self.score`**: numeric metric value (e.g., RELEVANT == 1, NOT RELEVANT == 0`)  \n",
    "- **`self.reason`**: human-readable explanation or context behind the score  \n",
    "\n",
    "In addition, we should set the **`server_hosted`** variable to `True` to enable server hosting of the custom scorer.\n",
    "\n",
    "To see more examples of Custom Scorers, take a look at our [HumanEval Custom Scorer cookbook](https://colab.research.google.com/github/JudgmentLabs/judgment-cookbook/blob/refactor/HumanEval_Custom_Scorer.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from judgeval.scorers.example_scorer import ExampleScorer\n",
    "from judgeval.data import Example\n",
    "from openai import OpenAI\n",
    "\n",
    "class Report(Example):\n",
    "    query: str\n",
    "    report: str\n",
    "\n",
    "class ReportRelevanceScorer(ExampleScorer):\n",
    "    name: str = \"Report Relevance Scorer\"\n",
    "    server_hosted: bool = True # Enable server hosting\n",
    "\n",
    "    async def a_score_example(self, example: Report):\n",
    "        client = OpenAI()\n",
    "        # Use LLM to evaluate if research report is relevant to the query\n",
    "        evaluation_prompt = f\"\"\"\n",
    "        Evaluate if this research report is relevant to the query.\n",
    "        \n",
    "        Query: {example.query}\n",
    "        Report: {example.report}\n",
    "        \n",
    "        Is the report relevant and does it answer the query? Answer only \"YES\" or \"NO\".\n",
    "        \"\"\"\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            messages=[{\"role\": \"user\", \"content\": evaluation_prompt}]\n",
    "        )\n",
    "        evaluation = completion.choices[0].message.content.strip().upper()\n",
    "\n",
    "        if evaluation == \"YES\":\n",
    "            self.reason = \"LLM evaluation: Report is relevant to the query\"\n",
    "            return 1.0\n",
    "        else:\n",
    "            self.reason = \"LLM evaluation: Report is not relevant to the query\"\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b5aef2",
   "metadata": {},
   "source": [
    "#### Upload Your Scorer\n",
    "\n",
    "These custom scorers may add application latency if you run them locally, thus, we have built secure infrastructure to run them with zero impact on Firecracker microVMs. In order to use our servers, first copy and paste the above cell into a Python file called `report_relevance_scorer.py` (we do this for you), and then deploy your scorer to our infrastructure with a single command:\n",
    "\n",
    "\n",
    "```bash\n",
    "echo -e \"pydantic\\nopenai\" > requirements.txt\n",
    "uv run judgeval upload_scorer report_relevance_scorer.py requirements.txt\n",
    "```\n",
    "\n",
    "Your scorer runs in its own secure sandbox. Re-upload anytime your scoring logic changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde4ebf4",
   "metadata": {},
   "source": [
    "#### Trajectory Level Scorer\n",
    "\n",
    "If you don't only want to check the output-level of your agent, but rather the behavior and tool calls it took throughout the process, we can use the `TracePromptScorer` to apply a prompt onto an LLM-as-a-judge to look over the trace of your agent. This can be helpful when you want to analyze tool-call ordering, agent reasoning, and general behavior based on certain user queries. \n",
    "\n",
    "In our example, we will use the `TracePromptScorer` to ensure that the generated report's citations actually reference documents that exist in the database. We will check this by verifying if the agent used the `get_document` tool to retrieve each cited document or if it fabricated the citations.\n",
    "\n",
    "The `TracePromptScorer` has two main methods:\n",
    "\n",
    "- **`.create(name, prompt, options)`**: Creates a scorer with that name and prompt on the server. The prompt instructs the LLM to evaluate the agent's trajectory and decision-making process. You can also provide an `options` parameter to map the LLM's text responses to numeric scores. For example, if the LLM responds with \"Accurate\" or \"Inaccurate\", you can set `options={\"Accurate\": 1.0, \"Inaccurate\": 0.0}` to convert these text responses into numeric scores.\n",
    "- **`.get(name)`**: Retrieves an existing scorer by name\n",
    "\n",
    "The `TracePromptScorer` pairs with a `TraceScorerConfig`, which we'll implement in a `@judgment.observe(span_type=\"function\", scorer_config=TraceScorerConfig(scorer=trace_scorer, model=\"gpt-4.1\"))` decorator where you can define the model to monitor all trajectories in real-time (more on this decorator later).\n",
    "\n",
    "NOTE: You only need to run the `.create` method once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from judgeval.tracer import Tracer, TraceScorerConfig\n",
    "from judgeval.scorers import TracePromptScorer\n",
    "\n",
    "# UNCOMMENT IF YOU HAVR NOT CREATED THE SCORER YET\n",
    "# trace_scorer = TracePromptScorer.create(\n",
    "#     name=\"Citation Count Scorer\",\n",
    "#     prompt=\"Count how many citations in the report are present in the document searches throughout the database? Then divide by the total number of citations in the report and return the score as a number between 0 and 1.\"\n",
    "# )\n",
    "\n",
    "trace_scorer = TracePromptScorer.get(\n",
    "    name=\"Citation Count Scorer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d528d58b",
   "metadata": {},
   "source": [
    "### Building the Agent with Online Monitoring!\n",
    "\n",
    "Now that we've built the custom scorers, we'll build the agent and instrument the custom scorers to monitor in real-time as each query is passed through the agent. This enables systematic scorer frameworks to run directly on your live agents in production, alerting you the instant agents begin to misbehave so you can push hotfixes before customers are affected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337bddf6",
   "metadata": {},
   "source": [
    "Now let's create some tools for our agent:\n",
    "\n",
    "- `search_documents(collection, query, n_results)` - Search for relevant texts inside `collection` based on `query` and returns the `n_results` most relevant results\n",
    "\n",
    "- `get_document(collection, doc_id)` - Get the document from collection at `doc_id`\n",
    "\n",
    "- `synthesize_report(query, documents)` - Based on the query and the list of documents, create a comprehensive report with citations based on documents\n",
    "\n",
    "\n",
    "The `@judgment.observe(span_type=\"tool\")` decorator on top of each function will capture all agent interactions using the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "@judgment.observe(span_type=\"tool\")\n",
    "def search_documents(collection, query: str, n_results: int = 10):\n",
    "    \"\"\"Search for relevant documents\"\"\"\n",
    "    results = collection.query(query_texts=[query], n_results=n_results)\n",
    "    if results['ids'][0]:\n",
    "        return results['ids'][0]\n",
    "    return []\n",
    "\n",
    "@judgment.observe(span_type=\"tool\")\n",
    "def get_document(collection, doc_id: str):\n",
    "    \"\"\"Get specific document by ID\"\"\"\n",
    "    result = collection.get(ids=[doc_id])\n",
    "\n",
    "    if result['ids']:\n",
    "        url = result['metadatas'][0]['url']\n",
    "        content = result['documents'][0]\n",
    "        return f\"Title: {doc_id}\\nURL: {url}\\nContent: {content}\"\n",
    "    return None\n",
    "\n",
    "@judgment.observe(span_type=\"tool\")\n",
    "def synthesize_report(query: str, documents: list) -> str:\n",
    "    \"\"\"Create a structured report with proper citations using LLM\"\"\"\n",
    "    \n",
    "    citation_prompt = f\"\"\"\n",
    "    Create a comprehensive research report for: \"{query}\"\n",
    "    \n",
    "    Use the following sources and provide proper citations. Each source contains the title, URL, and content:\n",
    "    \n",
    "    {chr(10).join([f\"Source {i+1}: {doc}...\" for i, doc in enumerate(documents, 1)])}\n",
    "    \n",
    "    Requirements:\n",
    "    1. Synthesize information from all sources\n",
    "    2. Create a structured report with introduction, main points, and conclusion\n",
    "    3. Ensure all information is properly attributed to its source\n",
    "    4. Write in a professional, academic style\n",
    "    5. Use numbered in-text citations throughout the report (e.g., \"This concept was first introduced [1] and later expanded [2]\")\n",
    "    \n",
    "    Format the report as:\n",
    "    # Research Report: [Query]\n",
    "    \n",
    "    ## Introduction\n",
    "    [Brief introduction to the topic]\n",
    "    \n",
    "    ## Key Findings\n",
    "    [Main points with numbered in-text citations like [1], [2], [3], etc.]\n",
    "    \n",
    "    ## Conclusion\n",
    "    [Summary and conclusions]\n",
    "    \n",
    "    ## Works Cited\n",
    "    [Numbered list of all sources used and their corresponding URL, formatted as: \"1. Title of Article - Wikipedia (URL)\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[{\"role\": \"user\", \"content\": citation_prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c535c",
   "metadata": {},
   "source": [
    "Now let's build the agent core that orchestrates these tools:\n",
    "\n",
    "- `ResearchAgent` class - The main agent that coordinates tool calls and decision-making\n",
    "- `run()` method - LLM-driven orchestration loop that parses XML tool calls and executes them\n",
    "- `call_tool()` method - Routes tool calls to the appropriate function handlers\n",
    "- `handle_request()` method - Entry point that integrates monitoring with `judgment.async_evaluate()`\n",
    "\n",
    "The `@judgment.observe(span_type=\"function\")` decorator on top of a function captures all agent-level interactions and decision-making.\n",
    "\n",
    "**Key Monitoring Components:**\n",
    "- `judgment.async_evaluate(scorer, example, sampling_rate)` runs hosted scorers with zero latency impact. The `scorer` is the custom scorer we defined earlier, the `example` is the data we want to score, and the `sampling_rate` controls the frequency of scoring (0.95 = 95% of requests)\n",
    "- `TraceScorerConfig` is used to configure trajectory-level scorers and specify the model for scoring\n",
    "\n",
    "Scorers can take time to execute, so they may appear slightly delayed on the UI.\n",
    "\n",
    "**Note**: `span_type=\"function\"` is used for functions that instrument agent logic, distinct from `span_type=\"tool\"` which is used for individual tool functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from report_relevance_scorer import ReportRelevanceScorer, Report\n",
    "import re\n",
    "\n",
    "class ResearchAgent:\n",
    "    def __init__(self, collection):\n",
    "        self.collection = collection\n",
    "        self.system_prompt = \"\"\"You are a research agent. You will use the following tools to complete a report based on the user's request.\n",
    "\n",
    "AVAILABLE TOOLS:\n",
    "1. search_documents(query, n_results) - Search for relevant documents, returns list of document IDs\n",
    "2. get_document(doc_id) - Get specific document content by ID (returns \"Title: [title]\\nContent: [content]\")\n",
    "3. synthesize_report(query, documents) - Create structured report from documents (documents should be actual text content, not IDs)\n",
    "\n",
    "EXECUTION PROCESS:\n",
    "1. **Search**: Find relevant documents with search_documents()\n",
    "2. **Retrieve**: Get actual content of each document with get_document()\n",
    "3. **Synthesize**: Create report with synthesize_report() using actual document content\n",
    "4. **Assess**: Have you COMPLETELY fulfilled the user's request?\n",
    "   - If NO: Continue searching until you have enough information to answer the question\n",
    "   - If YES: Complete the task\n",
    "\n",
    "WHAT \"DONE\" MEANS:\n",
    "- You have gathered ALL information the user requested\n",
    "- You have performed ALL actions the user asked for\n",
    "- You have delivered a COMPLETE answer to their question\n",
    "- Nothing from their original request is missing or incomplete\n",
    "\n",
    "KEY RULE:\n",
    "- **EVERY RESPONSE MUST END WITH A TOOL CALL UNLESS YOU HAVE COMPLETELY FULFILLED THE USER'S REQUEST**\n",
    "\n",
    "Format responses as:\n",
    "<plan>Your analysis and planning when needed</plan>\n",
    "<tool>\n",
    "{\"name\": \"tool_name\", \"args\": {\"parameter\": \"value\"}}\n",
    "</tool>\"\"\"\n",
    "    \n",
    "    @judgment.observe(span_type=\"function\")\n",
    "    def handle_request(self, query: str) -> str:\n",
    "        \"\"\"Handle a research request and generate a report\"\"\"\n",
    "        report = self.run(query)\n",
    "        \n",
    "        # Online evaluation with server-hosted scorer\n",
    "        judgment.async_evaluate(\n",
    "            scorer=ReportRelevanceScorer(),\n",
    "            example=Report(query=query, report=report),\n",
    "            sampling_rate=1.0  # Scores 100% of agent runs\n",
    "        )\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def call_tool(self, tool_name: str, params: dict):\n",
    "        \"\"\"Call a tool and return result\"\"\"\n",
    "        tool_handlers = {\n",
    "            \"search_documents\": lambda: search_documents(self.collection, **params),\n",
    "            \"get_document\": lambda: get_document(self.collection, **params),\n",
    "            \"synthesize_report\": lambda: synthesize_report(**params)\n",
    "        }\n",
    "        return tool_handlers.get(tool_name, lambda: None)()\n",
    "    \n",
    "    @judgment.observe(span_type=\"function\", scorer_config=TraceScorerConfig(scorer=trace_scorer, model=\"gpt-4.1\"))\n",
    "    def run(self, user_query: str, max_iterations: int = 50):\n",
    "        \"\"\"Run the agent with LLM-driven tool calling\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Query: {user_query}\"}\n",
    "        ]\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4.1\",\n",
    "                messages=messages,\n",
    "            )\n",
    "            \n",
    "            llm_output = response.choices[0].message.content\n",
    "            messages.append({\"role\": \"assistant\", \"content\": llm_output})\n",
    "            \n",
    "            # Parse tool calls\n",
    "            if \"<tool>\" in llm_output:\n",
    "                # Extract JSON from <tool> tags\n",
    "                tool_match = re.search(r'<tool>\\s*(\\{.*?\\})\\s*</tool>', llm_output, re.DOTALL)\n",
    "                if tool_match:\n",
    "                    import json\n",
    "                    tool_data = json.loads(tool_match.group(1))\n",
    "                    tool_name = tool_data[\"name\"]\n",
    "                    params = tool_data[\"args\"]\n",
    "                    \n",
    "                    tool_result = self.call_tool(tool_name, params)\n",
    "                    \n",
    "                    # If synthesize_report, return the report directly\n",
    "                    if tool_name == \"synthesize_report\":\n",
    "                        return tool_result\n",
    "                    \n",
    "                    # Add result to conversation\n",
    "                    messages.append({\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": f\"<result>{tool_result}</result>\"\n",
    "                    })\n",
    "            \n",
    "            # Check if agent is done (no tool call and has content)\n",
    "            elif \"<tool>\" not in llm_output and llm_output.strip():\n",
    "                return llm_output.strip()\n",
    "        \n",
    "        return \"Agent reached maximum iterations\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b4ce49",
   "metadata": {},
   "source": [
    "### Complete Agent in Action\n",
    "\n",
    "Now let's test our complete monitoring system! Run the agent with the following code and check out your project page for `research-agent-project` to see the real-time monitoring in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4fdbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the different types of sciences?\"\n",
    "agent = ResearchAgent()\n",
    "print(agent.handle_request(query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
